{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib\n",
    "import argparse\n",
    "import random\n",
    "import numpy as np\n",
    "import csv\n",
    "import pandas as pd\n",
    "from datascience import *\n",
    "import tensorflow as tf\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from keras.models import Model\n",
    "from keras.models import Sequential, load_model\n",
    "from keras.layers import *\n",
    "from keras.optimizers import Adam\n",
    "from keras import Input\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.utils import plot_model\n",
    "import keras\n",
    "from keras.backend.tensorflow_backend import set_session\n",
    "#config = tf.ConfigProto()\n",
    "# config.gpu_options.allocator_type = 'BFC' #A \"Best-fit with coalescing\" algorithm, simplified from a version of dlmalloc.\n",
    "# config.gpu_options.per_process_gpu_memory_fraction = 0.7\n",
    "#config.gpu_options.allow_growth =True\n",
    "\n",
    "#set_session(tf.Session(config=config)) \n",
    "import re\n",
    "os.environ['PYTHONHASHSEED'] = '0'\n",
    "random.seed(1)\n",
    "np.random.seed(1)\n",
    "tf.set_random_seed(1)\n",
    "from keras.callbacks import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "print(tf.test.is_gpu_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def sanitize(string):\n",
    "    words = string.split(' ')\n",
    "    return words\n",
    "\n",
    "def converter(x):\n",
    "    try:\n",
    "        return ' '.join([x.lower() for x in str(x).split() if x not in stop_words])\n",
    "    except AttributeError:\n",
    "        return None  # or some other value\n",
    "    \n",
    "def whole(x):\n",
    "    return int(round(x))\n",
    "\n",
    "whole(1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>qid1</th>\n",
       "      <th>qid2</th>\n",
       "      <th>question1</th>\n",
       "      <th>question2</th>\n",
       "      <th>is_duplicate</th>\n",
       "      <th>title1_tokenized</th>\n",
       "      <th>title2_tokenized</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>What is the step by step guide to invest in sh...</td>\n",
       "      <td>What is the step by step guide to invest in sh...</td>\n",
       "      <td>0</td>\n",
       "      <td>[What, is, the, step, by, step, guide, to, inv...</td>\n",
       "      <td>[What, is, the, step, by, step, guide, to, inv...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>What is the story of Kohinoor (Koh-i-Noor) Dia...</td>\n",
       "      <td>What would happen if the Indian government sto...</td>\n",
       "      <td>0</td>\n",
       "      <td>[What, is, the, story, of, Kohinoor, (Koh-i-No...</td>\n",
       "      <td>[What, would, happen, if, the, Indian, governm...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>How can I increase the speed of my internet co...</td>\n",
       "      <td>How can Internet speed be increased by hacking...</td>\n",
       "      <td>0</td>\n",
       "      <td>[How, can, I, increase, the, speed, of, my, in...</td>\n",
       "      <td>[How, can, Internet, speed, be, increased, by,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>8</td>\n",
       "      <td>Why am I mentally very lonely? How can I solve...</td>\n",
       "      <td>Find the remainder when [math]23^{24}[/math] i...</td>\n",
       "      <td>0</td>\n",
       "      <td>[Why, am, I, mentally, very, lonely?, How, can...</td>\n",
       "      <td>[Find, the, remainder, when, [math]23^{24}[/ma...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>9</td>\n",
       "      <td>10</td>\n",
       "      <td>Which one dissolve in water quikly sugar, salt...</td>\n",
       "      <td>Which fish would survive in salt water?</td>\n",
       "      <td>0</td>\n",
       "      <td>[Which, one, dissolve, in, water, quikly, suga...</td>\n",
       "      <td>[Which, fish, would, survive, in, salt, water?]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id  qid1  qid2                                          question1  \\\n",
       "0   0     1     2  What is the step by step guide to invest in sh...   \n",
       "1   1     3     4  What is the story of Kohinoor (Koh-i-Noor) Dia...   \n",
       "2   2     5     6  How can I increase the speed of my internet co...   \n",
       "3   3     7     8  Why am I mentally very lonely? How can I solve...   \n",
       "4   4     9    10  Which one dissolve in water quikly sugar, salt...   \n",
       "\n",
       "                                           question2  is_duplicate  \\\n",
       "0  What is the step by step guide to invest in sh...             0   \n",
       "1  What would happen if the Indian government sto...             0   \n",
       "2  How can Internet speed be increased by hacking...             0   \n",
       "3  Find the remainder when [math]23^{24}[/math] i...             0   \n",
       "4            Which fish would survive in salt water?             0   \n",
       "\n",
       "                                    title1_tokenized  \\\n",
       "0  [What, is, the, step, by, step, guide, to, inv...   \n",
       "1  [What, is, the, story, of, Kohinoor, (Koh-i-No...   \n",
       "2  [How, can, I, increase, the, speed, of, my, in...   \n",
       "3  [Why, am, I, mentally, very, lonely?, How, can...   \n",
       "4  [Which, one, dissolve, in, water, quikly, suga...   \n",
       "\n",
       "                                    title2_tokenized  \n",
       "0  [What, is, the, step, by, step, guide, to, inv...  \n",
       "1  [What, would, happen, if, the, Indian, governm...  \n",
       "2  [How, can, Internet, speed, be, increased, by,...  \n",
       "3  [Find, the, remainder, when, [math]23^{24}[/ma...  \n",
       "4    [Which, fish, would, survive, in, salt, water?]  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#read in the csv files \n",
    "\n",
    "quora = pd.read_csv(\"dataset/Quora/quora_duplicate_questions_sanitized.tsv\", sep='\\t')\n",
    "\n",
    "\n",
    "quora['question1']=quora['question1'].fillna(\"\")\n",
    "quora['question2']=quora['question2'].fillna(\"\")\n",
    "\n",
    "\n",
    "quora['title1_tokenized'] = \\\n",
    "    quora.loc[:, 'question1'] \\\n",
    "         .apply(sanitize)\n",
    "quora['title2_tokenized'] = \\\n",
    "    quora.loc[:, 'question2'] \\\n",
    "         .apply(sanitize)\n",
    "\n",
    "\n",
    "\n",
    "quora.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "    <thead>\n",
       "        <tr>\n",
       "            <th>id</th> <th>qid1</th> <th>qid2</th> <th>question1</th> <th>question2</th> <th>is_duplicate</th>\n",
       "        </tr>\n",
       "    </thead>\n",
       "    <tbody>\n",
       "        <tr>\n",
       "            <td>0   </td> <td>1   </td> <td>2   </td> <td>What is the step by step guide to invest in share market ...</td> <td>What is the step by step guide to invest in share market?   </td> <td>0           </td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>1   </td> <td>3   </td> <td>4   </td> <td>What is the story of Kohinoor (Koh-i-Noor) Diamond?         </td> <td>What would happen if the Indian government stole the Koh ...</td> <td>0           </td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>2   </td> <td>5   </td> <td>6   </td> <td>How can I increase the speed of my internet connection w ...</td> <td>How can Internet speed be increased by hacking through DNS? </td> <td>0           </td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>3   </td> <td>7   </td> <td>8   </td> <td>Why am I mentally very lonely? How can I solve it?          </td> <td>Find the remainder when [math]23^{24}[/math] is divided  ...</td> <td>0           </td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>4   </td> <td>9   </td> <td>10  </td> <td>Which one dissolve in water quikly sugar, salt, methane  ...</td> <td>Which fish would survive in salt water?                     </td> <td>0           </td>\n",
       "        </tr>\n",
       "    </tbody>\n",
       "</table>\n",
       "<p>... (404285 rows omitted)</p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "Tb = Table()\n",
    "list1 = []\n",
    "list2 = []\n",
    "\n",
    "with open(\"dataset/Quora/quora_duplicate_questions_sanitized.tsv\", 'r', encoding='utf-8') as fd:\n",
    "    rd = csv.reader(fd, delimiter=\"\\t\", quotechar='\"')\n",
    "    for row in rd:\n",
    "        \n",
    "        list1.append(row) #pairs start at index 1, with id, setence1, sentence2, label, respectively.\n",
    "     \n",
    "    for y in range(6):\n",
    "        temp=[]\n",
    "        for x in range(1, len(list1)):\n",
    "            temp.append(list1[x][y])\n",
    "            \n",
    "        list2.append(temp)\n",
    "\n",
    "    for ele in range(len(list2)):\n",
    "        Tb=Tb.with_columns(list1[0][ele], list2[ele])\n",
    "\n",
    "\n",
    "Tb.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from scipy import spatial\n",
    "\n",
    "def cos_sim(vec1, vec2):\n",
    "    return (1 - spatial.distance.cosine(vec1, vec2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "import difflib\n",
    "\n",
    "case_a = 'i like apple'\n",
    "case_b = 'i like pie'\n",
    "\n",
    "output_list = [li for li in difflib.ndiff(case_a, case_b) if li[0] != ' ']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "def diff(sen1, sen2):\n",
    "    a=[]\n",
    "    b=[]\n",
    "    c=[]\n",
    "    wordlist1 = sen1.split()\n",
    "    wordlist2 = sen2.split()   \n",
    "    for i in wordlist1:\n",
    "        if i not in wordlist2:\n",
    "            a=np.append(a, i)\n",
    "        else:\n",
    "            c=np.append(c, i)\n",
    "    for j in wordlist2:\n",
    "        if j not in wordlist1:\n",
    "            b=np.append(b, j)  \n",
    "    return np.array(a),np.array(b),np.array(c)       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['- a', '+ i', '- p', '- l']"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "    <thead>\n",
       "        <tr>\n",
       "            <th>id</th> <th>qid1</th> <th>qid2</th> <th>question1</th> <th>question2</th> <th>is_duplicate</th>\n",
       "        </tr>\n",
       "    </thead>\n",
       "    <tbody>\n",
       "        <tr>\n",
       "            <td>5   </td> <td>11  </td> <td>12  </td> <td>Astrology: I am a Capricorn Sun Cap moon and cap rising. ...</td> <td>I'm a triple Capricorn (Sun, Moon and ascendant in Capri ...</td> <td>1           </td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>7   </td> <td>15  </td> <td>16  </td> <td>How can I be a good geologist?                              </td> <td>What should I do to be a great geologist?                   </td> <td>1           </td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>11  </td> <td>23  </td> <td>24  </td> <td>How do I read and find my YouTube comments?                 </td> <td>How can I see all my Youtube comments?                      </td> <td>1           </td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>12  </td> <td>25  </td> <td>26  </td> <td>What can make Physics easy to learn?                        </td> <td>How can you make physics easy to learn?                     </td> <td>1           </td>\n",
       "        </tr>\n",
       "    </tbody>\n",
       "</table>\n",
       "<p>... (149259 rows omitted)</p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "true_Tb = Tb.where(\"is_duplicate\", are.equal_to(\"1\"))\n",
    "true_Tb.show(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent1 =  true_Tb.column(\"question1\")\n",
    "sent2=true_Tb.column(\"question2\")\n",
    "a=[0]*20\n",
    "b=[0]*20\n",
    "c=[0]*20\n",
    "\n",
    "for x in range(20):\n",
    "    d, e, f=diff(sent1[x], sent2[x])\n",
    "   \n",
    "    a[x]=d\n",
    "    b[x]=e\n",
    "    c[x]=f\n",
    "   \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_Tb=Table().with_column(\"sent1\", a, \"sent2\",b,\"same\",c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_Tb = true_Tb.select(\"question1\", \"question2\").sample(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "    <thead>\n",
       "        <tr>\n",
       "            <th>sent1</th> <th>sent2</th> <th>same</th>\n",
       "        </tr>\n",
       "    </thead>\n",
       "    <tbody>\n",
       "        <tr>\n",
       "            <td>['Astrology:' 'I' 'am' 'Sun' 'Cap' 'moon' 'cap' 'rising. ...</td> <td>[\"I'm\" 'triple' '(Sun,' 'Moon' 'ascendant' 'in' 'Caprico ...</td> <td>['a' 'Capricorn' 'and' 'does' 'say' 'about' 'me?']          </td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>['How' 'can' 'good']                                        </td> <td>['What' 'should' 'do' 'to' 'great']                         </td> <td>['I' 'be' 'a' 'geologist?']                                 </td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>['do' 'read' 'and' 'find' 'YouTube']                        </td> <td>['can' 'see' 'all' 'Youtube']                               </td> <td>['How' 'I' 'my' 'comments?']                                </td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>['What' 'Physics']                                          </td> <td>['How' 'you' 'physics']                                     </td> <td>['can' 'make' 'easy' 'to' 'learn?']                         </td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>['experience' 'like?']                                      </td> <td>['experience?']                                             </td> <td>['What' 'was' 'your' 'first' 'sexual']                      </td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>['What' 'would' 'mean' 'for' 'current' 'international' ' ...</td> <td>['How' 'will' 'affect' 'the' 'presently' 'in' 'US' 'or'  ...</td> <td>['a' 'Trump' 'presidency' 'students']                       </td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>['mean?']                                                   </td> <td>['means?']                                                  </td> <td>['What' 'does' 'manipulation']                              </td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>['are' 'so' 'many' 'users' 'posting' 'that' 'are' 'readi ...</td> <td>['do' 'people' 'ask' 'which' 'can' 'be' 'easily' 'by']      </td> <td>['Why' 'Quora' 'questions' 'answered' 'Google?']            </td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>['do' 'look']                                               </td> <td>['are' 'and' 'boosters' 'painted']                          </td> <td>['Why' 'rockets' 'white?']                                  </td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>['I' 'law?']                                                </td> <td>['one' 'know' 'that' 'he/she' 'completely' 'exam?']         </td> <td>['How' 'should' 'prepare' 'for' 'CA' 'final']               </td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>['What' 'are' 'some' 'special' 'cares' 'for' 'someone' ' ...</td> <td>['How' 'can' 'I' 'keep' 'my' 'from' 'getting' 'at']         </td> <td>['nose' 'stuffy' 'night?']                                  </td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>['likely' 'give' 'mercy?']                                  </td> <td>['like' 'at' 'mercy' 'of?']                                 </td> <td>['What' 'Game' 'of' 'Thrones' 'villain' 'would' 'be' 'th ...</td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>['we' 'UPSC?']                                              </td> <td>['I' 'civil' 'service?']                                    </td> <td>['How' 'do' 'prepare' 'for']                                </td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>['examples' 'that' 'can' 'be' 'make']                       </td> <td>['the' 'made']                                              </td> <td>['What' 'are' 'some' 'of' 'products' 'from' 'crude' 'oil?'] </td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>['do' 'I' 'friends.']                                       </td> <td>['to' 'friends' '?']                                        </td> <td>['How' 'make']                                              </td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>['Is' 'Career' 'Launcher' 'good' 'B' 'preparation?']        </td> <td>['How' 'is' 'career' 'launcher' 'online' 'program' 'B?']    </td> <td>['for' 'RBI' 'Grade']                                       </td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>['Will' 'If' 'so,' 'how?']                                  </td> <td>['How' 'can' 'you']                                         </td> <td>['a' 'Blu' 'Ray' 'play' 'on' 'a' 'regular' 'DVD' 'player?'] </td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>['best/most' 'memorable' 'thing']                           </td> <td>['most' 'delicious' 'dish']                                 </td> <td>['What' 'is' 'the' \"you've\" 'ever' 'eaten' 'and' 'why?']    </td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>['was' 'suddenly' 'logged' 'off' 'Gmail.' 'and' 'just' ' ...</td> <td>['or' 'email.' 'How' 'recover' 'e-mail?']                   </td> <td>['I' 'I' \"can't\" 'remember' 'my' 'Gmail' 'password' 'rec ...</td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>['Harry' 'Potter' \"'Harry\" 'Potter' 'and' 'Cursed' \"Chil ...</td> <td>['bad' 'by' 'J.K' 'Rowling?']                               </td> <td>['How' 'is' 'the' 'new' 'book' 'the']                       </td>\n",
       "        </tr>\n",
       "    </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "new_Tb.show(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_Tb.to_csv(\"QQP_Trues\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    [What, is, the, step, by, step, guide, to, inv...\n",
       "1    [What, is, the, story, of, Kohinoor, (Koh-i-No...\n",
       "2    [How, can, I, increase, the, speed, of, my, in...\n",
       "3    [Why, am, I, mentally, very, lonely?, How, can...\n",
       "4    [Which, one, dissolve, in, water, quikly, suga...\n",
       "dtype: object"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#create a dictionary\n",
    "\n",
    "MAX_NUM_WORDS = 20000\n",
    "tokenizer = keras \\\n",
    "    .preprocessing \\\n",
    "    .text \\\n",
    "    .Tokenizer(num_words=MAX_NUM_WORDS)\n",
    "\n",
    "corpus_x1 = quora.title1_tokenized\n",
    "corpus_x2 = quora.title2_tokenized\n",
    "corpus = pd.concat([\n",
    "  corpus_x1, corpus_x2])\n",
    "#corpus.shape\n",
    "tokenizer.fit_on_texts(corpus)\n",
    "x1 = tokenizer \\\n",
    "    .texts_to_sequences(corpus_x1)\n",
    "x2 = tokenizer \\\n",
    "    .texts_to_sequences(corpus_x2)\n",
    "\n",
    "word_index=tokenizer.word_index\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "corpus.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "115"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_seq_len = max([            #why does seq length change when I change dictionary size?\n",
    "    len(seq) for seq in x1])\n",
    "max_seq_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "115\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[    0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     2,     3,     1,  1394,    54,  1394,  2940,\n",
       "            7,   519,     8,   748,   566,     8,    58],\n",
       "       [    0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            2,     3,     1,   749,     9, 18336, 13264],\n",
       "       [    0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     4,    13,     5,   178,     1,   467,     9,\n",
       "           17,   522,  2556,   165,   128,     6,  7726],\n",
       "       [    0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,    16,    66,     5,  3010,\n",
       "          253,  5384,     4,    13,     5,   546,   112],\n",
       "       [    0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,    22,    49,  8304,\n",
       "            8,   275, 19661,    12,  2050, 13805, 13531]], dtype=int32)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_seq_len1 = max([\n",
    "    len(seq) for seq in x1])\n",
    "print(max_seq_len1)   \n",
    "\n",
    "MAX_SEQUENCE_LENGTH = 115  #better to have words covered than uncovered\n",
    "x1 = keras \\\n",
    "    .preprocessing \\\n",
    "    .sequence \\\n",
    "    .pad_sequences(x1, \n",
    "                   maxlen=MAX_SEQUENCE_LENGTH)\n",
    "\n",
    "\n",
    "x2 = keras \\\n",
    "    .preprocessing \\\n",
    "    .sequence \\\n",
    "    .pad_sequences(x2, \n",
    "                   maxlen=MAX_SEQUENCE_LENGTH)\n",
    "\n",
    "x1[:5]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "404290\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [0., 1.],\n",
       "       [1., 0.],\n",
       "       [0., 1.]], dtype=float32)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "dup = quora.is_duplicate\n",
    "\n",
    "y = keras \\\n",
    "    .utils \\\n",
    "    .to_categorical(dup)\n",
    "\n",
    "\n",
    "\n",
    "print(len(quora))\n",
    "\n",
    "y[:8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "\n",
    "def foo():\n",
    "    q=[]\n",
    "    p=0\n",
    "    for x in quora.is_duplicate:\n",
    "        if math.isnan(x):\n",
    "            q = np.append(q, p)\n",
    "        p+=1\n",
    "    return q\n",
    "foo()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 5.,  7., 11.])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We randomly select 5,000 paraphrases and 5,000 non-paraphrases as the dev set, and sample\n",
    "#another 5,000 paraphrases and 5,000 non-paraphrases as the\n",
    "#test set. We keep the remaining instances as the training set 2\n",
    "\n",
    "label = quora.is_duplicate\n",
    "eq_list = []\n",
    "uneq_list=[]\n",
    "\n",
    "count = 0\n",
    "\n",
    "for x in range(len(quora.is_duplicate)):\n",
    "    if label[x]==1:\n",
    "        eq_list = np.append(eq_list, x) \n",
    "    \n",
    "    else:\n",
    "        uneq_list = np.append(uneq_list, x)\n",
    "\n",
    "        \n",
    "eq_list[:3]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "eq_list=eq_list.astype(int)\n",
    "uneq_list=uneq_list.astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([283852, 317624, 403471, 382344, 103100, 362629, 127073,  49140,\n",
       "       164811, 302285])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "random.shuffle(eq_list)\n",
    "random.shuffle(uneq_list)\n",
    "\n",
    "eq_list[:10]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_test_eq = eq_list[:5000]\n",
    "\n",
    "index_test_uneq = uneq_list[:5000]\n",
    "\n",
    "index_dev_eq = eq_list[5000:10000]\n",
    "\n",
    "index_dev_uneq = uneq_list[5000:10000]\n",
    "\n",
    "index_train_eq = eq_list[10000:]\n",
    "\n",
    "index_train_uneq = uneq_list[10000:]\n",
    "\n",
    "index_test = np.append(index_test_eq, index_test_uneq)\n",
    "\n",
    "index_dev = np.append(index_dev_eq, index_dev_uneq)\n",
    "\n",
    "index_train = np.append(index_train_eq, index_train_uneq)\n",
    "\n",
    "random.shuffle(index_test)\n",
    "random.shuffle(index_dev)\n",
    "random.shuffle(index_train)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([338548, 136787,  18905, 365410])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index_test[:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Set\n",
      "----------\n",
      "(384290, 115)\n",
      "(384290, 115)\n",
      "(384290, 2)\n",
      "----------\n",
      "(10000, 115)\n",
      "(10000, 115)\n",
      "(10000, 2)\n",
      "Test Set\n",
      "[array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32)]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "x1_test = [x1[i] for i in index_test]\n",
    "\n",
    "x2_test = [x2[i] for i in index_test]\n",
    "\n",
    "y_test = [y[i] for i in index_test]\n",
    "\n",
    "\n",
    "x1_dev = [x1[i] for i in index_dev]\n",
    "\n",
    "x2_dev = [x2[i] for i in index_dev]\n",
    "\n",
    "y_dev=[y[i] for i in index_dev]\n",
    "\n",
    "\n",
    "x1_train = [x1[i] for i in index_train]\n",
    "\n",
    "x2_train = [x2[i] for i in index_train]\n",
    "\n",
    "y_train=[y[i] for i in index_train]\n",
    "\n",
    "\n",
    "print(\"Training Set\")\n",
    "print(\"-\" * 10)\n",
    "print(np.shape(x1_train))\n",
    "print(np.shape(x2_train))\n",
    "print(np.shape(y_train))\n",
    "\n",
    "\n",
    "print(\"-\" * 10)\n",
    "print(np.shape(x1_test))\n",
    "print(np.shape(x2_test))\n",
    "print(np.shape(y_test))\n",
    "\n",
    "\n",
    "print(\"Test Set\")\n",
    "print(y_test[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/scipy/spatial/distance.py:702: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  dist = 1.0 - uv / np.sqrt(uu * vv)\n"
     ]
    }
   ],
   "source": [
    "#add cos similarity value to last elem of word embedding\n",
    "x1_test_cos=[]\n",
    "x2_test_cos=[]\n",
    "x1_train_cos=[]\n",
    "x2_train_cos=[]\n",
    "x1_dev_cos=[]\n",
    "x2_dev_cos=[]\n",
    "\n",
    "from scipy import spatial\n",
    "\n",
    "def cos_sim(vec1, vec2):\n",
    "    return (1 - spatial.distance.cosine(vec1, vec2))\n",
    "\n",
    "for x in range(len(y_train)):\n",
    "    \n",
    "    \n",
    "    cos_train=cos_sim(x1_train[x], x2_train[x])\n",
    "    \n",
    "    \n",
    "    x1_train_cos.append(np.append(x1_train[x], cos_train))\n",
    "    x2_train_cos.append(np.append(x2_train[x], cos_train))\n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "for x in range(len(y_test)):\n",
    "    cos_dev=cos_sim(x1_dev[x], x2_dev[x])\n",
    "    cos_test=cos_sim(x1_test[x], x2_test[x])\n",
    "    x1_test_cos.append(np.append(x1_test[x], cos_test))\n",
    "    x2_test_cos.append(np.append(x2_test[x], cos_test))\n",
    "    x1_dev_cos.append(np.append(x1_dev[x], cos_dev))\n",
    "    x2_dev_cos.append(np.append(x2_dev[x], cos_dev))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "116"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(x1_test_cos)[:10]\n",
    "len(x1_train_cos[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 116)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_2 (InputLayer)            (None, 116)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)         (None, 116, 300)     60332400    input_1[0][0]                    \n",
      "                                                                 input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_1 (Bidirectional) (None, 256)          439296      embedding_1[0][0]                \n",
      "                                                                 embedding_1[1][0]                \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 512)          0           bidirectional_1[0][0]            \n",
      "                                                                 bidirectional_1[1][0]            \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 2)            1026        concatenate_1[0][0]              \n",
      "==================================================================================================\n",
      "Total params: 60,772,722\n",
      "Trainable params: 440,322\n",
      "Non-trainable params: 60,332,400\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#BiLSTM with GLoVE \n",
    "\n",
    "\n",
    "NUM_CLASSES=2 #boolean; 1 or 0\n",
    "\n",
    "MAX_NUM_WORDS = 20000 #rough estimate\n",
    "\n",
    "MAX_SEQUENCE_LENGTH = 116 #how long one sentence is\n",
    "\n",
    "NUM_EMBEDDING_DIM = 300 #this is arbitrary?\n",
    "\n",
    "NUM_LSTM_UNITS = 128 #output dimension\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "top_input = Input(\n",
    "    shape=(MAX_SEQUENCE_LENGTH, ),    #this is the first sentence\n",
    "    dtype='int32')\n",
    "\n",
    "bm_input = Input(\n",
    "    shape=(MAX_SEQUENCE_LENGTH, ),   #this is the second\n",
    "    dtype='int32')\n",
    "\n",
    "\n",
    "\n",
    "embedding_layer = embedding_layer = Embedding(len(word_index)+1,\n",
    "                            NUM_EMBEDDING_DIM,\n",
    "                            weights=[embedding_matrix],\n",
    "                            input_length=MAX_SEQUENCE_LENGTH,\n",
    "                            trainable=False)\n",
    "top_embedded = embedding_layer(\n",
    "    top_input)\n",
    "bm_embedded = embedding_layer(\n",
    "    bm_input)\n",
    "\n",
    "\n",
    "shared_lstm = Bidirectional(LSTM(NUM_LSTM_UNITS))\n",
    "top_output = shared_lstm(top_embedded)\n",
    "bm_output = shared_lstm(bm_embedded)\n",
    "\n",
    "\n",
    "merged = concatenate(\n",
    "    [top_output, bm_output], \n",
    "    axis=-1)\n",
    "\n",
    "\n",
    "dense =  Dense(\n",
    "    units=NUM_CLASSES, \n",
    "    activation='sigmoid')\n",
    "predictions = dense(merged)\n",
    "\n",
    "\n",
    "\n",
    "model = Model(\n",
    "    inputs=[top_input, bm_input], \n",
    "    outputs=predictions)\n",
    "\n",
    "\n",
    "model.summary()\n",
    "\n",
    "model.compile(\n",
    "    optimizer='Adam',\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=['accuracy'])\n",
    "\n",
    "BATCH_SIZE = 1024\n",
    "\n",
    "NUM_EPOCHS = 15\n",
    "\n",
    "earlyStopping = EarlyStopping(monitor='val_loss', patience=3, verbose=1, mode='min')\n",
    "mcp_save = ModelCheckpoint('quora_weights_with_GLoVE.h5', verbose=1, save_best_only=True, monitor='val_loss', mode='min')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Train on 384290 samples, validate on 10000 samples\n",
      "Epoch 1/15\n",
      "384290/384290 [==============================] - 302s 786us/step - loss: 0.5586 - acc: 0.7119 - val_loss: 0.5984 - val_acc: 0.6803\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.59837, saving model to quora_weights_with_GLoVE.h5\n",
      "Epoch 2/15\n",
      "384290/384290 [==============================] - 288s 749us/step - loss: 0.5126 - acc: 0.7474 - val_loss: 0.5627 - val_acc: 0.7122\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.59837 to 0.56267, saving model to quora_weights_with_GLoVE.h5\n",
      "Epoch 3/15\n",
      "384290/384290 [==============================] - 268s 698us/step - loss: 0.4926 - acc: 0.7602 - val_loss: 0.5487 - val_acc: 0.7209\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.56267 to 0.54866, saving model to quora_weights_with_GLoVE.h5\n",
      "Epoch 4/15\n",
      "384290/384290 [==============================] - 268s 697us/step - loss: 0.4759 - acc: 0.7709 - val_loss: 0.5332 - val_acc: 0.7316\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.54866 to 0.53319, saving model to quora_weights_with_GLoVE.h5\n",
      "Epoch 5/15\n",
      "384290/384290 [==============================] - 271s 706us/step - loss: 0.4613 - acc: 0.7797 - val_loss: 0.5396 - val_acc: 0.7332\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.53319\n",
      "Epoch 6/15\n",
      "384290/384290 [==============================] - 273s 711us/step - loss: 0.4471 - acc: 0.7881 - val_loss: 0.5239 - val_acc: 0.7382\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.53319 to 0.52392, saving model to quora_weights_with_GLoVE.h5\n",
      "Epoch 7/15\n",
      "384290/384290 [==============================] - 269s 700us/step - loss: 0.4381 - acc: 0.7937 - val_loss: 0.5263 - val_acc: 0.7358\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.52392\n",
      "Epoch 8/15\n",
      "384290/384290 [==============================] - 267s 696us/step - loss: 0.4216 - acc: 0.8022 - val_loss: 0.5187 - val_acc: 0.7443\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.52392 to 0.51868, saving model to quora_weights_with_GLoVE.h5\n",
      "Epoch 9/15\n",
      "384290/384290 [==============================] - 268s 698us/step - loss: 0.4087 - acc: 0.8090 - val_loss: 0.5292 - val_acc: 0.7436\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.51868\n",
      "Epoch 10/15\n",
      "384290/384290 [==============================] - 267s 694us/step - loss: 0.3952 - acc: 0.8165 - val_loss: 0.5305 - val_acc: 0.7473\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.51868\n",
      "Epoch 11/15\n",
      "384290/384290 [==============================] - 265s 690us/step - loss: 0.3839 - acc: 0.8221 - val_loss: 0.5290 - val_acc: 0.7517\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 0.51868\n",
      "Epoch 00011: early stopping\n"
     ]
    }
   ],
   "source": [
    "\n",
    "history = model.fit(x=([np.array(x1_train_cos), np.array(x2_train_cos)]), y=np.array(y_train), batch_size=BATCH_SIZE, epochs=NUM_EPOCHS, verbose=1, callbacks=[earlyStopping, mcp_save], validation_data=([np.array(x1_dev_cos), np.array(x2_dev_cos)], np.array(y_dev)),\n",
    "    \n",
    "    shuffle=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 384290 samples, validate on 10000 samples\n",
      "Epoch 1/15\n",
      "384290/384290 [==============================] - 301s 783us/step - loss: 0.5399 - acc: 0.7276 - val_loss: 0.5748 - val_acc: 0.6979\n",
      "Epoch 2/15\n",
      "384290/384290 [==============================] - 300s 779us/step - loss: 0.5068 - acc: 0.7511 - val_loss: 0.5521 - val_acc: 0.7143\n",
      "Epoch 3/15\n",
      "384290/384290 [==============================] - 299s 777us/step - loss: 0.4878 - acc: 0.7637 - val_loss: 0.5325 - val_acc: 0.7257\n",
      "Epoch 4/15\n",
      "384290/384290 [==============================] - 303s 789us/step - loss: 0.4697 - acc: 0.7747 - val_loss: 0.5322 - val_acc: 0.7318\n",
      "Epoch 5/15\n",
      "384290/384290 [==============================] - 295s 769us/step - loss: 0.4547 - acc: 0.7835 - val_loss: 0.5230 - val_acc: 0.7360\n",
      "Epoch 6/15\n",
      "384290/384290 [==============================] - 302s 785us/step - loss: 0.4395 - acc: 0.7930 - val_loss: 0.5301 - val_acc: 0.7397\n",
      "Epoch 7/15\n",
      "384290/384290 [==============================] - 303s 789us/step - loss: 0.4250 - acc: 0.8020 - val_loss: 0.5302 - val_acc: 0.7402\n",
      "Epoch 8/15\n",
      "384290/384290 [==============================] - 300s 781us/step - loss: 0.4116 - acc: 0.8091 - val_loss: 0.5466 - val_acc: 0.7330\n",
      "Epoch 00008: early stopping\n"
     ]
    }
   ],
   "source": [
    "\n",
    "history = model.fit(x=([np.array(x1_train), np.array(x2_train)]), y=np.array(y_train), batch_size=BATCH_SIZE, epochs=NUM_EPOCHS, verbose=1, callbacks=[earlyStopping, mcp_save], validation_data=([np.array(x1_dev), np.array(x2_dev)], np.array(y_dev)),\n",
    "    \n",
    "    shuffle=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = load_model(\"quora_weights_with_GLoVE.h5\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicts = model.predict(\n",
    "    [x1_test, x2_test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1., 0.], dtype=float32)"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7455\n"
     ]
    }
   ],
   "source": [
    "one = Tb.column('question1')\n",
    "two = Tb.column('question2')\n",
    "no = []\n",
    "err1 = []\n",
    "err2 = []\n",
    "wrongCat = []\n",
    "\n",
    "\n",
    "for x in range(len(index_test)):\n",
    "    if (np.argmax(predicts[x])!=(np.argmax(y_test[x]))):\n",
    "        no=np.append(no, index_test[x])\n",
    "        err1=np.append(err1, one[index_test[x]])\n",
    "        err2=np.append(err2, two[index_test[x]])\n",
    "        wrongCat = np.append(wrongCat, np.argmax(y_test[x]))\n",
    "        \n",
    "print(1-(len(wrongCat)/len(x1_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "    <thead>\n",
       "        <tr>\n",
       "            <th>id</th> <th>sent1</th> <th>sent2</th> <th>classification</th>\n",
       "        </tr>\n",
       "    </thead>\n",
       "    <tbody>\n",
       "        <tr>\n",
       "            <td>117573</td> <td>What was your favourite subject at school and why?          </td> <td>What is/was your favourite subject at school and why?       </td> <td>1             </td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>357893</td> <td>Suppose Host A sends two TCP segments back to back to Ho ...</td> <td>Suppose Host A sends two TCP segments back to back to Ho ...</td> <td>1             </td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>221481</td> <td>Will I always find chess easier than snooker?               </td> <td>Why do I find chess easier than snooker?                    </td> <td>0             </td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>187484</td> <td>What is the career of photography?                          </td> <td>What's necessary for a successful career in photography?    </td> <td>1             </td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>85852 </td> <td>Could we create an inmortal and indestructible brain usi ...</td> <td>Could we create an indestructible and inmortal brain wit ...</td> <td>1             </td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>307703</td> <td>What are good youtube channels that explain informative  ...</td> <td>What are the useful YouTube channels to gain knowledge?     </td> <td>0             </td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>182829</td> <td>Will I be in trouble if I download movies from torrents  ...</td> <td>In present days download movies from torrent is a crimin ...</td> <td>1             </td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>127301</td> <td>What is the best way to retain what you learn?              </td> <td>What are the best ways to improve my memory?                </td> <td>0             </td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>278486</td> <td>Why doesn't the police or the government help the child  ...</td> <td>Why doesn't the police or the government take actions ag ...</td> <td>1             </td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>203535</td> <td>In what ways is Australia better or different than America? </td> <td>How is living in Australia different from living in the  ...</td> <td>1             </td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>354377</td> <td>Is the second coming near?                                  </td> <td>Will the second coming of Jesus happen in the near future?  </td> <td>1             </td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>400515</td> <td>Is it really possible to lose fat and gain muscle at the ...</td> <td>How can I lower my body fat, while gaining lean muscles  ...</td> <td>0             </td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>115238</td> <td>What is feminism?                                           </td> <td>Can you explain feminism?                                   </td> <td>1             </td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>41602 </td> <td>Which battle has the most casualties in history?            </td> <td>What was the bloodiest battle in history?                   </td> <td>1             </td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>14799 </td> <td>Approximately how much does a Chevy G10 van weigh? Model ...</td> <td>Approximately how much does a Chevy G10 van weigh? Model ...</td> <td>1             </td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>342216</td> <td>What is the worst thing you can do to yourself?             </td> <td>What is the worst thing you have ever done to someone be ...</td> <td>0             </td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>176450</td> <td>Who exactly was Fidel Castro?                               </td> <td>Who is Fidel Castro?                                        </td> <td>1             </td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>97304 </td> <td>What is SIP (Systematic Investment Plan)?                   </td> <td>What is SIP or systematic investment plan?                  </td> <td>1             </td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>278416</td> <td>What is the function of VPN in an iPhone?                   </td> <td>What does VPN do on my iPhone?                              </td> <td>1             </td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>174193</td> <td>What lessons, if any, have you learned from the movie \"I ...</td> <td>What do I learn from the movie 'into the wild'?             </td> <td>1             </td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>48134 </td> <td>Why should I study physics?                                 </td> <td>Why do people study physics?                                </td> <td>1             </td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>177400</td> <td>How hard are Google's software engineer job interviews?     </td> <td>What should I expect in a Software Engineer interview at ...</td> <td>1             </td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>252269</td> <td>Who contributed to Indian independence more, Gandhi or S ...</td> <td>Who contributed to Indian independence more, Gandhi or S ...</td> <td>1             </td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>145437</td> <td>What's the best thing to do in life?                        </td> <td>What's the best thing in life?                              </td> <td>1             </td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>284760</td> <td>Why is \"step-daughter\" not included in the definition of ...</td> <td>Why is 'Step daughter' not been included in the definati ...</td> <td>1             </td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>255731</td> <td>How can Deadpool beat Goku?                                 </td> <td>Who would win in a fight: Deadpool or Goku?                 </td> <td>0             </td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>153359</td> <td>How do I know the name of the person the SIM card is reg ...</td> <td>How do I know on which name a particular SIM card is reg ...</td> <td>1             </td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>243324</td> <td>Is it too late to start learn programming again?            </td> <td>Is it too late to start learning programming at 30?         </td> <td>0             </td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>297316</td> <td>What Deve Gowda did for the country when he was prime mi ...</td> <td>What has Deve Gowda done as the PM of India?                </td> <td>1             </td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>40835 </td> <td>How does a neural turing machine work?                      </td> <td>How can Turing machine works?                               </td> <td>1             </td>\n",
       "        </tr>\n",
       "    </tbody>\n",
       "</table>\n",
       "<p>... (2515 rows omitted)</p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "error_analysis = Table()\n",
    "\n",
    "\n",
    "error_analysis=error_analysis.with_column(\"id\", no, \"sent1\", err1, \"sent2\", err2, \"classification\", wrongCat)\n",
    "error_analysis.show(30)\n",
    "           \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "    <thead>\n",
       "        <tr>\n",
       "            <th>id</th> <th>sent1</th> <th>sent2</th> <th>classification</th>\n",
       "        </tr>\n",
       "    </thead>\n",
       "    <tbody>\n",
       "        <tr>\n",
       "            <td>23  </td> <td>Does marijuana make you gain or lose weight?            </td> <td>Does marijuana make you lose weight?                        </td> <td>0             </td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>46  </td> <td>How can I learn online trading in india?                </td> <td>How can I do online trading in india?                       </td> <td>0             </td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>51  </td> <td>What if the 1802 Peace of Amiens would have been upheld?</td> <td>What is the best thing you have ever done to achieve inn ...</td> <td>0             </td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>53  </td> <td>What are some overrated firearms, and why?              </td> <td>Which historical figure is overrated?                       </td> <td>0             </td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>95  </td> <td>Pauls pemdulum theory?                                  </td> <td>With the modern technology, what are some impending dang ...</td> <td>0             </td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>114 </td> <td>How do I cope with major depression?                    </td> <td>How do you cope with depression?                            </td> <td>0             </td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>220 </td> <td>Is PESIT South campus mechanical really that bad?       </td> <td>Is PESIT south campus really bad?                           </td> <td>0             </td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>307 </td> <td>Who is likely to become the next President of India?    </td> <td>What should I do to become the next president of India?     </td> <td>0             </td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>324 </td> <td>How can India and China be friends?                     </td> <td>Should India join hands with China?                         </td> <td>0             </td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>397 </td> <td>How do I improve leadreship skills?                     </td> <td>How can we improve learning skills?                         </td> <td>0             </td>\n",
       "        </tr>\n",
       "    </tbody>\n",
       "</table>\n",
       "<p>... (12934 rows omitted)</p>"
      ],
      "text/plain": [
       "id   | sent1                                                    | sent2                                                        | classification\n",
       "23   | Does marijuana make you gain or lose weight?             | Does marijuana make you lose weight?                         | 0\n",
       "46   | How can I learn online trading in india?                 | How can I do online trading in india?                        | 0\n",
       "51   | What if the 1802 Peace of Amiens would have been upheld? | What is the best thing you have ever done to achieve inn ... | 0\n",
       "53   | What are some overrated firearms, and why?               | Which historical figure is overrated?                        | 0\n",
       "95   | Pauls pemdulum theory?                                   | With the modern technology, what are some impending dang ... | 0\n",
       "114  | How do I cope with major depression?                     | How do you cope with depression?                             | 0\n",
       "220  | Is PESIT South campus mechanical really that bad?        | Is PESIT south campus really bad?                            | 0\n",
       "307  | Who is likely to become the next President of India?     | What should I do to become the next president of India?      | 0\n",
       "324  | How can India and China be friends?                      | Should India join hands with China?                          | 0\n",
       "397  | How do I improve leadreship skills?                      | How can we improve learning skills?                          | 0\n",
       "... (12934 rows omitted)"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hue = error_analysis.where(\"classification\", are.equal_to(0)) \n",
    "hue\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2195895 word vectors.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "print('Found %s word vectors.' % len(glove))\n",
    "\n",
    "NUM_EMBEDDING_DIM = 300\n",
    "\n",
    "embedding_matrix = np.zeros((len(word_index)+1, NUM_EMBEDDING_DIM))\n",
    "\n",
    "\n",
    "for word, i in word_index.items():\n",
    "    embedding_vector = glove.get(word)\n",
    "    \n",
    "    if embedding_vector is not None:\n",
    "        # words not found in embedding index will be all-zeros.\n",
    "        embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_glove_line(line, dim):\n",
    "    word = None\n",
    "    embedding = None\n",
    "\n",
    "    try:\n",
    "        splitLine = line.split()\n",
    "        word = \" \".join(splitLine[:len(splitLine)-dim])\n",
    "        embedding = np.array([float(val) for val in splitLine[-dim:]])\n",
    "    except:\n",
    "        print(line)\n",
    "\n",
    "    return word, embedding\n",
    "\n",
    "def load_glove_model(glove_filepath, dim):\n",
    "    with open(glove_filepath, encoding=\"utf8\" ) as f:\n",
    "        content = f.readlines()\n",
    "        model = {}\n",
    "        for line in content:\n",
    "            word, embedding = process_glove_line(line, dim)\n",
    "            if embedding is not None:\n",
    "                model[word] = embedding\n",
    "        return model\n",
    "\n",
    "glove = load_glove_model(\"glove.840B.300d.txt\", 300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = load_model('BiLSTM_QQP.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
