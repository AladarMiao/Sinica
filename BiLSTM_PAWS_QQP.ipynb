{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib\n",
    "import argparse\n",
    "import random\n",
    "import numpy as np\n",
    "import csv\n",
    "import pandas as pd\n",
    "from datascience import *\n",
    "import tensorflow as tf\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from keras.models import Model\n",
    "from keras.models import Sequential, load_model\n",
    "\n",
    "from keras.layers import *\n",
    "from keras.optimizers import Adam\n",
    "from keras import Input\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.utils import plot_model\n",
    "import keras\n",
    "#from keras.backend.tensorflow_backend import set_session\n",
    "#config = tf.ConfigProto()\n",
    "# config.gpu_options.allocator_type = 'BFC' #A \"Best-fit with coalescing\" algorithm, simplified from a version of dlmalloc.\n",
    "# config.gpu_options.per_process_gpu_memory_fraction = 0.7\n",
    "#config.gpu_options.allow_growth =True\n",
    "\n",
    "#set_session(tf.Session(config=config)) \n",
    "import re\n",
    "os.environ['PYTHONHASHSEED'] = '0'\n",
    "random.seed(1)\n",
    "np.random.seed(1)\n",
    "tf.set_random_seed(1)\n",
    "from keras.callbacks import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sanitize(string):\n",
    "    words = string.split(' ')\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   id                                          sentence1  \\\n",
      "0   1  b'Will a message still say blocked if you were...   \n",
      "1   2  b'How can you treat ocd ? Is there any helpful...   \n",
      "2   3  b'If you do not do anything how you are motiva...   \n",
      "3   4  b'Why is new in system verily constructor not ...   \n",
      "4   5  b'What are the most common traffic convictions...   \n",
      "\n",
      "                                           sentence2  label  \n",
      "0  b'Will a message still say delivered if you we...      0  \n",
      "1  b'How can you treat OCD ? Is there any helpful...      1  \n",
      "2  b'If you do not seek anything how you are moti...      0  \n",
      "3  b'Why constructor new in system verilog is not...      0  \n",
      "4  b'What are the most common traffic convictions...      0  \n"
     ]
    }
   ],
   "source": [
    "#read csv\n",
    "\n",
    "train = pd.read_csv(\"PAWS_QQP_train.tsv\", sep='\\t')\n",
    "dev = pd.read_csv(\"PAWS_QQP_dev_and_test.tsv\", sep='\\t')\n",
    "test = pd.read_csv(\"PAWS_QQP_dev_and_test.tsv\", sep='\\t')\n",
    "\n",
    "print(train[:5])\n",
    "\n",
    "train['title1_tokenized'] = \\\n",
    "    train.loc[:, 'sentence1'] \\\n",
    "         .apply(sanitize)\n",
    "train['title2_tokenized'] = \\\n",
    "    train.loc[:, 'sentence2'] \\\n",
    "         .apply(sanitize)\n",
    "\n",
    "\n",
    "dev['title1_tokenized'] = \\\n",
    "    dev.loc[:, 'sentence1'] \\\n",
    "         .apply(sanitize)\n",
    "dev['title2_tokenized'] = \\\n",
    "    dev.loc[:, 'sentence2'] \\\n",
    "         .apply(sanitize)\n",
    "\n",
    "\n",
    "test['title1_tokenized'] = \\\n",
    "    test.loc[:, 'sentence1'] \\\n",
    "         .apply(sanitize)\n",
    "test['title2_tokenized'] = \\\n",
    "    test.loc[:, 'sentence2'] \\\n",
    "         .apply(sanitize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23726\n",
      "26402\n"
     ]
    }
   ],
   "source": [
    "MAX_NUM_WORDS = 10000\n",
    "tokenizer = keras \\\n",
    "    .preprocessing \\\n",
    "    .text \\\n",
    "    .Tokenizer(num_words=MAX_NUM_WORDS)\n",
    "\n",
    "train_corpus_x1 = train.title1_tokenized\n",
    "train_corpus_x2 = train.title2_tokenized\n",
    "train_corpus = pd.concat([\n",
    "    train_corpus_x1, train_corpus_x2])\n",
    "#corpus.shape\n",
    "tokenizer.fit_on_texts(train_corpus)\n",
    "tokenizer.texts_to_sequences(train_corpus)\n",
    "x1_train = tokenizer \\\n",
    "    .texts_to_sequences(train_corpus_x1)\n",
    "x2_train = tokenizer \\\n",
    "    .texts_to_sequences(train_corpus_x2)\n",
    "\n",
    "\n",
    "print(tokenizer.document_count)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "dev_corpus_x1 = dev.title1_tokenized\n",
    "dev_corpus_x2 = dev.title2_tokenized\n",
    "dev_corpus = pd.concat([\n",
    "    dev_corpus_x1, dev_corpus_x2])\n",
    "#corpus.shape\n",
    "tokenizer.fit_on_texts(dev_corpus)\n",
    "tokenizer.texts_to_sequences(dev_corpus)\n",
    "x1_dev = tokenizer \\\n",
    "    .texts_to_sequences(dev_corpus_x1)\n",
    "x2_dev = tokenizer \\\n",
    "    .texts_to_sequences(dev_corpus_x2)\n",
    "\n",
    "\n",
    "test_corpus_x1 = test.title1_tokenized\n",
    "test_corpus_x2 = test.title2_tokenized\n",
    "test_corpus = pd.concat([\n",
    "    test_corpus_x1, test_corpus_x2])\n",
    "#corpus.shape\n",
    "tokenizer.fit_on_texts(test_corpus)\n",
    "tokenizer.texts_to_sequences(test_corpus)\n",
    "x1_test = tokenizer \\\n",
    "    .texts_to_sequences(test_corpus_x1)\n",
    "x2_test = tokenizer \\\n",
    "    .texts_to_sequences(test_corpus_x2)\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "\n",
    "\n",
    "print(tokenizer.document_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50\n"
     ]
    }
   ],
   "source": [
    "max_seq_len1 = max([\n",
    "    len(seq) for seq in x1_train])\n",
    "print(max_seq_len1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,   11,  153,    2,  265,  521,   10,\n",
       "           2, 1441,  758,    8,    5,   30,   12,  236,  521,  112,    3,\n",
       "           2, 4981,  758,    6, 4982,    1],\n",
       "       [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0, 1850,\n",
       "         215,    9,  101,  689,  297, 4957,   71,    5,    9,  120,   47,\n",
       "         108,   28,   54, 1034,   84,   71,   52,   50,    9,  166,  259,\n",
       "         108,  718,   71,  108, 1037,   71,    5,  853,   71,   20,   59,\n",
       "          12,    9,  120,   79,  103,   19],\n",
       "       [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,  122,   12,    9,   96,   26,\n",
       "          93,  209,  146,   41,  690, 2899,   50,    9,   12,   54,   29,\n",
       "         752,    3,   26,  185,  146,   34,   81,    7,    2,  690, 2899,\n",
       "          57, 1030,   35,   32,  442,   19]], dtype=int32)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "MAX_SEQUENCE_LENGTH = 50  #better to have words covered than uncovered\n",
    "x1_train = keras \\\n",
    "    .preprocessing \\\n",
    "    .sequence \\\n",
    "    .pad_sequences(x1_train, \n",
    "                   maxlen=MAX_SEQUENCE_LENGTH)\n",
    "\n",
    "\n",
    "x2_train = keras \\\n",
    "    .preprocessing \\\n",
    "    .sequence \\\n",
    "    .pad_sequences(x2_train, \n",
    "                   maxlen=MAX_SEQUENCE_LENGTH)\n",
    "\n",
    "x1_dev = keras \\\n",
    "    .preprocessing \\\n",
    "    .sequence \\\n",
    "    .pad_sequences(x1_dev, \n",
    "                   maxlen=MAX_SEQUENCE_LENGTH)\n",
    "\n",
    "\n",
    "x2_dev = keras \\\n",
    "    .preprocessing \\\n",
    "    .sequence \\\n",
    "    .pad_sequences(x2_dev, \n",
    "                   maxlen=MAX_SEQUENCE_LENGTH)\n",
    "\n",
    "\n",
    "x1_test = keras \\\n",
    "    .preprocessing \\\n",
    "    .sequence \\\n",
    "    .pad_sequences(x1_test, \n",
    "                   maxlen=MAX_SEQUENCE_LENGTH)\n",
    "\n",
    "x2_test = keras \\\n",
    "    .preprocessing \\\n",
    "    .sequence \\\n",
    "    .pad_sequences(x2_test, \n",
    "                   maxlen=MAX_SEQUENCE_LENGTH)\n",
    "\n",
    "\n",
    "y_train = train.label\n",
    "\n",
    "y_train = keras \\\n",
    "    .utils \\\n",
    "    .to_categorical(y_train, num_classes=2)\n",
    "\n",
    "\n",
    "y_dev = dev.label\n",
    "\n",
    "y_dev = keras \\\n",
    "    .utils \\\n",
    "    .to_categorical(y_dev, num_classes=2)\n",
    "\n",
    "\n",
    "\n",
    "y_test = test.label\n",
    "\n",
    "y_test = keras \\\n",
    "    .utils \\\n",
    "    .to_categorical(y_test, num_classes=2)\n",
    "\n",
    "x1_test[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 50)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_2 (InputLayer)            (None, 50)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)         (None, 50, 256)      2560000     input_1[0][0]                    \n",
      "                                                                 input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_1 (Bidirectional) (None, 256)          394240      embedding_1[0][0]                \n",
      "                                                                 embedding_1[1][0]                \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 512)          0           bidirectional_1[0][0]            \n",
      "                                                                 bidirectional_1[1][0]            \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 2)            1026        concatenate_1[0][0]              \n",
      "==================================================================================================\n",
      "Total params: 2,955,266\n",
      "Trainable params: 2,955,266\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#BiLSTM?\n",
    "NUM_CLASSES=2 #boolean; 1 or 0\n",
    "\n",
    "#rough estimate\n",
    "\n",
    "MAX_SEQUENCE_LENGTH = 50 #how long one sentence is\n",
    "\n",
    "#this is arbitrary?\n",
    "\n",
    "NUM_LSTM_UNITS = 128 #output dimension\n",
    "\n",
    "\n",
    "NUM_EMBEDDING_DIM = 256\n",
    "\n",
    "top_input = Input(\n",
    "    shape=(MAX_SEQUENCE_LENGTH, ),    #this is the first sentence\n",
    "    dtype='int32')\n",
    "\n",
    "bm_input = Input(\n",
    "    shape=(MAX_SEQUENCE_LENGTH, ),   #this is the second\n",
    "    dtype='int32')\n",
    "\n",
    "\n",
    "embedding_layer = Embedding(\n",
    "    MAX_NUM_WORDS, NUM_EMBEDDING_DIM)\n",
    "\n",
    "top_embedded = embedding_layer(\n",
    "    top_input)\n",
    "bm_embedded = embedding_layer(\n",
    "    bm_input)\n",
    "\n",
    "\n",
    "shared_lstm = Bidirectional(LSTM(NUM_LSTM_UNITS))\n",
    "top_output = shared_lstm(top_embedded)\n",
    "bm_output = shared_lstm(bm_embedded)\n",
    "\n",
    "\n",
    "merged = concatenate(\n",
    "    [top_output, bm_output], \n",
    "    axis=-1)\n",
    "\n",
    "\n",
    "dense =  Dense(\n",
    "    units=NUM_CLASSES, \n",
    "    activation='sigmoid')\n",
    "predictions = dense(merged)\n",
    "\n",
    "\n",
    "\n",
    "model = Model(\n",
    "    inputs=[top_input, bm_input], \n",
    "    outputs=predictions)\n",
    "\n",
    "\n",
    "model.summary()\n",
    "\n",
    "model.compile(\n",
    "    optimizer='Adam',\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=['accuracy'])\n",
    "\n",
    "BATCH_SIZE = 512\n",
    "\n",
    "NUM_EPOCHS = 15\n",
    "\n",
    "earlyStopping = EarlyStopping(monitor='val_loss', patience=10, verbose=0, mode='min')\n",
    "mcp_save = ModelCheckpoint('BiLSTM_PAWS_QQP.h5', save_best_only=True, monitor='val_loss', mode='min')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fri Jun 21 00:36:16 2019       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 418.56       Driver Version: 418.56       CUDA Version: 10.1     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  GeForce GTX 1080    Off  | 00000000:02:00.0 Off |                  N/A |\n",
      "|  0%   47C    P2    37W / 200W |    185MiB /  8118MiB |      0%      Default |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                       GPU Memory |\n",
      "|  GPU       PID   Type   Process name                             Usage      |\n",
      "|=============================================================================|\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 11863 samples, validate on 669 samples\n",
      "Epoch 1/15\n",
      "11863/11863 [==============================] - 11s 936us/step - loss: 0.6276 - acc: 0.6817 - val_loss: 0.5632 - val_acc: 0.7220\n",
      "Epoch 2/15\n",
      "11863/11863 [==============================] - 8s 646us/step - loss: 0.5310 - acc: 0.7381 - val_loss: 0.5440 - val_acc: 0.7489\n",
      "Epoch 3/15\n",
      "11863/11863 [==============================] - 8s 646us/step - loss: 0.4195 - acc: 0.8065 - val_loss: 0.6154 - val_acc: 0.7220\n",
      "Epoch 4/15\n",
      "11863/11863 [==============================] - 8s 647us/step - loss: 0.3324 - acc: 0.8576 - val_loss: 0.7137 - val_acc: 0.6816\n",
      "Epoch 5/15\n",
      "11863/11863 [==============================] - 8s 645us/step - loss: 0.2545 - acc: 0.8989 - val_loss: 0.8329 - val_acc: 0.6786\n",
      "Epoch 6/15\n",
      "11863/11863 [==============================] - 8s 647us/step - loss: 0.1906 - acc: 0.9244 - val_loss: 1.1302 - val_acc: 0.6218\n",
      "Epoch 7/15\n",
      "11863/11863 [==============================] - 8s 659us/step - loss: 0.1467 - acc: 0.9450 - val_loss: 1.1623 - val_acc: 0.6682\n",
      "Epoch 8/15\n",
      "11863/11863 [==============================] - 8s 649us/step - loss: 0.1037 - acc: 0.9622 - val_loss: 1.3711 - val_acc: 0.6398\n",
      "Epoch 9/15\n",
      "11863/11863 [==============================] - 8s 646us/step - loss: 0.0815 - acc: 0.9718 - val_loss: 1.6335 - val_acc: 0.6368\n",
      "Epoch 10/15\n",
      "11863/11863 [==============================] - 8s 645us/step - loss: 0.0611 - acc: 0.9779 - val_loss: 1.7554 - val_acc: 0.6592\n",
      "Epoch 11/15\n",
      "11863/11863 [==============================] - 8s 645us/step - loss: 0.0436 - acc: 0.9857 - val_loss: 2.0334 - val_acc: 0.6472\n",
      "Epoch 12/15\n",
      "11863/11863 [==============================] - 8s 645us/step - loss: 0.0345 - acc: 0.9883 - val_loss: 2.1575 - val_acc: 0.6697\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(x=[x1_train, x2_train], y=y_train, batch_size=BATCH_SIZE, epochs=NUM_EPOCHS, callbacks=[earlyStopping, mcp_save], validation_data=([x1_dev, x2_dev], y_dev),\n",
    "    \n",
    "    shuffle=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n"
     ]
    }
   ],
   "source": [
    "model = load_model('BiLSTM_PAWS_QQP.h5')\n",
    "predicts = model.predict(\n",
    "    [x1_test, x2_test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "194\n"
     ]
    }
   ],
   "source": [
    "real1=test.sentence1\n",
    "real2=test.sentence2\n",
    "realCat=test.label\n",
    "no = []\n",
    "err1 = []\n",
    "err2 = []\n",
    "wrongCat = []\n",
    "\n",
    "\n",
    "for x in range(669):\n",
    "    if (np.argmax(predicts[x])!=(test.label[x])):\n",
    "        no=np.append(no, x)\n",
    "        err1=np.append(err1, real1[x])\n",
    "        err2=np.append(err2, real2[x])\n",
    "        wrongCat = np.append(wrongCat, realCat[x])\n",
    "print(len(wrongCat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "    <thead>\n",
       "        <tr>\n",
       "            <th>sent1</th> <th>sent2</th>\n",
       "        </tr>\n",
       "    </thead>\n",
       "    <tbody>\n",
       "        <tr>\n",
       "            <td>b'What are the driving rules in Georgia versus Mississip ...</td> <td>b'What are the driving rules in Mississippi versus Georg ...</td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>b\"Does AT & T have any specific plans for DirecTV 's ROO ...</td> <td>b\"Does DirecTV have any specific plans for AT & T 's ROO ...</td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>b'How do I start learning electronic music production ?  ...</td> <td>b'How do I begin learning electronic music production ?  ...</td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>b'What are the differences between abiotic factors and b ...</td> <td>b'What are the differences between biotic factors and ab ...</td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>b'What is actual meaning of life ? Indeen , it depend on ...</td> <td>b'What is other meaning of life ? Indeen , it depend on  ...</td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>b'How safe are Indian Rs 500 and Rs 2000 new currency no ...</td> <td>b'How safe is new Rs 500 and Rs 2000 Indian currency not ...</td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>b'Why do some people write with their left hand and some ...</td> <td>b'Why do some people write with their right hand and som ...</td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>b'Do Chinese/Korean people think they look like Japanese ...</td> <td>b'Do Japanese people think they look like Chinese/Korean ...</td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>b\"`` Are `` if I were ... `` and `` if I was ... `` both ...</td> <td>b\"Are `` if I was '' and `` if I were '' both grammatica ...</td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>b'Where can I find French film or tv shows with french s ...</td> <td>b'Where can I find french film or tv shows with French s ...</td>\n",
       "        </tr>\n",
       "    </tbody>\n",
       "</table>\n",
       "<p>... (184 rows omitted)</p>"
      ],
      "text/plain": [
       "sent1                                                        | sent2\n",
       "b'What are the driving rules in Georgia versus Mississip ... | b'What are the driving rules in Mississippi versus Georg ...\n",
       "b\"Does AT & T have any specific plans for DirecTV 's ROO ... | b\"Does DirecTV have any specific plans for AT & T 's ROO ...\n",
       "b'How do I start learning electronic music production ?  ... | b'How do I begin learning electronic music production ?  ...\n",
       "b'What are the differences between abiotic factors and b ... | b'What are the differences between biotic factors and ab ...\n",
       "b'What is actual meaning of life ? Indeen , it depend on ... | b'What is other meaning of life ? Indeen , it depend on  ...\n",
       "b'How safe are Indian Rs 500 and Rs 2000 new currency no ... | b'How safe is new Rs 500 and Rs 2000 Indian currency not ...\n",
       "b'Why do some people write with their left hand and some ... | b'Why do some people write with their right hand and som ...\n",
       "b'Do Chinese/Korean people think they look like Japanese ... | b'Do Japanese people think they look like Chinese/Korean ...\n",
       "b\"`` Are `` if I were ... `` and `` if I was ... `` both ... | b\"Are `` if I was '' and `` if I were '' both grammatica ...\n",
       "b'Where can I find French film or tv shows with french s ... | b'Where can I find french film or tv shows with French s ...\n",
       "... (184 rows omitted)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "error_analysis = Table()\n",
    "\n",
    "\n",
    "error_analysis.with_column(\"sent1\", err1, \"sent2\", err2,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
