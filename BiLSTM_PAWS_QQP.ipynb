{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib\n",
    "import argparse\n",
    "import random\n",
    "import numpy as np\n",
    "import csv\n",
    "import pandas as pd\n",
    "from datascience import *\n",
    "import tensorflow as tf\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from keras.models import Model\n",
    "from keras.models import Sequential\n",
    "from keras.layers import *\n",
    "from keras.optimizers import Adam\n",
    "from keras import Input\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.utils import plot_model\n",
    "import keras\n",
    "#from keras.backend.tensorflow_backend import set_session\n",
    "#config = tf.ConfigProto()\n",
    "# config.gpu_options.allocator_type = 'BFC' #A \"Best-fit with coalescing\" algorithm, simplified from a version of dlmalloc.\n",
    "# config.gpu_options.per_process_gpu_memory_fraction = 0.7\n",
    "#config.gpu_options.allow_growth =True\n",
    "\n",
    "#set_session(tf.Session(config=config)) \n",
    "import re\n",
    "os.environ['PYTHONHASHSEED'] = '0'\n",
    "random.seed(1)\n",
    "np.random.seed(1)\n",
    "tf.set_random_seed(1)\n",
    "from keras.callbacks import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sanitize(string):\n",
    "    words = string.split(' ')\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   id                                          sentence1  \\\n",
      "0   1  b'Will a message still say blocked if you were...   \n",
      "1   2  b'How can you treat ocd ? Is there any helpful...   \n",
      "2   3  b'If you do not do anything how you are motiva...   \n",
      "3   4  b'Why is new in system verily constructor not ...   \n",
      "4   5  b'What are the most common traffic convictions...   \n",
      "\n",
      "                                           sentence2  label  \n",
      "0  b'Will a message still say delivered if you we...      0  \n",
      "1  b'How can you treat OCD ? Is there any helpful...      1  \n",
      "2  b'If you do not seek anything how you are moti...      0  \n",
      "3  b'Why constructor new in system verilog is not...      0  \n",
      "4  b'What are the most common traffic convictions...      0  \n"
     ]
    }
   ],
   "source": [
    "#read csv\n",
    "\n",
    "train = pd.read_csv(\"PAWS_QQP_train.tsv\", sep='\\t')\n",
    "dev = pd.read_csv(\"PAWS_QQP_dev_and_test.tsv\", sep='\\t')\n",
    "test = pd.read_csv(\"PAWS_QQP_dev_and_test.tsv\", sep='\\t')\n",
    "\n",
    "print(train[:5])\n",
    "\n",
    "train['title1_tokenized'] = \\\n",
    "    train.loc[:, 'sentence1'] \\\n",
    "         .apply(sanitize)\n",
    "train['title2_tokenized'] = \\\n",
    "    train.loc[:, 'sentence2'] \\\n",
    "         .apply(sanitize)\n",
    "\n",
    "\n",
    "dev['title1_tokenized'] = \\\n",
    "    dev.loc[:, 'sentence1'] \\\n",
    "         .apply(sanitize)\n",
    "dev['title2_tokenized'] = \\\n",
    "    dev.loc[:, 'sentence2'] \\\n",
    "         .apply(sanitize)\n",
    "\n",
    "\n",
    "test['title1_tokenized'] = \\\n",
    "    test.loc[:, 'sentence1'] \\\n",
    "         .apply(sanitize)\n",
    "test['title2_tokenized'] = \\\n",
    "    test.loc[:, 'sentence2'] \\\n",
    "         .apply(sanitize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23726\n",
      "26402\n"
     ]
    }
   ],
   "source": [
    "MAX_NUM_WORDS = 10000\n",
    "tokenizer = keras \\\n",
    "    .preprocessing \\\n",
    "    .text \\\n",
    "    .Tokenizer(num_words=MAX_NUM_WORDS)\n",
    "\n",
    "train_corpus_x1 = train.title1_tokenized\n",
    "train_corpus_x2 = train.title2_tokenized\n",
    "train_corpus = pd.concat([\n",
    "    train_corpus_x1, train_corpus_x2])\n",
    "#corpus.shape\n",
    "tokenizer.fit_on_texts(train_corpus)\n",
    "tokenizer.texts_to_sequences(train_corpus)\n",
    "x1_train = tokenizer \\\n",
    "    .texts_to_sequences(train_corpus_x1)\n",
    "x2_train = tokenizer \\\n",
    "    .texts_to_sequences(train_corpus_x2)\n",
    "\n",
    "\n",
    "print(tokenizer.document_count)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "dev_corpus_x1 = dev.title1_tokenized\n",
    "dev_corpus_x2 = dev.title2_tokenized\n",
    "dev_corpus = pd.concat([\n",
    "    dev_corpus_x1, dev_corpus_x2])\n",
    "#corpus.shape\n",
    "tokenizer.fit_on_texts(dev_corpus)\n",
    "tokenizer.texts_to_sequences(dev_corpus)\n",
    "x1_dev = tokenizer \\\n",
    "    .texts_to_sequences(dev_corpus_x1)\n",
    "x2_dev = tokenizer \\\n",
    "    .texts_to_sequences(dev_corpus_x2)\n",
    "\n",
    "\n",
    "test_corpus_x1 = test.title1_tokenized\n",
    "test_corpus_x2 = test.title2_tokenized\n",
    "test_corpus = pd.concat([\n",
    "    test_corpus_x1, test_corpus_x2])\n",
    "#corpus.shape\n",
    "tokenizer.fit_on_texts(test_corpus)\n",
    "tokenizer.texts_to_sequences(test_corpus)\n",
    "x1_test = tokenizer \\\n",
    "    .texts_to_sequences(test_corpus_x1)\n",
    "x2_test = tokenizer \\\n",
    "    .texts_to_sequences(test_corpus_x2)\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "\n",
    "\n",
    "print(tokenizer.document_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,   11,  153,    2,  265,  521,   10,    2, 1441,  758,\n",
       "           8,    5,   30,   12,  236,  521,  112,    3,    2, 4981,  758,\n",
       "           6, 4982,    1],\n",
       "       [ 689,  297, 4957,   71,    5,    9,  120,   47,  108,   28,   54,\n",
       "        1034,   84,   71,   52,   50,    9,  166,  259,  108,  718,   71,\n",
       "         108, 1037,   71,    5,  853,   71,   20,   59,   12,    9,  120,\n",
       "          79,  103,   19],\n",
       "       [   0,    0,    0,  122,   12,    9,   96,   26,   93,  209,  146,\n",
       "          41,  690, 2899,   50,    9,   12,   54,   29,  752,    3,   26,\n",
       "         185,  146,   34,   81,    7,    2,  690, 2899,   57, 1030,   35,\n",
       "          32,  442,   19]], dtype=int32)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_seq_len1 = max([\n",
    "    len(seq) for seq in x1_train])\n",
    "print(max_seq_len1)   #36\n",
    "\n",
    "MAX_SEQUENCE_LENGTH = 36  #better to have words covered than uncovered\n",
    "x1_train = keras \\\n",
    "    .preprocessing \\\n",
    "    .sequence \\\n",
    "    .pad_sequences(x1_train, \n",
    "                   maxlen=MAX_SEQUENCE_LENGTH)\n",
    "\n",
    "\n",
    "x2_train = keras \\\n",
    "    .preprocessing \\\n",
    "    .sequence \\\n",
    "    .pad_sequences(x2_train, \n",
    "                   maxlen=MAX_SEQUENCE_LENGTH)\n",
    "\n",
    "x1_dev = keras \\\n",
    "    .preprocessing \\\n",
    "    .sequence \\\n",
    "    .pad_sequences(x1_dev, \n",
    "                   maxlen=MAX_SEQUENCE_LENGTH)\n",
    "\n",
    "\n",
    "x2_dev = keras \\\n",
    "    .preprocessing \\\n",
    "    .sequence \\\n",
    "    .pad_sequences(x2_dev, \n",
    "                   maxlen=MAX_SEQUENCE_LENGTH)\n",
    "\n",
    "\n",
    "x1_test = keras \\\n",
    "    .preprocessing \\\n",
    "    .sequence \\\n",
    "    .pad_sequences(x1_test, \n",
    "                   maxlen=MAX_SEQUENCE_LENGTH)\n",
    "\n",
    "x2_test = keras \\\n",
    "    .preprocessing \\\n",
    "    .sequence \\\n",
    "    .pad_sequences(x2_test, \n",
    "                   maxlen=MAX_SEQUENCE_LENGTH)\n",
    "\n",
    "\n",
    "y_train = train.label\n",
    "\n",
    "y_train = keras \\\n",
    "    .utils \\\n",
    "    .to_categorical(y_train, num_classes=2)\n",
    "\n",
    "\n",
    "y_dev = dev.label\n",
    "\n",
    "y_dev = keras \\\n",
    "    .utils \\\n",
    "    .to_categorical(y_dev, num_classes=2)\n",
    "\n",
    "\n",
    "\n",
    "y_test = test.label\n",
    "\n",
    "y_test = keras \\\n",
    "    .utils \\\n",
    "    .to_categorical(y_test, num_classes=2)\n",
    "\n",
    "x1_test[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 36)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_2 (InputLayer)            (None, 36)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)         (None, 36, 256)      2560000     input_1[0][0]                    \n",
      "                                                                 input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_1 (Bidirectional) (None, 256)          394240      embedding_1[0][0]                \n",
      "                                                                 embedding_1[1][0]                \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 512)          0           bidirectional_1[0][0]            \n",
      "                                                                 bidirectional_1[1][0]            \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 2)            1026        concatenate_1[0][0]              \n",
      "==================================================================================================\n",
      "Total params: 2,955,266\n",
      "Trainable params: 2,955,266\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#BiLSTM?\n",
    "NUM_CLASSES=2 #boolean; 1 or 0\n",
    "\n",
    "#rough estimate\n",
    "\n",
    "MAX_SEQUENCE_LENGTH = 36 #how long one sentence is\n",
    "\n",
    "#this is arbitrary?\n",
    "\n",
    "NUM_LSTM_UNITS = 128 #output dimension\n",
    "\n",
    "\n",
    "NUM_EMBEDDING_DIM = 256\n",
    "\n",
    "top_input = Input(\n",
    "    shape=(MAX_SEQUENCE_LENGTH, ),    #this is the first sentence\n",
    "    dtype='int32')\n",
    "\n",
    "bm_input = Input(\n",
    "    shape=(MAX_SEQUENCE_LENGTH, ),   #this is the second\n",
    "    dtype='int32')\n",
    "\n",
    "\n",
    "embedding_layer = Embedding(\n",
    "    MAX_NUM_WORDS, NUM_EMBEDDING_DIM)\n",
    "\n",
    "top_embedded = embedding_layer(\n",
    "    top_input)\n",
    "bm_embedded = embedding_layer(\n",
    "    bm_input)\n",
    "\n",
    "\n",
    "shared_lstm = Bidirectional(LSTM(NUM_LSTM_UNITS))\n",
    "top_output = shared_lstm(top_embedded)\n",
    "bm_output = shared_lstm(bm_embedded)\n",
    "\n",
    "\n",
    "merged = concatenate(\n",
    "    [top_output, bm_output], \n",
    "    axis=-1)\n",
    "\n",
    "\n",
    "dense =  Dense(\n",
    "    units=NUM_CLASSES, \n",
    "    activation='sigmoid')\n",
    "predictions = dense(merged)\n",
    "\n",
    "\n",
    "\n",
    "model = Model(\n",
    "    inputs=[top_input, bm_input], \n",
    "    outputs=predictions)\n",
    "\n",
    "\n",
    "model.summary()\n",
    "\n",
    "model.compile(\n",
    "    optimizer='Adam',\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=['accuracy'])\n",
    "\n",
    "BATCH_SIZE = 512\n",
    "\n",
    "NUM_EPOCHS = 20\n",
    "\n",
    "earlyStopping = EarlyStopping(monitor='val_loss', patience=10, verbose=0, mode='min')\n",
    "mcp_save = ModelCheckpoint('.mdl_wts.hdf5', save_best_only=True, monitor='val_loss', mode='min')\n",
    "reduce_lr_loss = ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=7, verbose=1, min_delta=1e-4, mode='min')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(x=[x1_train, x2_train], y=y_train, batch_size=BATCH_SIZE, epochs=NUM_EPOCHS, callbacks=[earlyStopping, mcp_save, reduce_lr_loss], validation_data=([x1_dev, x2_dev], y_dev),\n",
    "    \n",
    "    shuffle=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(x=[x1_train, x2_train], y=y_train, batch_size=BATCH_SIZE, epochs=NUM_EPOCHS, callbacks=[earlyStopping, mcp_save, reduce_lr_loss], validation_data=([x1_dev, x2_dev], y_dev),\n",
    "    \n",
    "    shuffle=True\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
