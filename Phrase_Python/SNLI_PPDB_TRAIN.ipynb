{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib\n",
    "import argparse\n",
    "import random\n",
    "import numpy as np\n",
    "import csv\n",
    "import pandas as pd\n",
    "from datascience import *\n",
    "import tensorflow as tf\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from keras.models import Model\n",
    "from keras.models import Sequential, load_model\n",
    "\n",
    "from keras_self_attention import SeqSelfAttention\n",
    "from keras.layers import *\n",
    "from keras.optimizers import Adam\n",
    "from keras import Input\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.utils import plot_model\n",
    "import keras\n",
    "#from keras.backend.tensorflow_backend import set_session\n",
    "#config = tf.ConfigProto()\n",
    "# config.gpu_options.allocator_type = 'BFC' #A \"Best-fit with coalescing\" algorithm, simplified from a version of dlmalloc.\n",
    "# config.gpu_options.per_process_gpu_memory_fraction = 0.7\n",
    "#config.gpu_options.allow_growth =True\n",
    "\n",
    "#set_session(tf.Session(config=config)) \n",
    "import re\n",
    "os.environ['PYTHONHASHSEED'] = '0'\n",
    "random.seed(1)\n",
    "np.random.seed(1)\n",
    "tf.set_random_seed(1)\n",
    "from keras.callbacks import *\n",
    "from keras.models import model_from_json\n",
    "import json\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/datascience/tables.py:132: FutureWarning: read_table is deprecated, use read_csv instead, passing sep='\\t'.\n",
      "  df = pandas.read_table(filepath_or_buffer, *args, **vargs)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "snli_train=Table.read_table(\"dataset/SNLI/snli_1.0_train.txt\") \n",
    "\n",
    "snli_dev=Table.read_table(\"dataset/SNLI/snli_1.0_dev.txt\") \n",
    "\n",
    "snli_test=Table.read_table(\"dataset/SNLI/snli_1.0_test.txt\") \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe = pd.read_csv(\"dataset/ppdb/sanitized.csv\")\n",
    "dataframe.to_csv(\"dataset/ppdb/sanitized.tsv\", sep='\\t', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "ppdb_Tb=Table.read_table(\"dataset/ppdb/sanitized.tsv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_id = snli_train.column(\"pairID\")\n",
    "train_sen1=snli_train.column(\"sentence1\")\n",
    "train_sen2=snli_train.column(\"sentence2\")  #SNLI\n",
    "train_gold = snli_train.column(\"gold_label\")\n",
    "\n",
    "\n",
    "phrase = ppdb_Tb.column(\"PHRASE\")\n",
    "paraphrase=ppdb_Tb.column(\"PARAPHRASE\")   #ppdb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['2677109430.jpg#1r1n', '2677109430.jpg#1r1e',\n",
       "       '2677109430.jpg#1r1c', ..., '152881593.jpg#1r1c',\n",
       "       '152881593.jpg#1r1e', '152881593.jpg#1r1n'], dtype='<U19')"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " another man \n",
      " another man \n",
      " another man \n",
      " another man \n",
      " another man \n",
      " another man \n",
      " another man \n",
      " another man \n",
      " another man \n",
      " another man \n",
      " another man \n",
      " another man \n",
      " photograph of \n",
      " photograph of \n",
      " young man \n",
      " young man \n",
      " play in \n",
      " play in \n",
      " walking around \n",
      " young man \n",
      " little girl \n",
      " the beginning of \n",
      " the beginning \n",
      " beginning of \n",
      " await \n",
      " two children \n",
      " two children \n",
      " sitting down \n",
      " little girl \n",
      " little girl \n",
      " little girl \n",
      " talks to \n",
      " very clear \n",
      " very clear \n",
      " clear \n",
      " very clear \n",
      " very clear \n",
      " clear \n",
      " little girl \n",
      " little girl \n",
      " sitting down \n",
      " another man \n",
      " young man \n",
      " men and women \n",
      " and women \n",
      " men and women \n",
      " of men and women \n",
      " and women \n",
      " is attempting to \n",
      " is attempting to \n",
      " attempting to \n",
      " play with \n",
      " playing with \n",
      " something \n",
      " something \n",
      " is sitting \n",
      " sitting next \n",
      " men and women \n",
      " and women \n",
      " men and women \n",
      " men and women \n",
      " and women \n",
      " men and women \n",
      " men and women \n",
      " little girl \n",
      " little girl \n",
      " little girl \n",
      " little girl is \n",
      " little girl \n",
      " little girl is \n",
      " little girl \n",
      " young man \n",
      " little girl \n",
      " little girl \n",
      " little girl \n",
      " getting ready \n",
      " running around \n",
      " two others \n",
      " one another \n",
      " one another \n",
      " here is \n"
     ]
    }
   ],
   "source": [
    "index_list=[]          #train\n",
    "PPDB_id = []\n",
    "sent1=[]\n",
    "sent2=[]\n",
    "para_list1=[]\n",
    "para_list2=[]\n",
    "gold = []\n",
    "\n",
    "ans = []\n",
    "for x in range(len(train_id)):   #index of SNLI\n",
    "    \n",
    "    for z in range(len(phrase)):  #index of PPDB\n",
    "        if (phrase[z] in train_sen1[x] and paraphrase[z] in train_sen2[x]):\n",
    "            print(phrase[z])\n",
    "            index_list=np.append(index_list, train_id[x])\n",
    "            PPDB_id=np.append(PPDB_id, z)\n",
    "            sent1=np.append(sent1, train_sen1[x])\n",
    "            sent2=np.append(sent2, train_sen2[x])\n",
    "            para_list1=np.append(para_list1, phrase[z])\n",
    "            para_list2=np.append(para_list2, paraphrase[z])\n",
    "            gold = np.append(gold, train_gold[x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "phrase_dataset = Table().with_column(\"SNLI_PairID\", index_list, \"PPDB_id\", PPDB_id, \"sent1\", sent1, \"sent2\", sent2, \"paraphrase1\", para_list1, \"paraphrase2\", para_list2, \"gold_label\", gold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "phrase_dataset=phrase_dataset.to_df()\n",
    "phrase_dataset.to_csv(\"dataset/Phrase_Level/SNLI/PPDB_SNLI_TRAIN.tsv\",sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_list=[]\n",
    "PPDB_id = []\n",
    "sent1=[]\n",
    "sent2=[]\n",
    "para_list1=[]\n",
    "para_list2=[]\n",
    "labeling = []\n",
    "\n",
    "ans = []\n",
    "for x in range(len(dev_id)):   #index of PAWS_QQP\n",
    "    \n",
    "    for z in range(len(phrase)):  #index of PPDB\n",
    "        if (phrase[z] in dev_sen1[x] and paraphrase[z] in dev_sen2[x]):\n",
    "            print(phrase[z])\n",
    "            index_list=np.append(index_list, x)\n",
    "            PPDB_id=np.append(PPDB_id, z)\n",
    "            sent1=np.append(sent1, dev_sen1[x])\n",
    "            sent2=np.append(sent2, dev_sen2[x])\n",
    "            para_list1=np.append(para_list1, phrase[z])\n",
    "            para_list2=np.append(para_list2, paraphrase[z])\n",
    "            labeling = np.append(labeling, dev_label[x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "phrase_dataset = Table().with_column(\"DEV_PAWS_QQP_id\", index_list, \"PPDB_id\", PPDB_id, \"sent1\", sent1, \"sent2\", sent2, \"paraphrase1\", para_list1, \"paraphrase2\", para_list2, \"label\", labeling)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "phrase_dataset.to_csv(\"dataset/Phrase_Level/PPDB_PAWS_QQP_DEV.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe = pd.read_csv(\"dataset/Phrase_Level/PPDB_PAWS_QQP_TRAIN-Copy1.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe.to_csv(\"dataset/Phrase_Level/PPDB_PAWS_QQP_TRAIN.tsv\", sep='\\t', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe = pd.read_csv(\"dataset/Phrase_Level/PPDB_PAWS_QQP_DEV-Copy1.csv\")\n",
    "dataframe.to_csv(\"dataset/Phrase_Level/PPDB_PAWS_QQP_DEV.tsv\", sep='\\t', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "phrase_dataset=Table.read_table('dataset/Phrase_Level/SNLI/SNLI_PPDB_TEST.tsv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "    <thead>\n",
       "        <tr>\n",
       "            <th>SNLI_PairID</th> <th>PPDB_id</th> <th>sent1</th> <th>sent2</th> <th>paraphrase1</th> <th>paraphrase2</th> <th>gold_label</th> <th>labels1</th> <th>labels2</th>\n",
       "        </tr>\n",
       "    </thead>\n",
       "    <tbody>\n",
       "        <tr>\n",
       "            <td>10         </td> <td>20265  </td> <td>A statue at a museum that no seems to be looking at.</td> <td>There is a statue that not many people seem to be intere ...</td> <td> seems to be </td> <td> seem to be </td> <td>entailment</td> <td>O O O O O O O B I E O O</td> <td>O O O O O O O O B I E O O</td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>10         </td> <td>34976  </td> <td>A statue at a museum that no seems to be looking at.</td> <td>There is a statue that not many people seem to be intere ...</td> <td> seems to be </td> <td> seem to be </td> <td>entailment</td> <td>O O O O O O O B I E O O</td> <td>O O O O O O O O B I E O O</td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>10         </td> <td>61957  </td> <td>A statue at a museum that no seems to be looking at.</td> <td>There is a statue that not many people seem to be intere ...</td> <td> seems to be </td> <td> seem to be </td> <td>entailment</td> <td>O O O O O O O B I E O O</td> <td>O O O O O O O O B I E O O</td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>10         </td> <td>125335 </td> <td>A statue at a museum that no seems to be looking at.</td> <td>There is a statue that not many people seem to be intere ...</td> <td> seems to    </td> <td> seem to    </td> <td>entailment</td> <td>O O O O O O O B E O O O</td> <td>O O O O O O O O B E O O O</td>\n",
       "        </tr>\n",
       "    </tbody>\n",
       "</table>\n",
       "<p>... (269 rows omitted)</p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "phrase_dataset.show(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "sent1=phrase_dataset.column(\"sent1\")\n",
    "sent2=phrase_dataset.column(\"sent2\")\n",
    "para1=phrase_dataset.column(\"paraphrase1\")\n",
    "para2=phrase_dataset.column(\"paraphrase2\")\n",
    "\n",
    "gg = [x.split() for x in sent1]\n",
    "hh = [x.split() for x in sent2]\n",
    "ii = [x.split() for x in para1]\n",
    "jj = [x.split() for x in para2]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "22"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels1=[]\n",
    "for x in range(len(gg)):\n",
    "    labels1.append(spit_label(gg[x], ii[x], len(ii[x]),len(ii[x]), []))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels2=[]\n",
    "for x in range(len(hh)):\n",
    "    labels2.append(spit_label(hh[x], jj[x], len(jj[x]),len(jj[x]), []))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "joined1=[]\n",
    "for x in labels1:\n",
    "    joined1.append(\" \".join(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "joined2=[]\n",
    "for x in labels2:\n",
    "    joined2.append(\" \".join(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['2677109430.jpg#1r1n', '2677109430.jpg#1r1e',\n",
       "       '2677109430.jpg#1r1c', ..., '152881593.jpg#1r1c',\n",
       "       '152881593.jpg#1r1e', '152881593.jpg#1r1n'], dtype='<U19')"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#phrase_dataset=phrase_dataset.with_columns(\"labels1\", joined1, \"labels2\", joined2)\n",
    "#phrase_dataset.show(4)\n",
    "phrase_dataset=phrase_dataset.to_df()\n",
    "phrase_dataset.to_csv(\"dataset/Phrase_Level/SNLI/SNLI_PPDB_TEST.tsv\", sep='\\t', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_Tb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "    def spit_label(split_sent, para_split, ori_length, length, listy):\n",
    "       \n",
    "    \n",
    "        if para_split[0]==split_sent[0]:\n",
    "            \n",
    "            if ori_length==1:\n",
    "                listy.append(\"S\")\n",
    "                for x in range(len(split_sent)-1):\n",
    "                    listy.append(\"O\")\n",
    "                return listy    \n",
    "            \n",
    "            elif ori_length==length:\n",
    "                listy.append(\"B\")\n",
    "                return spit_label(split_sent[1:], para_split[1:], ori_length, length-1, listy)\n",
    "                \n",
    "            elif length==1:\n",
    "                listy.append(\"E\")\n",
    "                for x in range(len(split_sent)-1):\n",
    "                    listy.append(\"O\")\n",
    "                return listy\n",
    "            \n",
    "            else:\n",
    "                listy.append(\"I\")\n",
    "                return spit_label(split_sent[1:], para_split[1:], ori_length, length-1, listy)\n",
    "                \n",
    "        else:\n",
    "            listy.append(\"O\")\n",
    "            return spit_label(split_sent[1:], para_split, ori_length, length, listy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "bbb=\"\".join(\"he \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'he '"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bbb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
