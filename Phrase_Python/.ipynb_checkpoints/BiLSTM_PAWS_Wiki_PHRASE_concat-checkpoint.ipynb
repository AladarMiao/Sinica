{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib\n",
    "import argparse\n",
    "import random\n",
    "import numpy as np\n",
    "import csv\n",
    "import pandas as pd\n",
    "from datascience import *\n",
    "import tensorflow as tf\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from keras.models import Model\n",
    "from keras.models import Sequential, load_model\n",
    "\n",
    "from keras.activations import softmax\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import to_categorical\n",
    "from keras.layers import *\n",
    "from keras.optimizers import Adam\n",
    "from keras import Input\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.utils import plot_model\n",
    "import keras\n",
    "#from keras.backend.tensorflow_backend import set_session\n",
    "#config = tf.ConfigProto()\n",
    "# config.gpu_options.allocator_type = 'BFC' #A \"Best-fit with coalescing\" algorithm, simplified from a version of dlmalloc.\n",
    "# config.gpu_options.per_process_gpu_memory_fraction = 0.7\n",
    "#config.gpu_options.allow_growth =True\n",
    "\n",
    "#set_session(tf.Session(config=config)) \n",
    "import re\n",
    "os.environ['PYTHONHASHSEED'] = '0'\n",
    "random.seed(1)\n",
    "np.random.seed(1)\n",
    "tf.set_random_seed(1)\n",
    "from keras.callbacks import *\n",
    "from keras.models import model_from_json\n",
    "import json\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sanitize(string):\n",
    "    words = string.split(' ')\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thu Aug  1 02:07:08 2019       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 418.56       Driver Version: 418.56       CUDA Version: 10.1     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  GeForce GTX 1080    Off  | 00000000:02:00.0 Off |                  N/A |\n",
      "|  0%   47C    P8     9W / 200W |   7797MiB /  8118MiB |      0%      Default |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                       GPU Memory |\n",
      "|  GPU       PID   Type   Process name                             Usage      |\n",
      "|=============================================================================|\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   TRAIN_PAWS_Wiki_id   PPDB_id  \\\n",
      "0                21.0  160456.0   \n",
      "1                21.0  243017.0   \n",
      "2                33.0  395059.0   \n",
      "3                33.0  395061.0   \n",
      "4                33.0  395071.0   \n",
      "\n",
      "                                               sent1  \\\n",
      "0  For their performances in the game , quarterba...   \n",
      "1  For their performances in the game , quarterba...   \n",
      "2  The leaves are generally 1.5-4 mm long and 0.2...   \n",
      "3  The leaves are generally 1.5-4 mm long and 0.2...   \n",
      "4  The leaves are generally 1.5-4 mm long and 0.2...   \n",
      "\n",
      "                                               sent2      paraphrase1  \\\n",
      "0  Quarterback P. J. Williams and Defensive Back ...   most valuable    \n",
      "1  Quarterback P. J. Williams and Defensive Back ...         's most    \n",
      "2  The leaves are usually 1.5-4 mm long and 0.2-0...   are generally    \n",
      "3  The leaves are usually 1.5-4 mm long and 0.2-0...   are generally    \n",
      "4  The leaves are usually 1.5-4 mm long and 0.2-0...   are generally    \n",
      "\n",
      "     paraphrase2  label                                            labels1  \\\n",
      "0      valuable       0  O O O O O O O O O O O O O O O O O O O O O B E O O   \n",
      "1          most       0  O O O O O O O O O O O O O O O O O O O O B E O O O   \n",
      "2   are usually       1                            O O B E O O O O O O O O   \n",
      "3   are usually       1                            O O B E O O O O O O O O   \n",
      "4   are usually       1                            O O B E O O O O O O O O   \n",
      "\n",
      "                                             labels2  \n",
      "0  O O O O O O O O O O O O O S O O O O O O O O O O O  \n",
      "1  O O O O O O O O O O O O S O O O O O O O O O O O O  \n",
      "2                            O O B E O O O O O O O O  \n",
      "3                            O O B E O O O O O O O O  \n",
      "4                            O O B E O O O O O O O O  \n"
     ]
    }
   ],
   "source": [
    "#read csv\n",
    "\n",
    "train = pd.read_csv(\"../dataset/Phrase_Level/PAWS_Wiki/LABELED_PAWS_Wiki_TRAIN.tsv\", sep='\\t')\n",
    "\n",
    "dev = pd.read_csv(\"../dataset/Phrase_Level/PAWS_Wiki/LABELED_PAWS_Wiki_DEV.tsv\", sep='\\t')\n",
    "\n",
    "test = pd.read_csv(\"../dataset/Phrase_Level/PAWS_Wiki/LABELED_PAWS_Wiki_TEST.tsv\", sep='\\t')\n",
    "\n",
    "\n",
    "print(train[:5])\n",
    "\n",
    "train['title1_tokenized'] = \\\n",
    "    train.loc[:, 'sent1'] \\\n",
    "         .apply(sanitize)\n",
    "train['title2_tokenized'] = \\\n",
    "    train.loc[:, 'sent2'] \\\n",
    "         .apply(sanitize)\n",
    "\n",
    "\n",
    "dev['title1_tokenized'] = \\\n",
    "    dev.loc[:, 'sent1'] \\\n",
    "         .apply(sanitize)\n",
    "dev['title2_tokenized'] = \\\n",
    "    dev.loc[:, 'sent2'] \\\n",
    "         .apply(sanitize)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    O O O O O O O O O O O O O O O O O O O O O B E O O\n",
       "1    O O O O O O O O O O O O O O O O O O O O B E O O O\n",
       "2                              O O B E O O O O O O O O\n",
       "3                              O O B E O O O O O O O O\n",
       "Name: labels1, dtype: object"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.labels1[:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TRAIN_PAWS_Wiki_id</th>\n",
       "      <th>PPDB_id</th>\n",
       "      <th>sent1</th>\n",
       "      <th>sent2</th>\n",
       "      <th>paraphrase1</th>\n",
       "      <th>paraphrase2</th>\n",
       "      <th>label</th>\n",
       "      <th>labels1</th>\n",
       "      <th>labels2</th>\n",
       "      <th>title1_tokenized</th>\n",
       "      <th>title2_tokenized</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>21.0</td>\n",
       "      <td>160456.0</td>\n",
       "      <td>For their performances in the game , quarterba...</td>\n",
       "      <td>Quarterback P. J. Williams and Defensive Back ...</td>\n",
       "      <td>most valuable</td>\n",
       "      <td>valuable</td>\n",
       "      <td>0</td>\n",
       "      <td>O O O O O O O O O O O O O O O O O O O O O B E O O</td>\n",
       "      <td>O O O O O O O O O O O O O S O O O O O O O O O O O</td>\n",
       "      <td>[For, their, performances, in, the, game, ,, q...</td>\n",
       "      <td>[Quarterback, P., J., Williams, and, Defensive...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>21.0</td>\n",
       "      <td>243017.0</td>\n",
       "      <td>For their performances in the game , quarterba...</td>\n",
       "      <td>Quarterback P. J. Williams and Defensive Back ...</td>\n",
       "      <td>'s most</td>\n",
       "      <td>most</td>\n",
       "      <td>0</td>\n",
       "      <td>O O O O O O O O O O O O O O O O O O O O B E O O O</td>\n",
       "      <td>O O O O O O O O O O O O S O O O O O O O O O O O O</td>\n",
       "      <td>[For, their, performances, in, the, game, ,, q...</td>\n",
       "      <td>[Quarterback, P., J., Williams, and, Defensive...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>33.0</td>\n",
       "      <td>395059.0</td>\n",
       "      <td>The leaves are generally 1.5-4 mm long and 0.2...</td>\n",
       "      <td>The leaves are usually 1.5-4 mm long and 0.2-0...</td>\n",
       "      <td>are generally</td>\n",
       "      <td>are usually</td>\n",
       "      <td>1</td>\n",
       "      <td>O O B E O O O O O O O O</td>\n",
       "      <td>O O B E O O O O O O O O</td>\n",
       "      <td>[The, leaves, are, generally, 1.5-4, mm, long,...</td>\n",
       "      <td>[The, leaves, are, usually, 1.5-4, mm, long, a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>33.0</td>\n",
       "      <td>395061.0</td>\n",
       "      <td>The leaves are generally 1.5-4 mm long and 0.2...</td>\n",
       "      <td>The leaves are usually 1.5-4 mm long and 0.2-0...</td>\n",
       "      <td>are generally</td>\n",
       "      <td>are usually</td>\n",
       "      <td>1</td>\n",
       "      <td>O O B E O O O O O O O O</td>\n",
       "      <td>O O B E O O O O O O O O</td>\n",
       "      <td>[The, leaves, are, generally, 1.5-4, mm, long,...</td>\n",
       "      <td>[The, leaves, are, usually, 1.5-4, mm, long, a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>33.0</td>\n",
       "      <td>395071.0</td>\n",
       "      <td>The leaves are generally 1.5-4 mm long and 0.2...</td>\n",
       "      <td>The leaves are usually 1.5-4 mm long and 0.2-0...</td>\n",
       "      <td>are generally</td>\n",
       "      <td>are usually</td>\n",
       "      <td>1</td>\n",
       "      <td>O O B E O O O O O O O O</td>\n",
       "      <td>O O B E O O O O O O O O</td>\n",
       "      <td>[The, leaves, are, generally, 1.5-4, mm, long,...</td>\n",
       "      <td>[The, leaves, are, usually, 1.5-4, mm, long, a...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   TRAIN_PAWS_Wiki_id   PPDB_id  \\\n",
       "0                21.0  160456.0   \n",
       "1                21.0  243017.0   \n",
       "2                33.0  395059.0   \n",
       "3                33.0  395061.0   \n",
       "4                33.0  395071.0   \n",
       "\n",
       "                                               sent1  \\\n",
       "0  For their performances in the game , quarterba...   \n",
       "1  For their performances in the game , quarterba...   \n",
       "2  The leaves are generally 1.5-4 mm long and 0.2...   \n",
       "3  The leaves are generally 1.5-4 mm long and 0.2...   \n",
       "4  The leaves are generally 1.5-4 mm long and 0.2...   \n",
       "\n",
       "                                               sent2      paraphrase1  \\\n",
       "0  Quarterback P. J. Williams and Defensive Back ...   most valuable    \n",
       "1  Quarterback P. J. Williams and Defensive Back ...         's most    \n",
       "2  The leaves are usually 1.5-4 mm long and 0.2-0...   are generally    \n",
       "3  The leaves are usually 1.5-4 mm long and 0.2-0...   are generally    \n",
       "4  The leaves are usually 1.5-4 mm long and 0.2-0...   are generally    \n",
       "\n",
       "     paraphrase2  label                                            labels1  \\\n",
       "0      valuable       0  O O O O O O O O O O O O O O O O O O O O O B E O O   \n",
       "1          most       0  O O O O O O O O O O O O O O O O O O O O B E O O O   \n",
       "2   are usually       1                            O O B E O O O O O O O O   \n",
       "3   are usually       1                            O O B E O O O O O O O O   \n",
       "4   are usually       1                            O O B E O O O O O O O O   \n",
       "\n",
       "                                             labels2  \\\n",
       "0  O O O O O O O O O O O O O S O O O O O O O O O O O   \n",
       "1  O O O O O O O O O O O O S O O O O O O O O O O O O   \n",
       "2                            O O B E O O O O O O O O   \n",
       "3                            O O B E O O O O O O O O   \n",
       "4                            O O B E O O O O O O O O   \n",
       "\n",
       "                                    title1_tokenized  \\\n",
       "0  [For, their, performances, in, the, game, ,, q...   \n",
       "1  [For, their, performances, in, the, game, ,, q...   \n",
       "2  [The, leaves, are, generally, 1.5-4, mm, long,...   \n",
       "3  [The, leaves, are, generally, 1.5-4, mm, long,...   \n",
       "4  [The, leaves, are, generally, 1.5-4, mm, long,...   \n",
       "\n",
       "                                    title2_tokenized  \n",
       "0  [Quarterback, P., J., Williams, and, Defensive...  \n",
       "1  [Quarterback, P., J., Williams, and, Defensive...  \n",
       "2  [The, leaves, are, usually, 1.5-4, mm, long, a...  \n",
       "3  [The, leaves, are, usually, 1.5-4, mm, long, a...  \n",
       "4  [The, leaves, are, usually, 1.5-4, mm, long, a...  "
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GloVe data loaded\n"
     ]
    }
   ],
   "source": [
    "embeddings_index = {}\n",
    "f = open('../glove.840B.300d.txt')\n",
    "for line in f:\n",
    "    values = line.split(' ')\n",
    "    word = values[0] ## The first entry is the word\n",
    "    coefs = np.asarray(values[1:], dtype='float32') ## These are the vecotrs representing the embedding for the word\n",
    "    embeddings_index[word] = coefs\n",
    "\n",
    "f.close()\n",
    "\n",
    "print('GloVe data loaded')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tokenize sentences\n",
    "\n",
    "def normalize_and_split(sentence):\n",
    "    s=sentence.split()\n",
    "    l=[x.lower() for x in s]\n",
    "    \n",
    "    return l\n",
    "\n",
    "def normalize_and_split2(sentence):\n",
    "    s=sentence.split()\n",
    "    l=[x.lower() for x in s]\n",
    "    first_word=(l[0])[2:]\n",
    "    l[0]=first_word\n",
    "    return l\n",
    "\n",
    "#tokenize labels\n",
    "\n",
    "def t_label(column, max_length):\n",
    "    \n",
    "    ret=[]\n",
    "  \n",
    "    for y in column:   #each list\n",
    "        \n",
    "        s = y.split() #split into individual alphabets\n",
    "        temp=[]\n",
    "        \n",
    "        for x in s:\n",
    "            temp.append(label_to_index[x])\n",
    "             \n",
    "    \n",
    "        for _ in range(max_length-len(s)): \n",
    "            temp.append(0)\n",
    "        \n",
    "        temp=np.asarray(temp)\n",
    "        \n",
    "        y_train = keras \\\n",
    "            .utils \\\n",
    "            .to_categorical(temp, 5)    #do one hot on each word\n",
    "        \n",
    "        \n",
    "        \n",
    "        ret.append(y_train)\n",
    "           \n",
    "    return list(ret)\n",
    "\n",
    "label_to_index = {\n",
    "    'O': 0, \n",
    "    'B': 1, \n",
    "    'I': 2,\n",
    "    'E': 3,\n",
    "    'S': 4\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i', 'like', 'to', 'eateat', 'pie']"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "normalize_and_split(\"I like to eateat piE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"b'i\", 'am', 'liking', 'itt']"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "normalize_and_split(\"b'I Am LiKINg iTt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_Tb=Table.read_table(\"../dataset/Phrase_Level/PAWS_Wiki/LABELED_PAWS_Wiki_TRAIN.tsv\")\n",
    "dev_Tb=Table.read_table(\"../dataset/Phrase_Level/PAWS_Wiki/LABELED_PAWS_Wiki_DEV.tsv\")\n",
    "test_Tb=Table.read_table(\"../dataset/Phrase_Level/PAWS_Wiki/LABELED_PAWS_Wiki_TEST.tsv\")\n",
    "\n",
    "train_Tb_Q=Table.read_table(\"../dataset/Phrase_Level/PAWS_QQP/LABELED_PAWS_QQP_TRAIN.tsv\")\n",
    "dev_Tb_Q=Table.read_table(\"../dataset/Phrase_Level/PAWS_QQP/LABELED_PAWS_QQP_DEV.tsv\")\n",
    "\n",
    "dev_Tb_S=Table.read_table(\"../dataset/Phrase_Level/SNLI/SNLI_PPDB_DEV.tsv\")\n",
    "test_Tb_S=Table.read_table(\"../dataset/Phrase_Level/SNLI/PPDB_SNLI_TEST.tsv\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_Tb=train_Tb.where(\"label\", are.equal_to(1))\n",
    "dev_Tb=dev_Tb.where(\"label\", are.equal_to(1))\n",
    "test_Tb=test_Tb.where(\"label\", are.equal_to(1))\n",
    "\n",
    "train_Tb_Q=train_Tb_Q.where(\"Ground Label\", are.equal_to(1))\n",
    "\n",
    "dev_Tb_Q=dev_Tb_Q.where(\"Ground Label\", are.equal_to(1))\n",
    "\n",
    "dev_Tb_S1=dev_Tb_S.where(\"gold_label\", are.equal_to(\"entailment\"))\n",
    "dev_Tb_S2=dev_Tb_S.where(\"gold_label\", are.equal_to(\"neutral\"))\n",
    "\n",
    "test_Tb_S1=test_Tb_S.where(\"gold_label\", are.equal_to(\"entailment\"))\n",
    "test_Tb_S2=test_Tb_S.where(\"gold_label\", are.equal_to(\"neutral\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "def keep_one_para_pair(Tb):   #takes in origin table, keeps only one pair of paraphrase per sentence, and then return the modified table\n",
    "    \n",
    "    to_drop=[]\n",
    "    pair_id=Tb.column(0)\n",
    "    for x in range(Tb.num_rows-1):\n",
    "        if pair_id[x]==pair_id[x+1]:\n",
    "            to_drop.append(x)\n",
    "    new_Tb=Tb.exclude(to_drop)\n",
    "    return new_Tb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_PAWS_Wiki=keep_one_para_pair(train_Tb)   #2739\n",
    "\n",
    "dev_PAWS_Wiki=keep_one_para_pair(dev_Tb)     #924\n",
    "\n",
    "test_PAWS_Wiki=keep_one_para_pair(test_Tb)   #1191\n",
    "\n",
    "train_PAWS_QQP=keep_one_para_pair(train_Tb_Q)  #791\n",
    "\n",
    "dev_PAWS_QQP=keep_one_para_pair(dev_Tb_Q)     #63\n",
    "\n",
    "dev_SNLI_S1=keep_one_para_pair(dev_Tb_S1)       #308   \n",
    "\n",
    "test_SNLI_S1=keep_one_para_pair(test_Tb_S1)       #229\n",
    "\n",
    "dev_SNLI_S2=keep_one_para_pair(dev_Tb_S2)       #308   \n",
    "\n",
    "test_SNLI_S2=keep_one_para_pair(test_Tb_S2)       #229\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "    <thead>\n",
       "        <tr>\n",
       "            <th>SNLI_PairID</th> <th>PPDB_id</th> <th>sent1</th> <th>sent2</th> <th>paraphrase1</th> <th>paraphrase2</th> <th>gold_label</th> <th>labels1</th> <th>labels2</th>\n",
       "        </tr>\n",
       "    </thead>\n",
       "    <tbody>\n",
       "        <tr>\n",
       "            <td>6526219567.jpg#4r1e</td> <td>1.10807e+06</td> <td>A statue at a museum that no seems to be looking at.        </td> <td>There is a statue that not many people seem to be intere ...</td> <td> seems to       </td> <td> seem to   </td> <td>entailment</td> <td>O O O O O O O B E O O O        </td> <td>O O O O O O O O B E O O O</td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>4460943467.jpg#0r1e</td> <td>302929     </td> <td>3 young man in hoods standing in the middle of a quiet s ...</td> <td>Three hood wearing people stand in a street.                </td> <td> standing in    </td> <td> stand in  </td> <td>entailment</td> <td>O O O O O B E O O O O O O O O O</td> <td>O O O O B E O O          </td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>3036382555.jpg#0r1n</td> <td>1.00273e+06</td> <td>The two young girls are dressed as fairies, and are play ...</td> <td>The two girls play in the Autumn.                           </td> <td> playing in     </td> <td> play in   </td> <td>neutral   </td> <td>O O O O O O O O O O B E O O O  </td> <td>O O O B E O O            </td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>4479330885.jpg#3r1e</td> <td>958598     </td> <td>Several younger people sitting in front of a statue.        </td> <td>several young people sitting outside                        </td> <td> younger people </td> <td> young     </td> <td>neutral   </td> <td>O B E O O O O O O              </td> <td>O S O O O                </td>\n",
       "        </tr>\n",
       "    </tbody>\n",
       "</table>\n",
       "<p>... (225 rows omitted)</p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "test_SNLI.show(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "def help_norm(Tb):\n",
    "     \n",
    "    \n",
    "    if Tb is train_PAWS_QQP or Tb is dev_PAWS_QQP:\n",
    "        a=Tb.apply(normalize_and_split2, \"sent1\")\n",
    "        b=Tb.apply(normalize_and_split2, \"sent2\")\n",
    "        \n",
    "    else:\n",
    "        a=Tb.apply(normalize_and_split, \"sent1\")\n",
    "        b=Tb.apply(normalize_and_split, \"sent2\")    \n",
    "        \n",
    "       \n",
    "    \n",
    "    \n",
    "    c=t_label(Tb.column(\"labels1\"), 58)\n",
    "    d=t_label(Tb.column(\"labels2\"), 58)\n",
    "    \n",
    "   \n",
    "\n",
    "    \n",
    "    return list(a),list(b),list(c),list(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tokenize sentences, and one-hot labels\n",
    "\n",
    "\n",
    "TRPWS1, TRPWS2, TRPWL1, TRPWL2=help_norm(train_PAWS_Wiki)   #training paws wiki sentence 1, btw, and the rest are named in a similar fashion\n",
    "\n",
    "DPWS1, DPWS2,DPWL1, DPWL2 = help_norm(dev_PAWS_Wiki)\n",
    "\n",
    "TEPWS1, TEPWS2, TEPWL1, TEPWL2=help_norm(test_PAWS_Wiki)\n",
    "\n",
    "TRPQS1, TRPQS2, TRPQL1, TRPQL2=help_norm(train_PAWS_QQP)\n",
    "\n",
    "DPQS1, DPQS2, DPQL1, DPQL2=help_norm(dev_PAWS_QQP)\n",
    "\n",
    "IDSS1, IDSS2, IDSL1, IDSL2 = help_norm(dev_SNLI_S1)\n",
    "\n",
    "IIDSS1, IIDSS2, IIDSL1, IIDSL2 = help_norm(dev_SNLI_S2)\n",
    "\n",
    "ITESS1, ITESS2, ITESL1, ITESL2=help_norm(test_SNLI_S1)\n",
    "\n",
    "IITESS1, IITESS2, IITESL1, IITESL2=help_norm(test_SNLI_S2)\n",
    "\n",
    "#also prints out maximum sentence length for each dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRPW=create_Tb(train_PAWS_Wiki, TRPWS1, TRPWS2, TRPWL1, TRPWL2)\n",
    "DPW=create_Tb(dev_PAWS_Wiki, DPWS1, DPWS2, DPWL1, DPWL2)\n",
    "TEPW=create_Tb(test_PAWS_Wiki, TEPWS1, TEPWS2, TEPWL1, TEPWL2)\n",
    "TRPQ=create_Tb(train_PAWS_QQP, TRPQS1, TRPQS2, TRPQL1, TRPQL2)\n",
    "DPQ=create_Tb(dev_PAWS_QQP, DPQS1, DPQS2, DPQL1, DPQL2)\n",
    "DS1=create_Tb(dev_SNLI_S1, IDSS1, IDSS2, IDSL1, IDSL2)\n",
    "DS2=create_Tb(dev_SNLI_S2, IIDSS1, IIDSS2, IIDSL1, IIDSL2)\n",
    "TES1=create_Tb(test_SNLI_S1, ITESS1, ITESS2, ITESL1, ITESL2)\n",
    "TES2=create_Tb(test_SNLI_S2, IITESS1, IITESS2, IITESL1, IITESL2)\n",
    "#max seq length say 58"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_Tb(Tb, s1, s2, l1, l2):         #visualize tokenization with a table\n",
    "    zz=Tb.with_columns(\"sent1_token\", s1, \"sent2_token\", s2, \"label1_token\", l1, \"label2_token\", l2)\n",
    "    return zz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_corpus=TRPWS1+TRPWS2+DPWS1+DPWS2+TEPWS1+TEPWS2+TRPQS1+TRPQS2+DPQS1+DPQS2\n",
    "\n",
    "dev_corpus= IDSS1 + IDSS2 + IIDSS1 + IIDSS2+ ITESS1+ ITESS2 + IITESS1 + IITESS2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 358,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5708"
      ]
     },
     "execution_count": 358,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_sen1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sen1=TRPWS1+DPWS1+TEPWS1+TRPQS1+DPQS1\n",
    "\n",
    "train_sen2=TRPWS2+DPWS2+TEPWS2+TRPQS2+DPQS2\n",
    "\n",
    "train_label1=np.array(TRPWL1+DPWL1+TEPWL1+TRPQL1+DPQL1)\n",
    "\n",
    "train_label2=np.array(TRPWL2+DPWL2+TEPWL2+TRPQL2+DPQL2)\n",
    "\n",
    "dev_sen1=IDSS1+IIDSS1+ITESS1+IITESS1\n",
    "dev_sen2=IDSS2+IIDSS2+ITESS2+IITESS2\n",
    "dev_label1=np.array(IDSL1+IIDSL1+ITESL1+IITESL1)\n",
    "dev_label2=np.array(IDSL2+IIDSL2+ITESL1+IITESL2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 0., 0., 0., 0.],\n",
       "       [1., 0., 0., 0., 0.],\n",
       "       [1., 0., 0., 0., 0.],\n",
       "       [1., 0., 0., 0., 0.],\n",
       "       [1., 0., 0., 0., 0.],\n",
       "       [1., 0., 0., 0., 0.],\n",
       "       [1., 0., 0., 0., 0.],\n",
       "       [1., 0., 0., 0., 0.],\n",
       "       [1., 0., 0., 0., 0.],\n",
       "       [1., 0., 0., 0., 0.],\n",
       "       [0., 1., 0., 0., 0.],\n",
       "       [0., 0., 0., 1., 0.],\n",
       "       [1., 0., 0., 0., 0.],\n",
       "       [1., 0., 0., 0., 0.],\n",
       "       [1., 0., 0., 0., 0.],\n",
       "       [1., 0., 0., 0., 0.],\n",
       "       [1., 0., 0., 0., 0.],\n",
       "       [1., 0., 0., 0., 0.],\n",
       "       [1., 0., 0., 0., 0.],\n",
       "       [1., 0., 0., 0., 0.],\n",
       "       [1., 0., 0., 0., 0.],\n",
       "       [1., 0., 0., 0., 0.],\n",
       "       [1., 0., 0., 0., 0.],\n",
       "       [1., 0., 0., 0., 0.],\n",
       "       [1., 0., 0., 0., 0.],\n",
       "       [1., 0., 0., 0., 0.],\n",
       "       [1., 0., 0., 0., 0.],\n",
       "       [1., 0., 0., 0., 0.],\n",
       "       [1., 0., 0., 0., 0.],\n",
       "       [1., 0., 0., 0., 0.],\n",
       "       [1., 0., 0., 0., 0.],\n",
       "       [1., 0., 0., 0., 0.],\n",
       "       [1., 0., 0., 0., 0.],\n",
       "       [1., 0., 0., 0., 0.],\n",
       "       [1., 0., 0., 0., 0.],\n",
       "       [1., 0., 0., 0., 0.],\n",
       "       [1., 0., 0., 0., 0.],\n",
       "       [1., 0., 0., 0., 0.],\n",
       "       [1., 0., 0., 0., 0.],\n",
       "       [1., 0., 0., 0., 0.],\n",
       "       [1., 0., 0., 0., 0.],\n",
       "       [1., 0., 0., 0., 0.],\n",
       "       [1., 0., 0., 0., 0.],\n",
       "       [1., 0., 0., 0., 0.],\n",
       "       [1., 0., 0., 0., 0.],\n",
       "       [1., 0., 0., 0., 0.],\n",
       "       [1., 0., 0., 0., 0.],\n",
       "       [1., 0., 0., 0., 0.],\n",
       "       [1., 0., 0., 0., 0.],\n",
       "       [1., 0., 0., 0., 0.],\n",
       "       [1., 0., 0., 0., 0.],\n",
       "       [1., 0., 0., 0., 0.],\n",
       "       [1., 0., 0., 0., 0.],\n",
       "       [1., 0., 0., 0., 0.],\n",
       "       [1., 0., 0., 0., 0.],\n",
       "       [1., 0., 0., 0., 0.],\n",
       "       [1., 0., 0., 0., 0.],\n",
       "       [1., 0., 0., 0., 0.]], dtype=float32)"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_label1[2]           #10000: O, 01000: B, 00100: I, 00010: E, 00001: S"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5024\n",
      "5810\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "\n",
    "MAX_NUM_WORDS = 10000\n",
    "MAX_SEQUENCE_LENGTH = 58\n",
    "\n",
    "\n",
    "#create a dictionary\n",
    "\n",
    "tokenizer = keras \\\n",
    "    .preprocessing \\\n",
    "    .text \\\n",
    "    .Tokenizer(num_words=MAX_NUM_WORDS)\n",
    "\n",
    "#corpus.shape\n",
    "tokenizer.fit_on_texts(train_corpus)\n",
    "tokenizer.texts_to_sequences(train_corpus)\n",
    "\n",
    "x1_train = tokenizer \\\n",
    "    .texts_to_sequences(train_sen1)\n",
    "x2_train = tokenizer \\\n",
    "    .texts_to_sequences(train_sen2)\n",
    "\n",
    "print(tokenizer.document_count)\n",
    "\n",
    "#corpus.shape\n",
    "tokenizer.fit_on_texts(dev_corpus)\n",
    "tokenizer.texts_to_sequences(dev_corpus)\n",
    "x1_dev = tokenizer \\\n",
    "    .texts_to_sequences(dev_sen1)\n",
    "x2_dev = tokenizer \\\n",
    "    .texts_to_sequences(dev_sen2)\n",
    "\n",
    "print(tokenizer.document_count)\n",
    "\n",
    "\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "x1_train = keras \\\n",
    "    .preprocessing \\\n",
    "    .sequence \\\n",
    "    .pad_sequences(x1_train, \n",
    "                   maxlen=MAX_SEQUENCE_LENGTH)\n",
    "\n",
    "\n",
    "x2_train = keras \\\n",
    "    .preprocessing \\\n",
    "    .sequence \\\n",
    "    .pad_sequences(x2_train, \n",
    "                   maxlen=MAX_SEQUENCE_LENGTH)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "x1_dev = keras \\\n",
    "    .preprocessing \\\n",
    "    .sequence \\\n",
    "    .pad_sequences(x1_dev, \n",
    "                   maxlen=MAX_SEQUENCE_LENGTH)\n",
    "\n",
    "\n",
    "x2_dev = keras \\\n",
    "    .preprocessing \\\n",
    "    .sequence \\\n",
    "    .pad_sequences(x2_dev, \n",
    "                   maxlen=MAX_SEQUENCE_LENGTH)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "58"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MAX_SEQUENCE_LENGTH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "## More code adapted from the keras reference (https://github.com/keras-team/keras/blob/master/examples/pretrained_word_embeddings.py)\n",
    "# prepare embedding matrix \n",
    "from keras.layers import Embedding\n",
    "from keras.initializers import Constant\n",
    "\n",
    "## EMBEDDING_DIM =  ## seems to need to match the embeddings_index dimension\n",
    "EMBEDDING_DIM = 300\n",
    "num_words = min(MAX_NUM_WORDS, len(word_index)) + 1\n",
    "embedding_matrix = np.zeros((num_words, EMBEDDING_DIM))\n",
    "for word, i in word_index.items():\n",
    "    if i > MAX_NUM_WORDS:\n",
    "        continue\n",
    "    embedding_vector = embeddings_index.get(word) ## This references the loaded embeddings dictionary\n",
    "    if embedding_vector is not None:\n",
    "        # words not found in embedding index will be all-zeros.\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "\n",
    "# load pre-trained word embeddings into an Embedding layer\n",
    "# note that we set trainable = False so as to keep the embeddings fixed\n",
    "embedding_layer = Embedding(num_words,\n",
    "                            EMBEDDING_DIM,\n",
    "                            embeddings_initializer=Constant(embedding_matrix),\n",
    "                            input_length=MAX_SEQUENCE_LENGTH,\n",
    "                            trainable=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "No module named 'keras_contrib'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-e1741521bcd4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallbacks\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mEarlyStopping\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mbackend\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mK\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras_contrib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcrf\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mCRF\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: No module named 'keras_contrib'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import argparse\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import random\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import json as json\n",
    "\n",
    "from keras import backend as Kbackend\n",
    "\n",
    "from keras.preprocessing.text import one_hot\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.layers import Dense, Flatten, Reshape\n",
    "from keras.layers import Dropout, Activation\n",
    "from keras.layers import Bidirectional, LSTM, GRU, TimeDistributed\n",
    "from keras.layers import Input, concatenate, add\n",
    "from keras.models import Model\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras import backend as K\n",
    "from keras_contrib.layers.crf import CRF\n",
    "\n",
    "\n",
    "gpu_options = tf.GPUOptions(allow_growth=True)\n",
    "sess = tf.Session(config=tf.ConfigProto(gpu_options=gpu_options))\n",
    "tf.keras.backend.set_session(sess)\n",
    "\n",
    "from numpy import array\n",
    "from keras.preprocessing.text import one_hot\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Flatten\n",
    "from keras.layers.embeddings import Embedding\n",
    "\n",
    "from keras.engine.topology import Layer\n",
    "from keras import initializers, regularizers, constraints\n",
    "\n",
    "class AttentionWithContext(Layer):\n",
    "    \"\"\"\n",
    "    Attention operation, with a context/query vector, for temporal data.\n",
    "    Supports Masking.\n",
    "    Follows the work of Yang et al. [https://www.cs.cmu.edu/~diyiy/docs/naacl16.pdf]\n",
    "    \"Hierarchical Attention Networks for Document Classification\"\n",
    "    by using a context vector to assist the attention\n",
    "    # Input shape\n",
    "        3D tensor with shape: `(samples, steps, features)`.\n",
    "    # Output shape\n",
    "        2D tensor with shape: `(samples, features)`.\n",
    "\n",
    "    How to use:\n",
    "    Just put it on top of an RNN Layer (GRU/LSTM/SimpleRNN) with return_sequences=True.\n",
    "    The dimensions are inferred based on the output shape of the RNN.\n",
    "\n",
    "    Note: The layer has been tested with Keras 2.0.6\n",
    "\n",
    "    Example:\n",
    "        model.add(LSTM(64, return_sequences=True))\n",
    "        model.add(AttentionWithContext())\n",
    "        # next add a Dense layer (for classification/regression) or whatever...\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 W_regularizer=None, u_regularizer=None, b_regularizer=None,\n",
    "                 W_constraint=None, u_constraint=None, b_constraint=None,\n",
    "                 bias=True, **kwargs):\n",
    "\n",
    "        self.supports_masking = True\n",
    "        self.init = initializers.get('glorot_uniform')\n",
    "\n",
    "        self.W_regularizer = regularizers.get(W_regularizer)\n",
    "        self.u_regularizer = regularizers.get(u_regularizer)\n",
    "        self.b_regularizer = regularizers.get(b_regularizer)\n",
    "\n",
    "        self.W_constraint = constraints.get(W_constraint)\n",
    "        self.u_constraint = constraints.get(u_constraint)\n",
    "        self.b_constraint = constraints.get(b_constraint)\n",
    "\n",
    "        self.bias = bias\n",
    "        super(AttentionWithContext, self).__init__(**kwargs)\n",
    "\n",
    "    def dot_product(self, x, kernel):\n",
    "        \"\"\"\n",
    "        Wrapper for dot product operation, in order to be compatible with both\n",
    "        Theano and Tensorflow\n",
    "        Args:\n",
    "            x (): input\n",
    "            kernel (): weights\n",
    "        Returns:\n",
    "        \"\"\"\n",
    "        if K.backend() == 'tensorflow':\n",
    "            return K.squeeze(K.dot(x, K.expand_dims(kernel)), axis=-1)\n",
    "        else:\n",
    "            return K.dot(x, kernel)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        assert len(input_shape) == 3\n",
    "\n",
    "        self.W = self.add_weight((input_shape[-1], input_shape[-1],),\n",
    "                                 initializer=self.init,\n",
    "                                 name='{}_W'.format(self.name),\n",
    "                                 regularizer=self.W_regularizer,\n",
    "                                 constraint=self.W_constraint)\n",
    "        if self.bias:\n",
    "            self.b = self.add_weight((input_shape[-1],),\n",
    "                                     initializer='zero',\n",
    "                                     name='{}_b'.format(self.name),\n",
    "                                     regularizer=self.b_regularizer,\n",
    "                                     constraint=self.b_constraint)\n",
    "\n",
    "        self.u = self.add_weight((input_shape[-1],),\n",
    "                                 initializer=self.init,\n",
    "                                 name='{}_u'.format(self.name),\n",
    "                                 regularizer=self.u_regularizer,\n",
    "                                 constraint=self.u_constraint)\n",
    "\n",
    "        super(AttentionWithContext, self).build(input_shape)\n",
    "\n",
    "    def compute_mask(self, input, input_mask=None):\n",
    "        # do not pass the mask to the next layers\n",
    "        return None\n",
    "\n",
    "    def call(self, x, mask=None):\n",
    "        uit = self.dot_product(x, self.W)\n",
    "\n",
    "        if self.bias:\n",
    "            uit += self.b\n",
    "\n",
    "        uit = K.tanh(uit)\n",
    "        \n",
    "        #ait = K.dot(uit, self.u)\n",
    "        ait = self.dot_product(uit, self.u)\n",
    "\n",
    "        a = K.exp(ait)\n",
    "\n",
    "        # apply mask after the exp. will be re-normalized next\n",
    "        if mask is not None:\n",
    "            # Cast the mask to floatX to avoid float64 upcasting in theano\n",
    "            a *= K.cast(mask, K.floatx())\n",
    "\n",
    "        # in some cases especially in the early stages of training the sum may be almost zero\n",
    "        # and this results in NaN's. A workaround is to add a very small positive number Îµ to the sum.\n",
    "        # a /= K.cast(K.sum(a, axis=1, keepdims=True), K.floatx())\n",
    "        a /= K.cast(K.sum(a, axis=1, keepdims=True) + K.epsilon(), K.floatx())\n",
    "\n",
    "        a = K.expand_dims(a)\n",
    "        weighted_input = x * a\n",
    "        return K.sum(weighted_input, axis=1)\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return input_shape[0], input_shape[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HAttLayer(Layer):\n",
    "    def __init__(self, attention_dim):\n",
    "        self.init = initializers.get('normal')\n",
    "        self.supports_masking = True\n",
    "        self.attention_dim = attention_dim\n",
    "        super(HAttLayer, self).__init__()\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        assert len(input_shape) == 3\n",
    "        self.W = K.variable(self.init((input_shape[-1], self.attention_dim)))\n",
    "        self.b = K.variable(self.init((self.attention_dim, )))\n",
    "        self.u = K.variable(self.init((self.attention_dim, 1)))\n",
    "        self.trainable_weights = [self.W, self.b, self.u]\n",
    "        super(HAttLayer, self).build(input_shape)\n",
    "\n",
    "    def compute_mask(self, inputs, mask=None):\n",
    "        return mask\n",
    "\n",
    "    def call(self, x, mask=None):\n",
    "        # size of x :[batch_size, sel_len, attention_dim]\n",
    "        # size of u :[batch_size, attention_dim]\n",
    "        # uit = tanh(xW+b)\n",
    "        uit = K.tanh(K.bias_add(K.dot(x, self.W), self.b))\n",
    "        ait = K.dot(uit, self.u)\n",
    "        ait = K.squeeze(ait, -1)\n",
    "\n",
    "        ait = K.exp(ait)\n",
    "\n",
    "        if mask is not None:\n",
    "            # Cast the mask to floatX to avoid float64 upcasting in theano\n",
    "            ait *= K.cast(mask, K.floatx())\n",
    "        ait /= K.cast(K.sum(ait, axis=1, keepdims=True) + K.epsilon(), K.floatx())\n",
    "        ait = K.expand_dims(ait)\n",
    "        weighted_input = x * ait\n",
    "        output = K.sum(weighted_input, axis=1)\n",
    "\n",
    "        return output\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return (input_shape[0], input_shape[-1])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "#list of numpy arrays as input\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "concat_train=[]\n",
    "for x in range(len(x1_train)):\n",
    "    concat_train.append(np.concatenate((x1_train[x], x2_train[x])))    #5708\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "concat_dev=[]\n",
    "for x in range(len(x1_dev)):\n",
    "    concat_dev.append(np.concatenate((x1_dev[x], x2_dev[x])))         #537"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "concat_train_l=[]\n",
    "concat_dev_l=[]\n",
    "\n",
    "for x in range(len(train_label1)):\n",
    "    \n",
    "    concat_train_l.append(np.concatenate((train_label1[x], train_label2[x])))\n",
    "\n",
    "\n",
    "for x in range(len(dev_label1)):\n",
    "    \n",
    "    concat_dev_l.append(np.concatenate((dev_label1[x], dev_label2[x])))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    " #10000: O, 01000: B, 00100: I, 00010: E, 00001: S\n",
    "\n",
    "al=[]\n",
    "for x in train_label1:  #x is each sentence\n",
    "    all_phrases=[]\n",
    "    for y in x:      #each word\n",
    "        if np.array_equal(y,OO)==False:   #if the word is part of paraphrase\n",
    "            all_phrases.append(y)\n",
    "    al.append(all_phrases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[array([0., 1., 0., 0., 0.], dtype=float32),\n",
       "  array([0., 0., 0., 1., 0.], dtype=float32)],\n",
       " [array([0., 1., 0., 0., 0.], dtype=float32),\n",
       "  array([0., 0., 0., 1., 0.], dtype=float32)],\n",
       " [array([0., 1., 0., 0., 0.], dtype=float32),\n",
       "  array([0., 0., 0., 1., 0.], dtype=float32)],\n",
       " [array([0., 1., 0., 0., 0.], dtype=float32),\n",
       "  array([0., 0., 0., 1., 0.], dtype=float32)],\n",
       " [array([0., 1., 0., 0., 0.], dtype=float32),\n",
       "  array([0., 0., 0., 1., 0.], dtype=float32)],\n",
       " [array([0., 1., 0., 0., 0.], dtype=float32),\n",
       "  array([0., 0., 0., 1., 0.], dtype=float32)],\n",
       " [array([0., 1., 0., 0., 0.], dtype=float32),\n",
       "  array([0., 0., 0., 1., 0.], dtype=float32)],\n",
       " [array([0., 1., 0., 0., 0.], dtype=float32),\n",
       "  array([0., 0., 0., 1., 0.], dtype=float32)],\n",
       " [array([0., 1., 0., 0., 0.], dtype=float32),\n",
       "  array([0., 0., 0., 1., 0.], dtype=float32)],\n",
       " [array([0., 1., 0., 0., 0.], dtype=float32),\n",
       "  array([0., 0., 0., 1., 0.], dtype=float32)]]"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "al[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "OO=train_label1[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_9 (InputLayer)            (None, 58)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_10 (InputLayer)           (None, 58)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_2 (Embedding)         (None, 58, 300)      2694900     input_9[0][0]                    \n",
      "                                                                 input_10[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_4 (Bidirectional) (None, 58, 256)      439296      embedding_2[0][0]                \n",
      "                                                                 embedding_2[1][0]                \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_4 (Concatenate)     (None, 116, 256)     0           bidirectional_4[0][0]            \n",
      "                                                                 bidirectional_4[1][0]            \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_4 (TimeDistrib (None, 116, 5)       1285        concatenate_4[0][0]              \n",
      "==================================================================================================\n",
      "Total params: 3,135,481\n",
      "Trainable params: 440,581\n",
      "Non-trainable params: 2,694,900\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "from keras import backend as K\n",
    "\n",
    "#this is arbitrary?\n",
    "\n",
    "\n",
    "NUM_LSTM_UNITS = 128 #output dimension\n",
    "\n",
    "\n",
    "NUM_EMBEDDING_DIM = EMBEDDING_DIM\n",
    "\n",
    "top_input = Input(\n",
    "    shape=(MAX_SEQUENCE_LENGTH, ),    #this is the first sentence\n",
    "    dtype='int32')\n",
    "\n",
    "bm_input = Input(\n",
    "    shape=(MAX_SEQUENCE_LENGTH, ),    #this is the first sentence\n",
    "    dtype='int32')\n",
    "\n",
    "top_embedded = embedding_layer(\n",
    "    top_input)\n",
    "\n",
    "bm_embedded=embedding_layer(bm_input)\n",
    "\n",
    "\n",
    "BiLSTM = Bidirectional(LSTM(NUM_LSTM_UNITS, return_sequences=True))\n",
    "\n",
    "\n",
    "\n",
    "top_output = BiLSTM(top_embedded)\n",
    "\n",
    "bm_output = BiLSTM(bm_embedded)\n",
    "\n",
    "\n",
    "\n",
    "merged = concatenate(\n",
    "    [top_output,bm_output], \n",
    "    axis=1)\n",
    "\n",
    "\n",
    "dense =  TimeDistributed(Dense(\n",
    "    units=5, \n",
    "    activation='softmax'))\n",
    "\n",
    "predictions1=dense(merged)\n",
    "\n",
    "\n",
    "#predictions2 = dense(merged)\n",
    "\n",
    "\n",
    "model = Model(\n",
    "    inputs=[top_input, bm_input], \n",
    "    outputs=[predictions1])\n",
    "\n",
    "\n",
    "model.summary()\n",
    "\n",
    "model.compile(\n",
    "    optimizer='Adam',\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=['categorical_accuracy'])\n",
    "\n",
    "BATCH_SIZE = 72\n",
    "\n",
    "NUM_EPOCHS = 30\n",
    "\n",
    "earlyStopping = EarlyStopping(monitor='val_loss', patience=10, verbose=1, mode='min')\n",
    "mcp_save = ModelCheckpoint('../Phrase_Models_H5/only_true.h5', save_best_only=True,  verbose=1, monitor='val_loss', mode='min')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 2512 samples, validate on 393 samples\n",
      "Epoch 1/30\n",
      "2512/2512 [==============================] - 16s 6ms/step - loss: 0.4098 - categorical_accuracy: 0.9401 - val_loss: 0.1611 - val_categorical_accuracy: 0.9700\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.16108, saving model to ../Phrase_Models_H5/only_true.h5\n",
      "Epoch 2/30\n",
      "2512/2512 [==============================] - 13s 5ms/step - loss: 0.1665 - categorical_accuracy: 0.9660 - val_loss: 0.1375 - val_categorical_accuracy: 0.9700\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.16108 to 0.13748, saving model to ../Phrase_Models_H5/only_true.h5\n",
      "Epoch 3/30\n",
      "2512/2512 [==============================] - 13s 5ms/step - loss: 0.1648 - categorical_accuracy: 0.9660 - val_loss: 0.1379 - val_categorical_accuracy: 0.9700\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.13748\n",
      "Epoch 4/30\n",
      "2512/2512 [==============================] - 13s 5ms/step - loss: 0.1645 - categorical_accuracy: 0.9660 - val_loss: 0.1397 - val_categorical_accuracy: 0.9700\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.13748\n",
      "Epoch 5/30\n",
      "2512/2512 [==============================] - 13s 5ms/step - loss: 0.1642 - categorical_accuracy: 0.9660 - val_loss: 0.1379 - val_categorical_accuracy: 0.9700\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.13748\n",
      "Epoch 6/30\n",
      "2512/2512 [==============================] - 13s 5ms/step - loss: 0.1640 - categorical_accuracy: 0.9660 - val_loss: 0.1380 - val_categorical_accuracy: 0.9700\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.13748\n",
      "Epoch 7/30\n",
      "2512/2512 [==============================] - 13s 5ms/step - loss: 0.1640 - categorical_accuracy: 0.9660 - val_loss: 0.1393 - val_categorical_accuracy: 0.9700\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.13748\n",
      "Epoch 8/30\n",
      "2512/2512 [==============================] - 13s 5ms/step - loss: 0.1639 - categorical_accuracy: 0.9660 - val_loss: 0.1389 - val_categorical_accuracy: 0.9700\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.13748\n",
      "Epoch 9/30\n",
      "2512/2512 [==============================] - 13s 5ms/step - loss: 0.1639 - categorical_accuracy: 0.9660 - val_loss: 0.1389 - val_categorical_accuracy: 0.9700\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.13748\n",
      "Epoch 10/30\n",
      "2512/2512 [==============================] - 13s 5ms/step - loss: 0.1635 - categorical_accuracy: 0.9660 - val_loss: 0.1405 - val_categorical_accuracy: 0.9700\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.13748\n",
      "Epoch 11/30\n",
      "2512/2512 [==============================] - 13s 5ms/step - loss: 0.1634 - categorical_accuracy: 0.9660 - val_loss: 0.1388 - val_categorical_accuracy: 0.9700\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 0.13748\n",
      "Epoch 12/30\n",
      "2512/2512 [==============================] - 13s 5ms/step - loss: 0.1636 - categorical_accuracy: 0.9660 - val_loss: 0.1386 - val_categorical_accuracy: 0.9700\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 0.13748\n",
      "Epoch 00012: early stopping\n"
     ]
    }
   ],
   "source": [
    "\n",
    "history = model.fit(x=[x1_train, x2_train], y=np.array(concat_train_l), batch_size=BATCH_SIZE, epochs=NUM_EPOCHS, callbacks=[earlyStopping,mcp_save], validation_data=([x1_dev, x2_dev], np.array(concat_dev_l)),\n",
    "    \n",
    "    shuffle=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'tolist'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-325-0d8a32033a45>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mx1_label_d\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mx1_label_d\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mx2_label_d\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mx2_label_d\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mnew\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx1_label_d\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mzz\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx1_label_d\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mx2_label_d\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'tolist'"
     ]
    }
   ],
   "source": [
    "x1_label_d=x1_label_d.tolist()\n",
    "x2_label_d=x2_label_d.tolist()\n",
    "new=[]\n",
    "for x in range (len(x1_label_d)):\n",
    "    zz=(x1_label_d[x]+x2_label_d[x])\n",
    "    new2.append(zz)\n",
    "new2=np.asarray(new2)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Error when checking model target: the list of Numpy arrays that you are passing to your model is not the size the model expected. Expected to see 1 array(s), but instead got the following list of 2 arrays: [array([[[1., 0., 0., 0., 0.],\n        [1., 0., 0., 0., 0.],\n        [1., 0., 0., 0., 0.],\n        ...,\n        [1., 0., 0., 0., 0.],\n        [1., 0., 0., 0., 0.],\n        [1., 0., 0., 0., 0.]],\n\n    ...",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-335-2b8819482f13>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m history = model.fit(x=[x1_train, x2_train], y=[train_label1, train_label2], batch_size=BATCH_SIZE, epochs=NUM_EPOCHS, callbacks=[earlyStopping,mcp_save], validation_data=([x1_dev,x2_dev], [dev_label1, dev_label2]),\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m )\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m    950\u001b[0m             \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    951\u001b[0m             \u001b[0mclass_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 952\u001b[0;31m             batch_size=batch_size)\n\u001b[0m\u001b[1;32m    953\u001b[0m         \u001b[0;31m# Prepare validation data.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    954\u001b[0m         \u001b[0mdo_validation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[0;34m(self, x, y, sample_weight, class_weight, check_array_lengths, batch_size)\u001b[0m\n\u001b[1;32m    787\u001b[0m                 \u001b[0mfeed_output_shapes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    788\u001b[0m                 \u001b[0mcheck_batch_axis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# Don't enforce the batch size.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 789\u001b[0;31m                 exception_prefix='target')\n\u001b[0m\u001b[1;32m    790\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    791\u001b[0m             \u001b[0;31m# Generate sample-wise weight values given the `sample_weight` and\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/engine/training_utils.py\u001b[0m in \u001b[0;36mstandardize_input_data\u001b[0;34m(data, names, shapes, check_batch_axis, exception_prefix)\u001b[0m\n\u001b[1;32m    100\u001b[0m                 \u001b[0;34m'Expected to see '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnames\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' array(s), '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m                 \u001b[0;34m'but instead got the following list of '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 102\u001b[0;31m                 str(len(data)) + ' arrays: ' + str(data)[:200] + '...')\n\u001b[0m\u001b[1;32m    103\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnames\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m             raise ValueError(\n",
      "\u001b[0;31mValueError\u001b[0m: Error when checking model target: the list of Numpy arrays that you are passing to your model is not the size the model expected. Expected to see 1 array(s), but instead got the following list of 2 arrays: [array([[[1., 0., 0., 0., 0.],\n        [1., 0., 0., 0., 0.],\n        [1., 0., 0., 0., 0.],\n        ...,\n        [1., 0., 0., 0., 0.],\n        [1., 0., 0., 0., 0.],\n        [1., 0., 0., 0., 0.]],\n\n    ..."
     ]
    }
   ],
   "source": [
    "\n",
    "history = model.fit(x=[x1_train, x2_train], y=[train_label1, train_label2], batch_size=BATCH_SIZE, epochs=NUM_EPOCHS, callbacks=[earlyStopping,mcp_save], validation_data=([x1_dev,x2_dev], [dev_label1, dev_label2]),\n",
    "    \n",
    "    shuffle=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 5708 samples, validate on 537 samples\n",
      "Epoch 1/30\n",
      "5708/5708 [==============================] - 31s 5ms/step - loss: 0.0606 - dense_26_loss: 0.0255 - dense_26_acc: 0.0801 - dense_26_acc_1: 0.0879 - val_loss: 0.0363 - val_dense_26_loss: 0.0112 - val_dense_26_acc: 0.0577 - val_dense_26_acc_1: 0.0801\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.03632, saving model to ../Phrase_Models_H5/big_merge_attention.h5\n",
      "Epoch 2/30\n",
      "5708/5708 [==============================] - 25s 4ms/step - loss: 0.0590 - dense_26_loss: 0.0248 - dense_26_acc: 0.1228 - dense_26_acc_1: 0.1312 - val_loss: 0.0367 - val_dense_26_loss: 0.0111 - val_dense_26_acc: 0.0633 - val_dense_26_acc_1: 0.1080\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 0.03632\n",
      "Epoch 3/30\n",
      "5708/5708 [==============================] - 25s 4ms/step - loss: 0.0554 - dense_26_loss: 0.0234 - dense_26_acc: 0.1869 - dense_26_acc_1: 0.1910 - val_loss: 0.0365 - val_dense_26_loss: 0.0112 - val_dense_26_acc: 0.0596 - val_dense_26_acc_1: 0.1527\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.03632\n",
      "Epoch 4/30\n",
      "5708/5708 [==============================] - 25s 4ms/step - loss: 0.0513 - dense_26_loss: 0.0217 - dense_26_acc: 0.2409 - dense_26_acc_1: 0.2565 - val_loss: 0.0376 - val_dense_26_loss: 0.0117 - val_dense_26_acc: 0.0689 - val_dense_26_acc_1: 0.1993\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.03632\n",
      "Epoch 5/30\n",
      "5708/5708 [==============================] - 24s 4ms/step - loss: 0.0474 - dense_26_loss: 0.0202 - dense_26_acc: 0.2880 - dense_26_acc_1: 0.3276 - val_loss: 0.0389 - val_dense_26_loss: 0.0120 - val_dense_26_acc: 0.0559 - val_dense_26_acc_1: 0.1713\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.03632\n",
      "Epoch 6/30\n",
      "5708/5708 [==============================] - 24s 4ms/step - loss: 0.0445 - dense_26_loss: 0.0189 - dense_26_acc: 0.3138 - dense_26_acc_1: 0.3670 - val_loss: 0.0388 - val_dense_26_loss: 0.0119 - val_dense_26_acc: 0.0987 - val_dense_26_acc_1: 0.2421\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.03632\n",
      "Epoch 7/30\n",
      "5708/5708 [==============================] - 24s 4ms/step - loss: 0.0422 - dense_26_loss: 0.0179 - dense_26_acc: 0.3395 - dense_26_acc_1: 0.4122 - val_loss: 0.0392 - val_dense_26_loss: 0.0121 - val_dense_26_acc: 0.0875 - val_dense_26_acc_1: 0.1769\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.03632\n",
      "Epoch 8/30\n",
      "5708/5708 [==============================] - 25s 4ms/step - loss: 0.0399 - dense_26_loss: 0.0169 - dense_26_acc: 0.3483 - dense_26_acc_1: 0.4434 - val_loss: 0.0398 - val_dense_26_loss: 0.0123 - val_dense_26_acc: 0.0857 - val_dense_26_acc_1: 0.1881\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.03632\n",
      "Epoch 9/30\n",
      "5708/5708 [==============================] - 25s 4ms/step - loss: 0.0382 - dense_26_loss: 0.0162 - dense_26_acc: 0.3646 - dense_26_acc_1: 0.4713 - val_loss: 0.0407 - val_dense_26_loss: 0.0127 - val_dense_26_acc: 0.0652 - val_dense_26_acc_1: 0.1490\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.03632\n",
      "Epoch 10/30\n",
      "5708/5708 [==============================] - 25s 4ms/step - loss: 0.0369 - dense_26_loss: 0.0156 - dense_26_acc: 0.3700 - dense_26_acc_1: 0.4928 - val_loss: 0.0399 - val_dense_26_loss: 0.0125 - val_dense_26_acc: 0.0912 - val_dense_26_acc_1: 0.1918\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.03632\n",
      "Epoch 11/30\n",
      "5708/5708 [==============================] - 25s 4ms/step - loss: 0.0357 - dense_26_loss: 0.0151 - dense_26_acc: 0.3816 - dense_26_acc_1: 0.5061 - val_loss: 0.0398 - val_dense_26_loss: 0.0125 - val_dense_26_acc: 0.1192 - val_dense_26_acc_1: 0.2104\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 0.03632\n",
      "Epoch 00011: early stopping\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(x=[x1_train, x2_train], y=[x1_label_t, x2_label_t], batch_size=BATCH_SIZE, epochs=NUM_EPOCHS, callbacks=[earlyStopping,mcp_save], validation_data=([x1_dev,x2_dev],[x1_label_d, x2_label_d]),\n",
    "    \n",
    "    shuffle=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_20 (InputLayer)           (None, 58)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_21 (InputLayer)           (None, 58)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_3 (Embedding)         (None, 58, 300)      3000300     input_20[0][0]                   \n",
      "                                                                 input_21[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_12 (Bidirectional (None, 58, 256)      439296      embedding_3[14][0]               \n",
      "                                                                 embedding_3[15][0]               \n",
      "__________________________________________________________________________________________________\n",
      "dense_12 (Dense)                (None, 58, 290)      74530       bidirectional_12[0][0]           \n",
      "                                                                 bidirectional_12[1][0]           \n",
      "==================================================================================================\n",
      "Total params: 3,514,126\n",
      "Trainable params: 513,826\n",
      "Non-trainable params: 3,000,300\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#BiLSTM without attention\n",
    "\n",
    "\n",
    "#this is arbitrary?\n",
    "\n",
    "NUM_LSTM_UNITS = 128 #output dimension\n",
    "\n",
    "\n",
    "NUM_EMBEDDING_DIM = EMBEDDING_DIM\n",
    "\n",
    "top_input = Input(\n",
    "    shape=(MAX_SEQUENCE_LENGTH, ),    #this is the first sentence\n",
    "    )\n",
    "\n",
    "bm_input = Input(\n",
    "    shape=(MAX_SEQUENCE_LENGTH, ),   #this is the second\n",
    "    )\n",
    "\n",
    "\n",
    "top_embedded = embedding_layer(\n",
    "    top_input)\n",
    "bm_embedded = embedding_layer(\n",
    "    bm_input)\n",
    "\n",
    "BiLSTM = Bidirectional(LSTM(NUM_LSTM_UNITS, return_sequences=True))\n",
    "\n",
    "\n",
    "\n",
    "top_output = BiLSTM(top_embedded)\n",
    "\n",
    "bm_output = BiLSTM(bm_embedded)\n",
    "\n",
    "\n",
    "  \n",
    "\n",
    "dense =  Dense(\n",
    "    units=290, \n",
    "    activation='sigmoid')\n",
    "\n",
    "predictions1 = dense(top_output)\n",
    "\n",
    "predictions2 = dense(bm_output)\n",
    "\n",
    "\n",
    "model = Model(\n",
    "    inputs=[top_input, bm_input], \n",
    "    outputs=[predictions1, predictions2])\n",
    "\n",
    "\n",
    "model.summary()\n",
    "\n",
    "model.compile(\n",
    "    optimizer='Adam',\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=['categorical_accuracy'])\n",
    "\n",
    "BATCH_SIZE = 72\n",
    "\n",
    "NUM_EPOCHS = 30\n",
    "\n",
    "earlyStopping = EarlyStopping(monitor='val_loss', patience=7, verbose=1, mode='min')\n",
    "mcp_save = ModelCheckpoint('../Phrase_Models_H5/big_merge.h5', save_best_only=True,  verbose=1, monitor='val_loss', mode='min')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5708, 58, 5)"
      ]
     },
     "execution_count": 250,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Error when checking target: expected dense_12 to have shape (58, 290) but got array with shape (58, 5)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-211-c3e1ec4dee70>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m history = model.fit(x=[x1_train, x2_train], y=[train_label1, train_label2], batch_size=BATCH_SIZE, epochs=NUM_EPOCHS, callbacks=[earlyStopping,mcp_save], validation_data=([x1_dev, x2_dev],[dev_label1, dev_label2]),\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m )\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m    950\u001b[0m             \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    951\u001b[0m             \u001b[0mclass_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 952\u001b[0;31m             batch_size=batch_size)\n\u001b[0m\u001b[1;32m    953\u001b[0m         \u001b[0;31m# Prepare validation data.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    954\u001b[0m         \u001b[0mdo_validation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[0;34m(self, x, y, sample_weight, class_weight, check_array_lengths, batch_size)\u001b[0m\n\u001b[1;32m    787\u001b[0m                 \u001b[0mfeed_output_shapes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    788\u001b[0m                 \u001b[0mcheck_batch_axis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# Don't enforce the batch size.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 789\u001b[0;31m                 exception_prefix='target')\n\u001b[0m\u001b[1;32m    790\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    791\u001b[0m             \u001b[0;31m# Generate sample-wise weight values given the `sample_weight` and\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/engine/training_utils.py\u001b[0m in \u001b[0;36mstandardize_input_data\u001b[0;34m(data, names, shapes, check_batch_axis, exception_prefix)\u001b[0m\n\u001b[1;32m    136\u001b[0m                             \u001b[0;34m': expected '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mnames\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' to have shape '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    137\u001b[0m                             \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' but got array with shape '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 138\u001b[0;31m                             str(data_shape))\n\u001b[0m\u001b[1;32m    139\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Error when checking target: expected dense_12 to have shape (58, 290) but got array with shape (58, 5)"
     ]
    }
   ],
   "source": [
    "history = model.fit(x=[x1_train, x2_train], y=[train_label1, train_label2], batch_size=BATCH_SIZE, epochs=NUM_EPOCHS, callbacks=[earlyStopping,mcp_save], validation_data=([x1_dev, x2_dev],[dev_label1, dev_label2]),\n",
    "    \n",
    "    shuffle=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved model to disk\n"
     ]
    }
   ],
   "source": [
    "\n",
    "model.save(\"BiLSTM_PAWS_QQP.h5\")        ##first attempt\n",
    "print(\"Saved model to disk\")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = load_model(\"../Phrase_Models_H5/only_true.h5\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "predicts = model.predict(\n",
    "    [x1_train, x2_train])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2.1087265e-04 9.9950051e-01 5.8515518e-05 1.1890196e-04 1.1118726e-04]\n",
      "[0. 1. 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "print(predicts[0][22])\n",
    "print(concat_train_l[0][22])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., 1., 0.], dtype=float32)"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "                #10000: O, 01000: B, 00100: I, 00010: E, 00001: S\n",
    "    \n",
    "dicti={}    #the sentence and word of error\n",
    "errs = []\n",
    "truth=[]\n",
    "sen1=[]\n",
    "sen2=[]\n",
    "id_sent=[]\n",
    "id_word=[]\n",
    "\n",
    "for x in range(len(predicts)): \n",
    "    errr=[]\n",
    "    for y in range(len(predicts[x])):\n",
    "        \n",
    "        if (np.argmax(predicts[x][y]))!=(np.argmax(concat_train_l[x][y])):\n",
    "            \n",
    "            truth.append(concat_train_l[x][y])\n",
    "            errs.append(predicts[x][y])\n",
    "            errr.append(y)\n",
    "            id_sent.append(x)\n",
    "            id_word.append(y)\n",
    "            \n",
    "            sen1.append(train_sen1[x])\n",
    "            sen2.append(train_sen2[x])\n",
    "            \n",
    "    dicti[x]=errr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2512"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dicti)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Tabl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 355,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['for',\n",
       " 'their',\n",
       " 'performances',\n",
       " 'in',\n",
       " 'the',\n",
       " 'game',\n",
       " ',',\n",
       " 'quarterback',\n",
       " 'jameis',\n",
       " 'winston',\n",
       " 'and',\n",
       " 'defensive',\n",
       " 'back',\n",
       " 'p.',\n",
       " 'j.',\n",
       " 'williams',\n",
       " 'were',\n",
       " 'named',\n",
       " 'the',\n",
       " 'game',\n",
       " \"'s\",\n",
       " 'most',\n",
       " 'valuable',\n",
       " 'players',\n",
       " '.']"
      ]
     },
     "execution_count": 355,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "err_ana.column(4)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 348,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5707"
      ]
     },
     "execution_count": 348,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(err_ana.column(0)[22178])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 343,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-343-c8ce6edbfe96>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0merr_ana\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0merr_ana\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m22178\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0merr_ana\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m22178\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "err_ana.column(4)[err_ana.column(0)[22178]][err_ana.column(1)[22178]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'err_ana' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-72-b9b3f89cd1aa>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0msen1_err\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0msen2_err\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0merr_ana\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_rows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0merr_ana\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m<\u001b[0m\u001b[0;36m58\u001b[0m\u001b[0;34m:\u001b[0m   \u001b[0;31m#in the first sentence\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0msen1_err\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_sen1\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0merr_ana\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0merr_ana\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'err_ana' is not defined"
     ]
    }
   ],
   "source": [
    "sen1_err=[]\n",
    "sen2_err=[]\n",
    "for x in range (err_ana.num_rows):\n",
    "    if err_ana.column(1)[x]<58:   #in the first sentence\n",
    "        sen1_err.append(train_sen1[err_ana.column(0)[x]][err_ana.column(1)[x]])\n",
    "        sen2_err.append(0)\n",
    "    else:\n",
    "        sen1_err.append(0)\n",
    "        sen2_err.append(train_sen2[err_ana.column(0)[x]][(err_ana.column(1)[x]-58)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['the',\n",
       " 'leaves',\n",
       " 'are',\n",
       " 'usually',\n",
       " '1.5-4',\n",
       " 'mm',\n",
       " 'long',\n",
       " 'and',\n",
       " '0.2-0.7',\n",
       " 'mm',\n",
       " 'wide',\n",
       " '.']"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "sen2[2]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "    <thead>\n",
       "        <tr>\n",
       "            <th>id_sent</th> <th>id_word</th> <th>ground truth</th> <th>predictions</th> <th>sen1</th> <th>sen2</th>\n",
       "        </tr>\n",
       "    </thead>\n",
       "    <tbody>\n",
       "        <tr>\n",
       "            <td>0      </td> <td>2      </td> <td>[0. 1. 0. 0. 0.]</td> <td>[0.8814463  0.04183972 0.01579723 0.04140313 0.01951371]</td> <td>['the', 'leaves', 'are', 'generally', '1.5-4', 'mm', 'lo ...</td> <td>['the', 'leaves', 'are', 'usually', '1.5-4', 'mm', 'long ...</td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>0      </td> <td>3      </td> <td>[0. 0. 0. 1. 0.]</td> <td>[0.88554895 0.04035616 0.01525072 0.04010696 0.01873713]</td> <td>['the', 'leaves', 'are', 'generally', '1.5-4', 'mm', 'lo ...</td> <td>['the', 'leaves', 'are', 'usually', '1.5-4', 'mm', 'long ...</td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>0      </td> <td>60     </td> <td>[0. 1. 0. 0. 0.]</td> <td>[0.8814463  0.0418397  0.01579722 0.04140312 0.01951369]</td> <td>['the', 'leaves', 'are', 'generally', '1.5-4', 'mm', 'lo ...</td> <td>['the', 'leaves', 'are', 'usually', '1.5-4', 'mm', 'long ...</td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>0      </td> <td>61     </td> <td>[0. 0. 0. 1. 0.]</td> <td>[0.8855492  0.04035613 0.0152507  0.04010694 0.01873711]</td> <td>['the', 'leaves', 'are', 'generally', '1.5-4', 'mm', 'lo ...</td> <td>['the', 'leaves', 'are', 'usually', '1.5-4', 'mm', 'long ...</td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>1      </td> <td>14     </td> <td>[0. 1. 0. 0. 0.]</td> <td>[0.93328744 0.0230635  0.00891811 0.02469879 0.01003214]</td> <td>['lemmings', ',', 'by', 'contrast', ',', 'are', 'conspic ...</td> <td>['by', 'contrast', ',', 'the', 'lemmings', 'are', 'strik ...</td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>1      </td> <td>15     </td> <td>[0. 0. 0. 1. 0.]</td> <td>[0.9380454  0.0213599  0.00828395 0.02310042 0.0092104 ]</td> <td>['lemmings', ',', 'by', 'contrast', ',', 'are', 'conspic ...</td> <td>['by', 'contrast', ',', 'the', 'lemmings', 'are', 'strik ...</td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>1      </td> <td>73     </td> <td>[0. 0. 0. 0. 1.]</td> <td>[0.9380451  0.02136038 0.00828366 0.02310061 0.0092103 ]</td> <td>['lemmings', ',', 'by', 'contrast', ',', 'are', 'conspic ...</td> <td>['by', 'contrast', ',', 'the', 'lemmings', 'are', 'strik ...</td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>2      </td> <td>10     </td> <td>[0. 1. 0. 0. 0.]</td> <td>[0.9149856  0.02965363 0.01134732 0.03073235 0.01328119]</td> <td>['of', 'the', 'twelve', 'stories', 'that', 'are', 'inclu ...</td> <td>['of', 'the', 'twelve', 'stories', 'included', ',', 'six ...</td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>2      </td> <td>11     </td> <td>[0. 0. 0. 1. 0.]</td> <td>[0.9194552  0.02803647 0.01075368 0.02927791 0.01247689]</td> <td>['of', 'the', 'twelve', 'stories', 'that', 'are', 'inclu ...</td> <td>['of', 'the', 'twelve', 'stories', 'included', ',', 'six ...</td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>2      </td> <td>67     </td> <td>[0. 0. 0. 0. 1.]</td> <td>[0.9105881  0.0312495  0.01193169 0.03215189 0.01407877]</td> <td>['of', 'the', 'twelve', 'stories', 'that', 'are', 'inclu ...</td> <td>['of', 'the', 'twelve', 'stories', 'included', ',', 'six ...</td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>3      </td> <td>9      </td> <td>[0. 1. 0. 0. 0.]</td> <td>[0.9105727  0.03125865 0.0119359  0.03215284 0.01407997]</td> <td>['he', 'moved', 'to', 'quebec', 'in', '1685', 'and', 'li ...</td> <td>['he', 'moved', 'to', 'quebec', 'around', '1685', 'and', ...</td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>3      </td> <td>10     </td> <td>[0. 0. 0. 1. 0.]</td> <td>[0.91495925 0.02966911 0.01135431 0.03073397 0.01328337]</td> <td>['he', 'moved', 'to', 'quebec', 'in', '1685', 'and', 'li ...</td> <td>['he', 'moved', 'to', 'quebec', 'around', '1685', 'and', ...</td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>3      </td> <td>70     </td> <td>[0. 0. 0. 0. 1.]</td> <td>[0.9239681  0.02641496 0.0101583  0.02778976 0.01166892]</td> <td>['he', 'moved', 'to', 'quebec', 'in', '1685', 'and', 'li ...</td> <td>['he', 'moved', 'to', 'quebec', 'around', '1685', 'and', ...</td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>4      </td> <td>1      </td> <td>[0. 1. 0. 0. 0.]</td> <td>[0.87728673 0.04332945 0.01635224 0.04272831 0.02030317]</td> <td>['the', 'world', \"'s\", 'first', 'laser', 'was', 'develop ...</td> <td>['in', '1960', ',', 'the', 'world', \"'s\", 'first', 'lase ...</td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>4      </td> <td>2      </td> <td>[0. 0. 0. 1. 0.]</td> <td>[0.8814677  0.04182826 0.0157917  0.04140076 0.01951157]</td> <td>['the', 'world', \"'s\", 'first', 'laser', 'was', 'develop ...</td> <td>['in', '1960', ',', 'the', 'world', \"'s\", 'first', 'lase ...</td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>4      </td> <td>62     </td> <td>[0. 0. 0. 0. 1.]</td> <td>[0.88965803 0.03885842 0.01470231 0.03881698 0.0179642 ]</td> <td>['the', 'world', \"'s\", 'first', 'laser', 'was', 'develop ...</td> <td>['in', '1960', ',', 'the', 'world', \"'s\", 'first', 'lase ...</td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>5      </td> <td>1      </td> <td>[0. 1. 0. 0. 0.]</td> <td>[0.8772676  0.04333948 0.01635723 0.04273054 0.02030517]</td> <td>['now', 'resolve', 'the', 'indifference', 'bid', 'price' ...</td> <td>['now', 'to', 'solve', 'the', 'indifference', 'bid', 'pr ...</td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>5      </td> <td>2      </td> <td>[0. 0. 0. 1. 0.]</td> <td>[0.8814458  0.04183986 0.01579737 0.04140317 0.01951375]</td> <td>['now', 'resolve', 'the', 'indifference', 'bid', 'price' ...</td> <td>['now', 'to', 'solve', 'the', 'indifference', 'bid', 'pr ...</td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>5      </td> <td>60     </td> <td>[0. 1. 0. 0. 0.]</td> <td>[0.8814465  0.04183966 0.01579719 0.04140306 0.01951368]</td> <td>['now', 'resolve', 'the', 'indifference', 'bid', 'price' ...</td> <td>['now', 'to', 'solve', 'the', 'indifference', 'bid', 'pr ...</td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>5      </td> <td>61     </td> <td>[0. 0. 0. 1. 0.]</td> <td>[0.88554925 0.0403561  0.01525068 0.04010688 0.01873709]</td> <td>['now', 'resolve', 'the', 'indifference', 'bid', 'price' ...</td> <td>['now', 'to', 'solve', 'the', 'indifference', 'bid', 'pr ...</td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>6      </td> <td>4      </td> <td>[0. 1. 0. 0. 0.]</td> <td>[0.88963515 0.03887125 0.01470757 0.03881982 0.01796634]</td> <td>['google', 'allows', 'business', 'owners', 'to', 'check' ...</td> <td>['google', 'allows', 'business', 'owners', 'to', 'check' ...</td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>6      </td> <td>16     </td> <td>[0. 0. 0. 1. 0.]</td> <td>[0.94287163 0.01963304 0.00763705 0.02146978 0.00838848]</td> <td>['google', 'allows', 'business', 'owners', 'to', 'check' ...</td> <td>['google', 'allows', 'business', 'owners', 'to', 'check' ...</td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>6      </td> <td>62     </td> <td>[0. 1. 0. 0. 0.]</td> <td>[0.8896372  0.03886995 0.01470707 0.03881965 0.01796609]</td> <td>['google', 'allows', 'business', 'owners', 'to', 'check' ...</td> <td>['google', 'allows', 'business', 'owners', 'to', 'check' ...</td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>6      </td> <td>63     </td> <td>[0. 0. 0. 1. 0.]</td> <td>[0.8937285  0.0373812  0.01416499 0.03752802 0.01719726]</td> <td>['google', 'allows', 'business', 'owners', 'to', 'check' ...</td> <td>['google', 'allows', 'business', 'owners', 'to', 'check' ...</td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>7      </td> <td>23     </td> <td>[0. 1. 0. 0. 0.]</td> <td>[0.97574824 0.00807399 0.00321943 0.00982557 0.00313282]</td> <td>['the', 'festival', 'was', 'founded', 'in', '2007', 'and ...</td> <td>['the', 'festival', 'originated', 'in', '2007', 'and', ' ...</td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>7      </td> <td>24     </td> <td>[0. 0. 0. 1. 0.]</td> <td>[0.97974086 0.00670441 0.00268447 0.00832231 0.00254791]</td> <td>['the', 'festival', 'was', 'founded', 'in', '2007', 'and ...</td> <td>['the', 'festival', 'originated', 'in', '2007', 'and', ' ...</td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>7      </td> <td>71     </td> <td>[0. 0. 0. 0. 1.]</td> <td>[0.92863953 0.0247242  0.00953168 0.02625732 0.01084733]</td> <td>['the', 'festival', 'was', 'founded', 'in', '2007', 'and ...</td> <td>['the', 'festival', 'originated', 'in', '2007', 'and', ' ...</td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>8      </td> <td>1      </td> <td>[0. 1. 0. 0. 0.]</td> <td>[0.87726814 0.04333929 0.01635703 0.04273042 0.02030507]</td> <td>['after', 'two', 'years', 'away', ',', 'byrd', 'reunited ...</td> <td>['after', 'two', 'years', ',', 'byrd', 'reunited', 'with ...</td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>8      </td> <td>2      </td> <td>[0. 0. 0. 1. 0.]</td> <td>[0.8814465  0.04183965 0.01579716 0.04140306 0.01951366]</td> <td>['after', 'two', 'years', 'away', ',', 'byrd', 'reunited ...</td> <td>['after', 'two', 'years', ',', 'byrd', 'reunited', 'with ...</td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>8      </td> <td>60     </td> <td>[0. 0. 0. 0. 1.]</td> <td>[0.8814463  0.04183973 0.01579723 0.04140311 0.01951369]</td> <td>['after', 'two', 'years', 'away', ',', 'byrd', 'reunited ...</td> <td>['after', 'two', 'years', ',', 'byrd', 'reunited', 'with ...</td>\n",
       "        </tr>\n",
       "    </tbody>\n",
       "</table>\n",
       "<p>... (9881 rows omitted)</p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "err_analy = Table().with_columns(\"id_sent\", id_sent, \"id_word\", id_word, \"ground truth\", truth, \"predictions\", errs, \"sen1\", sen1 ,\"sen2\", sen2 )\n",
    "err_analy.show(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "    <thead>\n",
       "        <tr>\n",
       "            <th>id</th> <th>sent1</th> <th>sent2</th> <th>label</th>\n",
       "        </tr>\n",
       "    </thead>\n",
       "    <tbody>\n",
       "        <tr>\n",
       "            <td>1   </td> <td>b'How can you treat ocd ? Is there any helpful suggestio ...</td> <td>b'How can you treat OCD ? Is there any helpful suggestio ...</td> <td>1    </td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>7   </td> <td>b'What are some beautiful lines to comment on Beautiful  ...</td> <td>b'What are some Beautiful lines to comment on beautiful  ...</td> <td>1    </td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>8   </td> <td>b'Can small dogs breed with large dogs ?'                   </td> <td>b'Can small dogs breed with large dogs ?'                   </td> <td>1    </td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>13  </td> <td>b'Are Indian IT services companies like Tech Mahindra/ W ...</td> <td>b'Are Indian IT services companies like Wipro/TCS/Tech . ...</td> <td>1    </td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>14  </td> <td>b'Is nuclear energy renewable energy ?'                     </td> <td>b\"`` Is nuclear energy `` renewable energy `` ? ''\"         </td> <td>1    </td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>15  </td> <td>b'What should I solve and what or how should I revise fo ...</td> <td>b'What should I revise and what or how should I solve fo ...</td> <td>0    </td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>20  </td> <td>b'How is the relative ratio of brain waves ( alpha/beta/ ...</td> <td>b'How is the ratio of relative brain waves ( alpha/beta/ ...</td> <td>0    </td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>21  </td> <td>b'How can I make an android app using Python 3 ? Is ther ...</td> <td>b'How can I develop an android app using Python 3 ? Is t ...</td> <td>1    </td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>24  </td> <td>b'Headbreak : What can I do to prevent heart break night ...</td> <td>b'Headbreak : What can I do to prevent heart break night ...</td> <td>1    </td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>25  </td> <td>b'When is latent heat positive ?'                           </td> <td>b'When is latent heat positive ?'                           </td> <td>1    </td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>28  </td> <td>b\"What does the phrase 'Do you feel me ' mean ?\"            </td> <td>b\"What does the phrase `Do you feel me'mean ?\"              </td> <td>1    </td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>29  </td> <td>b'What are some unexpected things first-time visitors to ...</td> <td>b'What are some unexpected things , first-time visitors  ...</td> <td>1    </td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>35  </td> <td>b'How do I prepare for cognizant campus placement online ...</td> <td>b\"How do I prepare for Cognizant 's online campus placem ...</td> <td>1    </td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>40  </td> <td>b'Which city is better to choose in India to settle from ...</td> <td>b'Which city is better to settle in India to choose from ...</td> <td>1    </td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>53  </td> <td>b'How does quality of life in Vancouver compare to that  ...</td> <td>b'How does quality of life in Vancouver compare to that  ...</td> <td>1    </td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>59  </td> <td>b'How do you write an algorithm to find the sum of the f ...</td> <td>b'How do you write an algorithm to find the sum of the f ...</td> <td>1    </td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>61  </td> <td>b'I have only a very nice startup idea but I m not good  ...</td> <td>b'I have only a very good startup idea but I m not nice  ...</td> <td>1    </td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>67  </td> <td>b'What should a mechanical engineer do besides doing col ...</td> <td>b'What should a mechanical engineer do besides studying  ...</td> <td>1    </td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>73  </td> <td>b'How can entropy of two reversible process be same , be ...</td> <td>b'How can entropy of two same process be reversible , be ...</td> <td>0    </td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>83  </td> <td>b'During muscle gain program , Heavy weight with more re ...</td> <td>b'During muscle gain program , Heavy weight with less re ...</td> <td>0    </td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>85  </td> <td>b'When does the second batch of training for freshers in ...</td> <td>b'When does the first batch of training for freshers in  ...</td> <td>0    </td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>105 </td> <td>b'It is possible to apply at the Diversity Immigrant Vis ...</td> <td>b'It is possible to get at the Diversity Immigrant Visa  ...</td> <td>0    </td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>106 </td> <td>b\"Why do n't audiobooks create Text-to-speech technology ...</td> <td>b\"Why do n't audiobooks use Text-to-speech technology to ...</td> <td>0    </td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>108 </td> <td>b'I have polycythemia . Can I qualify for government job ...</td> <td>b'I have polycythemia can I apply for government job can ...</td> <td>0    </td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>109 </td> <td>b'If I see a video on YouTube , does the owner of the vi ...</td> <td>b'If I watch a video on YouTube , does the owner of the  ...</td> <td>1    </td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>111 </td> <td>b'Who should they cast if they remake Friends in Bollywo ...</td> <td>b'Who should they cast if they remake Friends in Bollywo ...</td> <td>1    </td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>113 </td> <td>b\"I want to learn piano but I do n't have the basic know ...</td> <td>b\"I want to learn piano but I do n't have the basic know ...</td> <td>1    </td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>120 </td> <td>b'Can you shoot a Dangerous Human easily death with only ...</td> <td>b'Can you shoot a Dangerous Human easily death with only ...</td> <td>1    </td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>123 </td> <td>b'Why Are angels shown as having wings ? are they ever s ...</td> <td>b'Why are angels shown as having wings ? Are they ever s ...</td> <td>1    </td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>134 </td> <td>b\"I have to apply fresh passport for my wife and childre ...</td> <td>b\"I have to apply fresh passport for my wife and childre ...</td> <td>1    </td>\n",
       "        </tr>\n",
       "    </tbody>\n",
       "</table>\n",
       "<p>... (2563 rows omitted)</p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "error_analysis = Table()\n",
    "\n",
    "\n",
    "error_analysis = error_analysis.with_column(\"id\", no, \"sent1\", err1, \"sent2\", err2, \"label\", wrongCat)\n",
    "\n",
    "\n",
    "#false POSITIVE: swap two words\n",
    "#false negative: swap two specific nouns\n",
    "error_analysis.show(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "    <thead>\n",
       "        <tr>\n",
       "            <th>id</th> <th>sent1</th> <th>sent2</th> <th>label</th>\n",
       "        </tr>\n",
       "    </thead>\n",
       "    <tbody>\n",
       "        <tr>\n",
       "            <td>1   </td> <td>b'How can you treat ocd ? Is there any helpful suggestio ...</td> <td>b'How can you treat OCD ? Is there any helpful suggestio ...</td> <td>1    </td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>7   </td> <td>b'What are some beautiful lines to comment on Beautiful  ...</td> <td>b'What are some Beautiful lines to comment on beautiful  ...</td> <td>1    </td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>8   </td> <td>b'Can small dogs breed with large dogs ?'                   </td> <td>b'Can small dogs breed with large dogs ?'                   </td> <td>1    </td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>13  </td> <td>b'Are Indian IT services companies like Tech Mahindra/ W ...</td> <td>b'Are Indian IT services companies like Wipro/TCS/Tech . ...</td> <td>1    </td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>14  </td> <td>b'Is nuclear energy renewable energy ?'                     </td> <td>b\"`` Is nuclear energy `` renewable energy `` ? ''\"         </td> <td>1    </td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>21  </td> <td>b'How can I make an android app using Python 3 ? Is ther ...</td> <td>b'How can I develop an android app using Python 3 ? Is t ...</td> <td>1    </td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>24  </td> <td>b'Headbreak : What can I do to prevent heart break night ...</td> <td>b'Headbreak : What can I do to prevent heart break night ...</td> <td>1    </td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>25  </td> <td>b'When is latent heat positive ?'                           </td> <td>b'When is latent heat positive ?'                           </td> <td>1    </td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>28  </td> <td>b\"What does the phrase 'Do you feel me ' mean ?\"            </td> <td>b\"What does the phrase `Do you feel me'mean ?\"              </td> <td>1    </td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>29  </td> <td>b'What are some unexpected things first-time visitors to ...</td> <td>b'What are some unexpected things , first-time visitors  ...</td> <td>1    </td>\n",
       "        </tr>\n",
       "    </tbody>\n",
       "</table>\n",
       "<p>... (1967 rows omitted)</p>"
      ],
      "text/plain": [
       "id   | sent1                                                        | sent2                                                        | label\n",
       "1    | b'How can you treat ocd ? Is there any helpful suggestio ... | b'How can you treat OCD ? Is there any helpful suggestio ... | 1\n",
       "7    | b'What are some beautiful lines to comment on Beautiful  ... | b'What are some Beautiful lines to comment on beautiful  ... | 1\n",
       "8    | b'Can small dogs breed with large dogs ?'                    | b'Can small dogs breed with large dogs ?'                    | 1\n",
       "13   | b'Are Indian IT services companies like Tech Mahindra/ W ... | b'Are Indian IT services companies like Wipro/TCS/Tech . ... | 1\n",
       "14   | b'Is nuclear energy renewable energy ?'                      | b\"`` Is nuclear energy `` renewable energy `` ? ''\"          | 1\n",
       "21   | b'How can I make an android app using Python 3 ? Is ther ... | b'How can I develop an android app using Python 3 ? Is t ... | 1\n",
       "24   | b'Headbreak : What can I do to prevent heart break night ... | b'Headbreak : What can I do to prevent heart break night ... | 1\n",
       "25   | b'When is latent heat positive ?'                            | b'When is latent heat positive ?'                            | 1\n",
       "28   | b\"What does the phrase 'Do you feel me ' mean ?\"             | b\"What does the phrase `Do you feel me'mean ?\"               | 1\n",
       "29   | b'What are some unexpected things first-time visitors to ... | b'What are some unexpected things , first-time visitors  ... | 1\n",
       "... (1967 rows omitted)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
