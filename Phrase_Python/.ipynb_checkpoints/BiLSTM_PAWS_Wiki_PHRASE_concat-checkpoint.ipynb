{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib\n",
    "import argparse\n",
    "import random\n",
    "import numpy as np\n",
    "import csv\n",
    "import pandas as pd\n",
    "from datascience import *\n",
    "import tensorflow as tf\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from keras.models import Model\n",
    "from keras.models import Sequential, load_model\n",
    "\n",
    "from keras.activations import softmax\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import to_categorical\n",
    "from keras.layers import *\n",
    "from keras.optimizers import Adam\n",
    "from keras import Input\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.utils import plot_model\n",
    "import keras\n",
    "#from keras.backend.tensorflow_backend import set_session\n",
    "#config = tf.ConfigProto()\n",
    "# config.gpu_options.allocator_type = 'BFC' #A \"Best-fit with coalescing\" algorithm, simplified from a version of dlmalloc.\n",
    "# config.gpu_options.per_process_gpu_memory_fraction = 0.7\n",
    "#config.gpu_options.allow_growth =True\n",
    "\n",
    "#set_session(tf.Session(config=config)) \n",
    "import re\n",
    "os.environ['PYTHONHASHSEED'] = '0'\n",
    "random.seed(1)\n",
    "np.random.seed(1)\n",
    "tf.set_random_seed(1)\n",
    "from keras.callbacks import *\n",
    "from keras.models import model_from_json\n",
    "import json\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sanitize(string):\n",
    "    words = string.split(' ')\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mon Jul 22 06:38:39 2019       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 418.56       Driver Version: 418.56       CUDA Version: 10.1     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  GeForce GTX 1080    Off  | 00000000:02:00.0 Off |                  N/A |\n",
      "|  0%   47C    P8     8W / 200W |    360MiB /  8118MiB |      0%      Default |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                       GPU Memory |\n",
      "|  GPU       PID   Type   Process name                             Usage      |\n",
      "|=============================================================================|\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   TRAIN_PAWS_Wiki_id   PPDB_id  \\\n",
      "0                21.0  160456.0   \n",
      "1                21.0  243017.0   \n",
      "2                33.0  395059.0   \n",
      "3                33.0  395061.0   \n",
      "4                33.0  395071.0   \n",
      "\n",
      "                                               sent1  \\\n",
      "0  For their performances in the game , quarterba...   \n",
      "1  For their performances in the game , quarterba...   \n",
      "2  The leaves are generally 1.5-4 mm long and 0.2...   \n",
      "3  The leaves are generally 1.5-4 mm long and 0.2...   \n",
      "4  The leaves are generally 1.5-4 mm long and 0.2...   \n",
      "\n",
      "                                               sent2      paraphrase1  \\\n",
      "0  Quarterback P. J. Williams and Defensive Back ...   most valuable    \n",
      "1  Quarterback P. J. Williams and Defensive Back ...         's most    \n",
      "2  The leaves are usually 1.5-4 mm long and 0.2-0...   are generally    \n",
      "3  The leaves are usually 1.5-4 mm long and 0.2-0...   are generally    \n",
      "4  The leaves are usually 1.5-4 mm long and 0.2-0...   are generally    \n",
      "\n",
      "     paraphrase2  label                                            labels1  \\\n",
      "0      valuable       0  O O O O O O O O O O O O O O O O O O O O O B E O O   \n",
      "1          most       0  O O O O O O O O O O O O O O O O O O O O B E O O O   \n",
      "2   are usually       1                            O O B E O O O O O O O O   \n",
      "3   are usually       1                            O O B E O O O O O O O O   \n",
      "4   are usually       1                            O O B E O O O O O O O O   \n",
      "\n",
      "                                             labels2  \n",
      "0  O O O O O O O O O O O O O S O O O O O O O O O O O  \n",
      "1  O O O O O O O O O O O O S O O O O O O O O O O O O  \n",
      "2                            O O B E O O O O O O O O  \n",
      "3                            O O B E O O O O O O O O  \n",
      "4                            O O B E O O O O O O O O  \n"
     ]
    }
   ],
   "source": [
    "#read csv\n",
    "\n",
    "train = pd.read_csv(\"../dataset/Phrase_Level/PAWS_Wiki/LABELED_PAWS_Wiki_TRAIN.tsv\", sep='\\t')\n",
    "\n",
    "dev = pd.read_csv(\"../dataset/Phrase_Level/PAWS_Wiki/LABELED_PAWS_Wiki_DEV.tsv\", sep='\\t')\n",
    "\n",
    "test = pd.read_csv(\"../dataset/Phrase_Level/PAWS_Wiki/LABELED_PAWS_Wiki_TEST.tsv\", sep='\\t')\n",
    "\n",
    "\n",
    "print(train[:5])\n",
    "\n",
    "train['title1_tokenized'] = \\\n",
    "    train.loc[:, 'sent1'] \\\n",
    "         .apply(sanitize)\n",
    "train['title2_tokenized'] = \\\n",
    "    train.loc[:, 'sent2'] \\\n",
    "         .apply(sanitize)\n",
    "\n",
    "\n",
    "dev['title1_tokenized'] = \\\n",
    "    dev.loc[:, 'sent1'] \\\n",
    "         .apply(sanitize)\n",
    "dev['title2_tokenized'] = \\\n",
    "    dev.loc[:, 'sent2'] \\\n",
    "         .apply(sanitize)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    O O O O O O O O O O O O O O O O O O O O O B E O O\n",
       "1    O O O O O O O O O O O O O O O O O O O O B E O O O\n",
       "2                              O O B E O O O O O O O O\n",
       "3                              O O B E O O O O O O O O\n",
       "Name: labels1, dtype: object"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.labels1[:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TRAIN_PAWS_Wiki_id</th>\n",
       "      <th>PPDB_id</th>\n",
       "      <th>sent1</th>\n",
       "      <th>sent2</th>\n",
       "      <th>paraphrase1</th>\n",
       "      <th>paraphrase2</th>\n",
       "      <th>label</th>\n",
       "      <th>labels1</th>\n",
       "      <th>labels2</th>\n",
       "      <th>title1_tokenized</th>\n",
       "      <th>title2_tokenized</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>21.0</td>\n",
       "      <td>160456.0</td>\n",
       "      <td>For their performances in the game , quarterba...</td>\n",
       "      <td>Quarterback P. J. Williams and Defensive Back ...</td>\n",
       "      <td>most valuable</td>\n",
       "      <td>valuable</td>\n",
       "      <td>0</td>\n",
       "      <td>O O O O O O O O O O O O O O O O O O O O O B E O O</td>\n",
       "      <td>O O O O O O O O O O O O O S O O O O O O O O O O O</td>\n",
       "      <td>[For, their, performances, in, the, game, ,, q...</td>\n",
       "      <td>[Quarterback, P., J., Williams, and, Defensive...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>21.0</td>\n",
       "      <td>243017.0</td>\n",
       "      <td>For their performances in the game , quarterba...</td>\n",
       "      <td>Quarterback P. J. Williams and Defensive Back ...</td>\n",
       "      <td>'s most</td>\n",
       "      <td>most</td>\n",
       "      <td>0</td>\n",
       "      <td>O O O O O O O O O O O O O O O O O O O O B E O O O</td>\n",
       "      <td>O O O O O O O O O O O O S O O O O O O O O O O O O</td>\n",
       "      <td>[For, their, performances, in, the, game, ,, q...</td>\n",
       "      <td>[Quarterback, P., J., Williams, and, Defensive...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>33.0</td>\n",
       "      <td>395059.0</td>\n",
       "      <td>The leaves are generally 1.5-4 mm long and 0.2...</td>\n",
       "      <td>The leaves are usually 1.5-4 mm long and 0.2-0...</td>\n",
       "      <td>are generally</td>\n",
       "      <td>are usually</td>\n",
       "      <td>1</td>\n",
       "      <td>O O B E O O O O O O O O</td>\n",
       "      <td>O O B E O O O O O O O O</td>\n",
       "      <td>[The, leaves, are, generally, 1.5-4, mm, long,...</td>\n",
       "      <td>[The, leaves, are, usually, 1.5-4, mm, long, a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>33.0</td>\n",
       "      <td>395061.0</td>\n",
       "      <td>The leaves are generally 1.5-4 mm long and 0.2...</td>\n",
       "      <td>The leaves are usually 1.5-4 mm long and 0.2-0...</td>\n",
       "      <td>are generally</td>\n",
       "      <td>are usually</td>\n",
       "      <td>1</td>\n",
       "      <td>O O B E O O O O O O O O</td>\n",
       "      <td>O O B E O O O O O O O O</td>\n",
       "      <td>[The, leaves, are, generally, 1.5-4, mm, long,...</td>\n",
       "      <td>[The, leaves, are, usually, 1.5-4, mm, long, a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>33.0</td>\n",
       "      <td>395071.0</td>\n",
       "      <td>The leaves are generally 1.5-4 mm long and 0.2...</td>\n",
       "      <td>The leaves are usually 1.5-4 mm long and 0.2-0...</td>\n",
       "      <td>are generally</td>\n",
       "      <td>are usually</td>\n",
       "      <td>1</td>\n",
       "      <td>O O B E O O O O O O O O</td>\n",
       "      <td>O O B E O O O O O O O O</td>\n",
       "      <td>[The, leaves, are, generally, 1.5-4, mm, long,...</td>\n",
       "      <td>[The, leaves, are, usually, 1.5-4, mm, long, a...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   TRAIN_PAWS_Wiki_id   PPDB_id  \\\n",
       "0                21.0  160456.0   \n",
       "1                21.0  243017.0   \n",
       "2                33.0  395059.0   \n",
       "3                33.0  395061.0   \n",
       "4                33.0  395071.0   \n",
       "\n",
       "                                               sent1  \\\n",
       "0  For their performances in the game , quarterba...   \n",
       "1  For their performances in the game , quarterba...   \n",
       "2  The leaves are generally 1.5-4 mm long and 0.2...   \n",
       "3  The leaves are generally 1.5-4 mm long and 0.2...   \n",
       "4  The leaves are generally 1.5-4 mm long and 0.2...   \n",
       "\n",
       "                                               sent2      paraphrase1  \\\n",
       "0  Quarterback P. J. Williams and Defensive Back ...   most valuable    \n",
       "1  Quarterback P. J. Williams and Defensive Back ...         's most    \n",
       "2  The leaves are usually 1.5-4 mm long and 0.2-0...   are generally    \n",
       "3  The leaves are usually 1.5-4 mm long and 0.2-0...   are generally    \n",
       "4  The leaves are usually 1.5-4 mm long and 0.2-0...   are generally    \n",
       "\n",
       "     paraphrase2  label                                            labels1  \\\n",
       "0      valuable       0  O O O O O O O O O O O O O O O O O O O O O B E O O   \n",
       "1          most       0  O O O O O O O O O O O O O O O O O O O O B E O O O   \n",
       "2   are usually       1                            O O B E O O O O O O O O   \n",
       "3   are usually       1                            O O B E O O O O O O O O   \n",
       "4   are usually       1                            O O B E O O O O O O O O   \n",
       "\n",
       "                                             labels2  \\\n",
       "0  O O O O O O O O O O O O O S O O O O O O O O O O O   \n",
       "1  O O O O O O O O O O O O S O O O O O O O O O O O O   \n",
       "2                            O O B E O O O O O O O O   \n",
       "3                            O O B E O O O O O O O O   \n",
       "4                            O O B E O O O O O O O O   \n",
       "\n",
       "                                    title1_tokenized  \\\n",
       "0  [For, their, performances, in, the, game, ,, q...   \n",
       "1  [For, their, performances, in, the, game, ,, q...   \n",
       "2  [The, leaves, are, generally, 1.5-4, mm, long,...   \n",
       "3  [The, leaves, are, generally, 1.5-4, mm, long,...   \n",
       "4  [The, leaves, are, generally, 1.5-4, mm, long,...   \n",
       "\n",
       "                                    title2_tokenized  \n",
       "0  [Quarterback, P., J., Williams, and, Defensive...  \n",
       "1  [Quarterback, P., J., Williams, and, Defensive...  \n",
       "2  [The, leaves, are, usually, 1.5-4, mm, long, a...  \n",
       "3  [The, leaves, are, usually, 1.5-4, mm, long, a...  \n",
       "4  [The, leaves, are, usually, 1.5-4, mm, long, a...  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GloVe data loaded\n"
     ]
    }
   ],
   "source": [
    "embeddings_index = {}\n",
    "f = open('../glove.840B.300d.txt')\n",
    "for line in f:\n",
    "    values = line.split(' ')\n",
    "    word = values[0] ## The first entry is the word\n",
    "    coefs = np.asarray(values[1:], dtype='float32') ## These are the vecotrs representing the embedding for the word\n",
    "    embeddings_index[word] = coefs\n",
    "\n",
    "f.close()\n",
    "\n",
    "print('GloVe data loaded')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tokenize sentences\n",
    "\n",
    "def normalize_and_split(sentence):\n",
    "    s=sentence.split()\n",
    "    l=[x.lower() for x in s]\n",
    "    \n",
    "    return l\n",
    "\n",
    "def normalize_and_split2(sentence):\n",
    "    s=sentence.split()\n",
    "    l=[x.lower() for x in s]\n",
    "    first_word=(l[0])[2:]\n",
    "    l[0]=first_word\n",
    "    return l\n",
    "\n",
    "#tokenize labels\n",
    "\n",
    "def t_label(column, max_length):\n",
    "    \n",
    "    ret=[]\n",
    "  \n",
    "    for y in column:   #each list\n",
    "        \n",
    "        s = y.split() #split into individual alphabets\n",
    "        temp=[]\n",
    "        \n",
    "        for x in s:\n",
    "            temp.append(label_to_index[x])\n",
    "             \n",
    "    \n",
    "        for _ in range(max_length-len(s)): \n",
    "            temp.append(0)\n",
    "        \n",
    "        temp=np.asarray(temp)\n",
    "        \n",
    "        y_train = keras \\\n",
    "            .utils \\\n",
    "            .to_categorical(temp, 5)    #do one hot on each word\n",
    "        \n",
    "        \n",
    "        \n",
    "        ret.append(y_train)\n",
    "           \n",
    "    return list(ret)\n",
    "\n",
    "label_to_index = {\n",
    "    'O': 0, \n",
    "    'B': 1, \n",
    "    'I': 2,\n",
    "    'E': 3,\n",
    "    'S': 4\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i', 'like', 'to', 'eateat', 'pie']"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "normalize_and_split(\"I like to eateat piE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i', 'am', 'liking', 'itt']"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "normalize_and_split(\"b'I Am LiKINg iTt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_Tb=Table.read_table(\"../dataset/Phrase_Level/PAWS_Wiki/LABELED_PAWS_Wiki_TRAIN.tsv\")\n",
    "dev_Tb=Table.read_table(\"../dataset/Phrase_Level/PAWS_Wiki/LABELED_PAWS_Wiki_DEV.tsv\")\n",
    "test_Tb=Table.read_table(\"../dataset/Phrase_Level/PAWS_Wiki/LABELED_PAWS_Wiki_TEST.tsv\")\n",
    "\n",
    "train_Tb_Q=Table.read_table(\"../dataset/Phrase_Level/PAWS_QQP/LABELED_PAWS_QQP_TRAIN.tsv\")\n",
    "dev_Tb_Q=Table.read_table(\"../dataset/Phrase_Level/PAWS_QQP/LABELED_PAWS_QQP_DEV.tsv\")\n",
    "\n",
    "dev_Tb_S=Table.read_table(\"../dataset/Phrase_Level/SNLI/SNLI_PPDB_DEV.tsv\")\n",
    "test_Tb_S=Table.read_table(\"../dataset/Phrase_Level/SNLI/PPDB_SNLI_TEST.tsv\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def keep_one_para_pair(Tb):   #takes in origin table, keeps only one pair of paraphrase per sentence, and then return the modified table\n",
    "    \n",
    "    to_drop=[]\n",
    "    pair_id=Tb.column(0)\n",
    "    for x in range(Tb.num_rows-1):\n",
    "        if pair_id[x]==pair_id[x+1]:\n",
    "            to_drop.append(x)\n",
    "    new_Tb=Tb.exclude(to_drop)\n",
    "    return new_Tb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_PAWS_Wiki=keep_one_para_pair(train_Tb)   #2739\n",
    "\n",
    "dev_PAWS_Wiki=keep_one_para_pair(dev_Tb)     #924\n",
    "\n",
    "test_PAWS_Wiki=keep_one_para_pair(test_Tb)   #1191\n",
    "\n",
    "train_PAWS_QQP=keep_one_para_pair(train_Tb_Q)  #791\n",
    "\n",
    "dev_PAWS_QQP=keep_one_para_pair(dev_Tb_Q)     #63\n",
    "\n",
    "dev_SNLI=keep_one_para_pair(dev_Tb_S)       #308   \n",
    "\n",
    "test_SNLI=keep_one_para_pair(test_Tb_S)       #229\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "    <thead>\n",
       "        <tr>\n",
       "            <th>SNLI_PairID</th> <th>PPDB_id</th> <th>sent1</th> <th>sent2</th> <th>paraphrase1</th> <th>paraphrase2</th> <th>gold_label</th> <th>labels1</th> <th>labels2</th>\n",
       "        </tr>\n",
       "    </thead>\n",
       "    <tbody>\n",
       "        <tr>\n",
       "            <td>6526219567.jpg#4r1e</td> <td>1.10807e+06</td> <td>A statue at a museum that no seems to be looking at.        </td> <td>There is a statue that not many people seem to be intere ...</td> <td> seems to       </td> <td> seem to   </td> <td>entailment</td> <td>O O O O O O O B E O O O        </td> <td>O O O O O O O O B E O O O</td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>4460943467.jpg#0r1e</td> <td>302929     </td> <td>3 young man in hoods standing in the middle of a quiet s ...</td> <td>Three hood wearing people stand in a street.                </td> <td> standing in    </td> <td> stand in  </td> <td>entailment</td> <td>O O O O O B E O O O O O O O O O</td> <td>O O O O B E O O          </td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>3036382555.jpg#0r1n</td> <td>1.00273e+06</td> <td>The two young girls are dressed as fairies, and are play ...</td> <td>The two girls play in the Autumn.                           </td> <td> playing in     </td> <td> play in   </td> <td>neutral   </td> <td>O O O O O O O O O O B E O O O  </td> <td>O O O B E O O            </td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>4479330885.jpg#3r1e</td> <td>958598     </td> <td>Several younger people sitting in front of a statue.        </td> <td>several young people sitting outside                        </td> <td> younger people </td> <td> young     </td> <td>neutral   </td> <td>O B E O O O O O O              </td> <td>O S O O O                </td>\n",
       "        </tr>\n",
       "    </tbody>\n",
       "</table>\n",
       "<p>... (225 rows omitted)</p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "test_SNLI.show(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def help_norm(Tb):\n",
    "     \n",
    "    \n",
    "    if Tb is train_PAWS_QQP or Tb is dev_PAWS_QQP:\n",
    "        a=Tb.apply(normalize_and_split2, \"sent1\")\n",
    "        b=Tb.apply(normalize_and_split2, \"sent2\")\n",
    "        \n",
    "    else:\n",
    "        a=Tb.apply(normalize_and_split, \"sent1\")\n",
    "        b=Tb.apply(normalize_and_split, \"sent2\")    \n",
    "        \n",
    "       \n",
    "    \n",
    "    \n",
    "    c=t_label(Tb.column(\"labels1\"), 58)\n",
    "    d=t_label(Tb.column(\"labels2\"), 58)\n",
    "    \n",
    "   \n",
    "\n",
    "    \n",
    "    return list(a),list(b),list(c),list(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 379,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2, 4]"
      ]
     },
     "execution_count": 379,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tokenize sentences, and one-hot labels\n",
    "\n",
    "TRPWS1, TRPWS2, TRPWL1, TRPWL2=help_norm(train_PAWS_Wiki)   #training paws wiki sentence 1, btw, and the rest are named in a similar fashion\n",
    "\n",
    "DPWS1, DPWS2,DPWL1, DPWL2 = help_norm(dev_PAWS_Wiki)\n",
    "\n",
    "TEPWS1, TEPWS2, TEPWL1, TEPWL2=help_norm(test_PAWS_Wiki)\n",
    "\n",
    "TRPQS1, TRPQS2, TRPQL1, TRPQL2=help_norm(train_PAWS_QQP)\n",
    "\n",
    "DPQS1, DPQS2, DPQL1, DPQL2=help_norm(dev_PAWS_QQP)\n",
    "\n",
    "DSS1, DSS2, DSL1, DSL2 = help_norm(dev_SNLI)\n",
    "\n",
    "TESS1, TESS2, TESL1, TESL2=help_norm(test_SNLI)\n",
    "\n",
    "#also prints out maximum sentence length for each dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'create_Tb' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-03419f1bd7ef>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mTRPW\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcreate_Tb\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_PAWS_Wiki\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTRPWS1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTRPWS2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTRPWL1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTRPWL2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mDPW\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcreate_Tb\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdev_PAWS_Wiki\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDPWS1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDPWS2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDPWL1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDPWL2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mTEPW\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcreate_Tb\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_PAWS_Wiki\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTEPWS1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTEPWS2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTEPWL1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTEPWL2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mTRPQ\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcreate_Tb\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_PAWS_QQP\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTRPQS1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTRPQS2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTRPQL1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTRPQL2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mDPQ\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcreate_Tb\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdev_PAWS_QQP\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDPQS1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDPQS2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDPQL1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDPQL2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'create_Tb' is not defined"
     ]
    }
   ],
   "source": [
    "TRPW=create_Tb(train_PAWS_Wiki, TRPWS1, TRPWS2, TRPWL1, TRPWL2)\n",
    "DPW=create_Tb(dev_PAWS_Wiki, DPWS1, DPWS2, DPWL1, DPWL2)\n",
    "TEPW=create_Tb(test_PAWS_Wiki, TEPWS1, TEPWS2, TEPWL1, TEPWL2)\n",
    "TRPQ=create_Tb(train_PAWS_QQP, TRPQS1, TRPQS2, TRPQL1, TRPQL2)\n",
    "DPQ=create_Tb(dev_PAWS_QQP, DPQS1, DPQS2, DPQL1, DPQL2)\n",
    "DS=create_Tb(dev_SNLI, DSS1, DSS2, DSL1, DSL2)\n",
    "TES=create_Tb(test_SNLI, TESS1, TESS2, TESL1, TESL2)\n",
    "#max seq length say 58"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_Tb(Tb, s1, s2, l1, l2):         #visualize tokenization with a table\n",
    "    zz=Tb.with_columns(\"sent1_token\", s1, \"sent2_token\", s2, \"label1_token\", l1, \"label2_token\", l2)\n",
    "    return zz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "    <thead>\n",
       "        <tr>\n",
       "            <th>TRAIN_PAWS_Wiki_id</th> <th>PPDB_id</th> <th>sent1</th> <th>sent2</th> <th>paraphrase1</th> <th>paraphrase2</th> <th>label</th> <th>labels1</th> <th>labels2</th> <th>sent1_token</th> <th>sent2_token</th> <th>label1_token</th> <th>label2_token</th>\n",
       "        </tr>\n",
       "    </thead>\n",
       "    <tbody>\n",
       "        <tr>\n",
       "            <td>21                </td> <td>243017 </td> <td>For their performances in the game , quarterback Jameis  ...</td> <td>Quarterback P. J. Williams and Defensive Back Jameis Win ...</td> <td> 's most       </td> <td> most            </td> <td>0    </td> <td>O O O O O O O O O O O O O O O O O O O O B E O O O</td> <td>O O O O O O O O O O O O S O O O O O O O O O O O O</td> <td>['for', 'their', 'performances', 'in', 'the', 'game', ', ...</td> <td>['quarterback', 'p.', 'j.', 'williams', 'and', 'defensiv ...</td> <td>[[1. 0. 0. 0. 0.]\n",
       " [1. 0. 0. 0. 0.]\n",
       " [1. 0. 0. 0. 0.]\n",
       " [ ...</td> <td>[[1. 0. 0. 0. 0.]\n",
       " [1. 0. 0. 0. 0.]\n",
       " [1. 0. 0. 0. 0.]\n",
       " [ ...</td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>33                </td> <td>395099 </td> <td>The leaves are generally 1.5-4 mm long and 0.2-0.7 mm wide .</td> <td>The leaves are usually 1.5-4 mm long and 0.2-0.7 mm wide .  </td> <td> are generally </td> <td> are usually     </td> <td>1    </td> <td>O O B E O O O O O O O O                          </td> <td>O O B E O O O O O O O O                          </td> <td>['the', 'leaves', 'are', 'generally', '1.5-4', 'mm', 'lo ...</td> <td>['the', 'leaves', 'are', 'usually', '1.5-4', 'mm', 'long ...</td> <td>[[1. 0. 0. 0. 0.]\n",
       " [1. 0. 0. 0. 0.]\n",
       " [0. 1. 0. 0. 0.]\n",
       " [ ...</td> <td>[[1. 0. 0. 0. 0.]\n",
       " [1. 0. 0. 0. 0.]\n",
       " [0. 1. 0. 0. 0.]\n",
       " [ ...</td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>63                </td> <td>100574 </td> <td>She has taken part in the second season of the most popu ...</td> <td>She has participated in the second season of the most co ...</td> <td> taken part in </td> <td> participated in </td> <td>0    </td> <td>O O B I E O O O O O O O O O O O O O O O O O O O O</td> <td>O O B E O O O O O O O O O O O O O O O O O O O    </td> <td>['she', 'has', 'taken', 'part', 'in', 'the', 'second', ' ...</td> <td>['she', 'has', 'participated', 'in', 'the', 'second', 's ...</td> <td>[[1. 0. 0. 0. 0.]\n",
       " [1. 0. 0. 0. 0.]\n",
       " [0. 1. 0. 0. 0.]\n",
       " [ ...</td> <td>[[1. 0. 0. 0. 0.]\n",
       " [1. 0. 0. 0. 0.]\n",
       " [0. 1. 0. 0. 0.]\n",
       " [ ...</td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>86                </td> <td>404940 </td> <td>Lemmings , by contrast , are conspicuously colored and b ...</td> <td>By contrast , the lemmings are strikingly colored and be ...</td> <td> even human    </td> <td> human           </td> <td>1    </td> <td>O O O O O O O O O O O O O O B E O O              </td> <td>O O O O O O O O O O O O O O O S O O              </td> <td>['lemmings', ',', 'by', 'contrast', ',', 'are', 'conspic ...</td> <td>['by', 'contrast', ',', 'the', 'lemmings', 'are', 'strik ...</td> <td>[[1. 0. 0. 0. 0.]\n",
       " [1. 0. 0. 0. 0.]\n",
       " [1. 0. 0. 0. 0.]\n",
       " [ ...</td> <td>[[1. 0. 0. 0. 0.]\n",
       " [1. 0. 0. 0. 0.]\n",
       " [1. 0. 0. 0. 0.]\n",
       " [ ...</td>\n",
       "        </tr>\n",
       "    </tbody>\n",
       "</table>\n",
       "<p>... (2735 rows omitted)</p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "TRPW.show(4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_corpus=TRPWS1+TRPWS2+DPWS1+DPWS2+TEPWS1+TEPWS2+TRPQS1+TRPQS2+DPQS1+DPQS2\n",
    "\n",
    "dev_corpus=DSS1 +DSS2+ TESS1+ TESS2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sen1=TRPWS1+DPWS1+TEPWS1+TRPQS1+DPQS1\n",
    "\n",
    "train_sen2=TRPWS2+DPWS2+TEPWS2+TRPQS2+DPQS2\n",
    "\n",
    "train_label1=np.array(TRPWL1+DPWL1+TEPWL1+TRPQL1+DPQL1)\n",
    "\n",
    "train_label2=np.array(TRPWL2+DPWL2+TEPWL2+TRPQL2+DPQL2)\n",
    "\n",
    "dev_sen1=DSS1+TESS1\n",
    "dev_sen2=DSS2+TESS2\n",
    "dev_label1=np.array(DSL1+TESL1)\n",
    "dev_label2=np.array(DSL2 + TESL2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['the',\n",
       " 'leaves',\n",
       " 'are',\n",
       " 'generally',\n",
       " '1.5-4',\n",
       " 'mm',\n",
       " 'long',\n",
       " 'and',\n",
       " '0.2-0.7',\n",
       " 'mm',\n",
       " 'wide',\n",
       " '.']"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11416\n",
      "12490\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "\n",
    "MAX_NUM_WORDS = 10000\n",
    "MAX_SEQUENCE_LENGTH = 58\n",
    "\n",
    "\n",
    "#create a dictionary\n",
    "\n",
    "tokenizer = keras \\\n",
    "    .preprocessing \\\n",
    "    .text \\\n",
    "    .Tokenizer(num_words=MAX_NUM_WORDS)\n",
    "\n",
    "#corpus.shape\n",
    "tokenizer.fit_on_texts(train_corpus)\n",
    "tokenizer.texts_to_sequences(train_corpus)\n",
    "\n",
    "x1_train = tokenizer \\\n",
    "    .texts_to_sequences(train_sen1)\n",
    "x2_train = tokenizer \\\n",
    "    .texts_to_sequences(train_sen2)\n",
    "\n",
    "print(tokenizer.document_count)\n",
    "\n",
    "#corpus.shape\n",
    "tokenizer.fit_on_texts(dev_corpus)\n",
    "tokenizer.texts_to_sequences(dev_corpus)\n",
    "x1_dev = tokenizer \\\n",
    "    .texts_to_sequences(dev_sen1)\n",
    "x2_dev = tokenizer \\\n",
    "    .texts_to_sequences(dev_sen2)\n",
    "\n",
    "print(tokenizer.document_count)\n",
    "\n",
    "\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "x1_train = keras \\\n",
    "    .preprocessing \\\n",
    "    .sequence \\\n",
    "    .pad_sequences(x1_train, \n",
    "                   maxlen=MAX_SEQUENCE_LENGTH)\n",
    "\n",
    "\n",
    "x2_train = keras \\\n",
    "    .preprocessing \\\n",
    "    .sequence \\\n",
    "    .pad_sequences(x2_train, \n",
    "                   maxlen=MAX_SEQUENCE_LENGTH)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "x1_dev = keras \\\n",
    "    .preprocessing \\\n",
    "    .sequence \\\n",
    "    .pad_sequences(x1_dev, \n",
    "                   maxlen=MAX_SEQUENCE_LENGTH)\n",
    "\n",
    "\n",
    "x2_dev = keras \\\n",
    "    .preprocessing \\\n",
    "    .sequence \\\n",
    "    .pad_sequences(x2_dev, \n",
    "                   maxlen=MAX_SEQUENCE_LENGTH)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "## More code adapted from the keras reference (https://github.com/keras-team/keras/blob/master/examples/pretrained_word_embeddings.py)\n",
    "# prepare embedding matrix \n",
    "from keras.layers import Embedding\n",
    "from keras.initializers import Constant\n",
    "\n",
    "## EMBEDDING_DIM =  ## seems to need to match the embeddings_index dimension\n",
    "EMBEDDING_DIM = 300\n",
    "num_words = min(MAX_NUM_WORDS*2, len(word_index)) + 1\n",
    "embedding_matrix = np.zeros((num_words, EMBEDDING_DIM))\n",
    "for word, i in word_index.items():\n",
    "    if i > MAX_NUM_WORDS*2:\n",
    "        continue\n",
    "    embedding_vector = embeddings_index.get(word) ## This references the loaded embeddings dictionary\n",
    "    if embedding_vector is not None:\n",
    "        # words not found in embedding index will be all-zeros.\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "\n",
    "# load pre-trained word embeddings into an Embedding layer\n",
    "# note that we set trainable = False so as to keep the embeddings fixed\n",
    "embedding_layer = Embedding(num_words,\n",
    "                            EMBEDDING_DIM,\n",
    "                            embeddings_initializer=Constant(embedding_matrix),\n",
    "                            input_length=MAX_SEQUENCE_LENGTH,\n",
    "                            trainable=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import argparse\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import random\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import json as json\n",
    "\n",
    "from keras import backend as Kbackend\n",
    "\n",
    "from keras.preprocessing.text import one_hot\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.layers import Dense, Flatten, Reshape\n",
    "from keras.layers import Dropout, Activation\n",
    "from keras.layers import Bidirectional, LSTM, GRU, TimeDistributed\n",
    "from keras.layers import Input, concatenate, add\n",
    "from keras.models import Model\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras import backend as K\n",
    "\n",
    "\n",
    "\n",
    "gpu_options = tf.GPUOptions(allow_growth=True)\n",
    "sess = tf.Session(config=tf.ConfigProto(gpu_options=gpu_options))\n",
    "tf.keras.backend.set_session(sess)\n",
    "\n",
    "from numpy import array\n",
    "from keras.preprocessing.text import one_hot\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Flatten\n",
    "from keras.layers.embeddings import Embedding\n",
    "\n",
    "from keras.engine.topology import Layer\n",
    "from keras import initializers, regularizers, constraints\n",
    "\n",
    "class AttentionWithContext(Layer):\n",
    "    \"\"\"\n",
    "    Attention operation, with a context/query vector, for temporal data.\n",
    "    Supports Masking.\n",
    "    Follows the work of Yang et al. [https://www.cs.cmu.edu/~diyiy/docs/naacl16.pdf]\n",
    "    \"Hierarchical Attention Networks for Document Classification\"\n",
    "    by using a context vector to assist the attention\n",
    "    # Input shape\n",
    "        3D tensor with shape: `(samples, steps, features)`.\n",
    "    # Output shape\n",
    "        2D tensor with shape: `(samples, features)`.\n",
    "\n",
    "    How to use:\n",
    "    Just put it on top of an RNN Layer (GRU/LSTM/SimpleRNN) with return_sequences=True.\n",
    "    The dimensions are inferred based on the output shape of the RNN.\n",
    "\n",
    "    Note: The layer has been tested with Keras 2.0.6\n",
    "\n",
    "    Example:\n",
    "        model.add(LSTM(64, return_sequences=True))\n",
    "        model.add(AttentionWithContext())\n",
    "        # next add a Dense layer (for classification/regression) or whatever...\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 W_regularizer=None, u_regularizer=None, b_regularizer=None,\n",
    "                 W_constraint=None, u_constraint=None, b_constraint=None,\n",
    "                 bias=True, **kwargs):\n",
    "\n",
    "        self.supports_masking = True\n",
    "        self.init = initializers.get('glorot_uniform')\n",
    "\n",
    "        self.W_regularizer = regularizers.get(W_regularizer)\n",
    "        self.u_regularizer = regularizers.get(u_regularizer)\n",
    "        self.b_regularizer = regularizers.get(b_regularizer)\n",
    "\n",
    "        self.W_constraint = constraints.get(W_constraint)\n",
    "        self.u_constraint = constraints.get(u_constraint)\n",
    "        self.b_constraint = constraints.get(b_constraint)\n",
    "\n",
    "        self.bias = bias\n",
    "        super(AttentionWithContext, self).__init__(**kwargs)\n",
    "\n",
    "    def dot_product(self, x, kernel):\n",
    "        \"\"\"\n",
    "        Wrapper for dot product operation, in order to be compatible with both\n",
    "        Theano and Tensorflow\n",
    "        Args:\n",
    "            x (): input\n",
    "            kernel (): weights\n",
    "        Returns:\n",
    "        \"\"\"\n",
    "        if K.backend() == 'tensorflow':\n",
    "            return K.squeeze(K.dot(x, K.expand_dims(kernel)), axis=-1)\n",
    "        else:\n",
    "            return K.dot(x, kernel)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        assert len(input_shape) == 3\n",
    "\n",
    "        self.W = self.add_weight((input_shape[-1], input_shape[-1],),\n",
    "                                 initializer=self.init,\n",
    "                                 name='{}_W'.format(self.name),\n",
    "                                 regularizer=self.W_regularizer,\n",
    "                                 constraint=self.W_constraint)\n",
    "        if self.bias:\n",
    "            self.b = self.add_weight((input_shape[-1],),\n",
    "                                     initializer='zero',\n",
    "                                     name='{}_b'.format(self.name),\n",
    "                                     regularizer=self.b_regularizer,\n",
    "                                     constraint=self.b_constraint)\n",
    "\n",
    "        self.u = self.add_weight((input_shape[-1],),\n",
    "                                 initializer=self.init,\n",
    "                                 name='{}_u'.format(self.name),\n",
    "                                 regularizer=self.u_regularizer,\n",
    "                                 constraint=self.u_constraint)\n",
    "\n",
    "        super(AttentionWithContext, self).build(input_shape)\n",
    "\n",
    "    def compute_mask(self, input, input_mask=None):\n",
    "        # do not pass the mask to the next layers\n",
    "        return None\n",
    "\n",
    "    def call(self, x, mask=None):\n",
    "        uit = self.dot_product(x, self.W)\n",
    "\n",
    "        if self.bias:\n",
    "            uit += self.b\n",
    "\n",
    "        uit = K.tanh(uit)\n",
    "        \n",
    "        #ait = K.dot(uit, self.u)\n",
    "        ait = self.dot_product(uit, self.u)\n",
    "\n",
    "        a = K.exp(ait)\n",
    "\n",
    "        # apply mask after the exp. will be re-normalized next\n",
    "        if mask is not None:\n",
    "            # Cast the mask to floatX to avoid float64 upcasting in theano\n",
    "            a *= K.cast(mask, K.floatx())\n",
    "\n",
    "        # in some cases especially in the early stages of training the sum may be almost zero\n",
    "        # and this results in NaN's. A workaround is to add a very small positive number ε to the sum.\n",
    "        # a /= K.cast(K.sum(a, axis=1, keepdims=True), K.floatx())\n",
    "        a /= K.cast(K.sum(a, axis=1, keepdims=True) + K.epsilon(), K.floatx())\n",
    "\n",
    "        a = K.expand_dims(a)\n",
    "        weighted_input = x * a\n",
    "        return K.sum(weighted_input, axis=1)\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return input_shape[0], input_shape[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HAttLayer(Layer):\n",
    "    def __init__(self, attention_dim):\n",
    "        self.init = initializers.get('normal')\n",
    "        self.supports_masking = True\n",
    "        self.attention_dim = attention_dim\n",
    "        super(HAttLayer, self).__init__()\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        assert len(input_shape) == 3\n",
    "        self.W = K.variable(self.init((input_shape[-1], self.attention_dim)))\n",
    "        self.b = K.variable(self.init((self.attention_dim, )))\n",
    "        self.u = K.variable(self.init((self.attention_dim, 1)))\n",
    "        self.trainable_weights = [self.W, self.b, self.u]\n",
    "        super(HAttLayer, self).build(input_shape)\n",
    "\n",
    "    def compute_mask(self, inputs, mask=None):\n",
    "        return mask\n",
    "\n",
    "    def call(self, x, mask=None):\n",
    "        # size of x :[batch_size, sel_len, attention_dim]\n",
    "        # size of u :[batch_size, attention_dim]\n",
    "        # uit = tanh(xW+b)\n",
    "        uit = K.tanh(K.bias_add(K.dot(x, self.W), self.b))\n",
    "        ait = K.dot(uit, self.u)\n",
    "        ait = K.squeeze(ait, -1)\n",
    "\n",
    "        ait = K.exp(ait)\n",
    "\n",
    "        if mask is not None:\n",
    "            # Cast the mask to floatX to avoid float64 upcasting in theano\n",
    "            ait *= K.cast(mask, K.floatx())\n",
    "        ait /= K.cast(K.sum(ait, axis=1, keepdims=True) + K.epsilon(), K.floatx())\n",
    "        ait = K.expand_dims(ait)\n",
    "        weighted_input = x * ait\n",
    "        output = K.sum(weighted_input, axis=1)\n",
    "\n",
    "        return output\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return (input_shape[0], input_shape[-1])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#list of numpy arrays as input\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "concat_train=[]\n",
    "for x in range(len(x1_train)):\n",
    "    concat_train.append(np.vstack((x1_train[x], x2_train[x])))    #5708"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "concat_dev=[]\n",
    "for x in range(len(x1_dev)):\n",
    "    concat_dev.append(np.vstack((x1_dev[x], x2_dev[x])))         #537"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "concat_train_l=[]\n",
    "concat_dev_l=[]\n",
    "for x in range(len(train_label1)):\n",
    "    \n",
    "    concat_train_l.append(np.vstack((train_label1[0], train_label2[0])))\n",
    "\n",
    "\n",
    "for x in range(len(dev_label1)):\n",
    "    \n",
    "    concat_dev_l.append(np.vstack((dev_label1[0], dev_label2[0])))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 385,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_label1_reshape=train_label1.reshape(5708, 290)   #reshape\n",
    "train_label2_reshape=train_label2.reshape(5708, 290)\n",
    "dev_label1_reshape=dev_label1.reshape(537, 290)\n",
    "dev_label2_reshape=dev_label2.reshape(537, 290)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 388,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[1., 0., 0., 0., 0.],\n",
       "        [1., 0., 0., 0., 0.],\n",
       "        [1., 0., 0., 0., 0.],\n",
       "        ...,\n",
       "        [1., 0., 0., 0., 0.],\n",
       "        [1., 0., 0., 0., 0.],\n",
       "        [1., 0., 0., 0., 0.]],\n",
       "\n",
       "       [[1., 0., 0., 0., 0.],\n",
       "        [1., 0., 0., 0., 0.],\n",
       "        [0., 1., 0., 0., 0.],\n",
       "        ...,\n",
       "        [1., 0., 0., 0., 0.],\n",
       "        [1., 0., 0., 0., 0.],\n",
       "        [1., 0., 0., 0., 0.]],\n",
       "\n",
       "       [[1., 0., 0., 0., 0.],\n",
       "        [1., 0., 0., 0., 0.],\n",
       "        [0., 1., 0., 0., 0.],\n",
       "        ...,\n",
       "        [1., 0., 0., 0., 0.],\n",
       "        [1., 0., 0., 0., 0.],\n",
       "        [1., 0., 0., 0., 0.]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[1., 0., 0., 0., 0.],\n",
       "        [0., 1., 0., 0., 0.],\n",
       "        [0., 0., 0., 1., 0.],\n",
       "        ...,\n",
       "        [1., 0., 0., 0., 0.],\n",
       "        [1., 0., 0., 0., 0.],\n",
       "        [1., 0., 0., 0., 0.]],\n",
       "\n",
       "       [[1., 0., 0., 0., 0.],\n",
       "        [1., 0., 0., 0., 0.],\n",
       "        [1., 0., 0., 0., 0.],\n",
       "        ...,\n",
       "        [1., 0., 0., 0., 0.],\n",
       "        [1., 0., 0., 0., 0.],\n",
       "        [1., 0., 0., 0., 0.]],\n",
       "\n",
       "       [[1., 0., 0., 0., 0.],\n",
       "        [1., 0., 0., 0., 0.],\n",
       "        [1., 0., 0., 0., 0.],\n",
       "        ...,\n",
       "        [1., 0., 0., 0., 0.],\n",
       "        [1., 0., 0., 0., 0.],\n",
       "        [1., 0., 0., 0., 0.]]], dtype=float32)"
      ]
     },
     "execution_count": 388,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_label1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "\"input_length\" is 58, but received input has shape (None, 116)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-48-c13bd8c9e769>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m top_embedded = embedding_layer(\n\u001b[0;32m---> 23\u001b[0;31m     top_input)\n\u001b[0m\u001b[1;32m     24\u001b[0m bm_embedded = embedding_layer(\n\u001b[1;32m     25\u001b[0m     bm_input)\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs, **kwargs)\u001b[0m\n\u001b[1;32m    472\u001b[0m             if all([s is not None\n\u001b[1;32m    473\u001b[0m                     for s in to_list(input_shape)]):\n\u001b[0;32m--> 474\u001b[0;31m                 \u001b[0moutput_shape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute_output_shape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_shape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    475\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    476\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_shape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/layers/embeddings.py\u001b[0m in \u001b[0;36mcompute_output_shape\u001b[0;34m(self, input_shape)\u001b[0m\n\u001b[1;32m    131\u001b[0m                         raise ValueError(\n\u001b[1;32m    132\u001b[0m                             \u001b[0;34m'\"input_length\" is %s, but received input has shape %s'\u001b[0m \u001b[0;34m%\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 133\u001b[0;31m                             (str(self.input_length), str(input_shape)))\n\u001b[0m\u001b[1;32m    134\u001b[0m                     \u001b[0;32melif\u001b[0m \u001b[0ms1\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    135\u001b[0m                         \u001b[0min_lens\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0ms2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: \"input_length\" is 58, but received input has shape (None, 116)"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "from keras import backend as K\n",
    "\n",
    "#this is arbitrary?\n",
    "\n",
    "\n",
    "NUM_LSTM_UNITS = 128 #output dimension\n",
    "\n",
    "\n",
    "NUM_EMBEDDING_DIM = EMBEDDING_DIM\n",
    "\n",
    "top_input = Input(\n",
    "    shape=(MAX_SEQUENCE_LENGTH*2, ),    #this is the first sentence\n",
    "    dtype='int32')\n",
    "\n",
    "bm_input = Input(\n",
    "    shape=(MAX_SEQUENCE_LENGTH*2, ),   #this is the second\n",
    "    dtype='int32')\n",
    "\n",
    "\n",
    "top_embedded = embedding_layer(\n",
    "    top_input)\n",
    "bm_embedded = embedding_layer(\n",
    "    bm_input)\n",
    "\n",
    "BiLSTM = Bidirectional(LSTM(NUM_LSTM_UNITS, return_sequences=True))\n",
    "\n",
    "\n",
    "\n",
    "top_output = BiLSTM(top_embedded)\n",
    "\n",
    "#bm_output = BiLSTM(bm_embedded)\n",
    "\n",
    "a=AttentionWithContext()(top_output)\n",
    "#b=AttentionWithContext()(bm_output)\n",
    "        #Attention Layer\n",
    "\n",
    "\n",
    "merged = concatenate(\n",
    "    [a,b], \n",
    "    axis=-1)\n",
    "\n",
    "\n",
    "dense =  Dense(\n",
    "    units=290, \n",
    "    activation='softmax')\n",
    "\n",
    "predictions1=dense(a)\n",
    "#predictions2 = dense(merged)\n",
    "\n",
    "\n",
    "model = Model(\n",
    "    inputs=[top_input], \n",
    "    outputs=[predictions1, predictions2])\n",
    "\n",
    "\n",
    "model.summary()\n",
    "\n",
    "model.compile(\n",
    "    optimizer='Adam',\n",
    "    loss='mean_squared_error',\n",
    "    metrics=['accuracy'])\n",
    "\n",
    "BATCH_SIZE = 72\n",
    "\n",
    "NUM_EPOCHS = 30\n",
    "\n",
    "earlyStopping = EarlyStopping(monitor='val_loss', patience=10, verbose=1, mode='min')\n",
    "mcp_save = ModelCheckpoint('../Phrase_Models_H5/zerohot_concatenate_attention.h5', save_best_only=True,  verbose=1, monitor='val_loss', mode='min')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 394,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 5708 samples, validate on 537 samples\n",
      "Epoch 1/30\n",
      "1944/5708 [=========>....................] - ETA: 32s - loss: 0.3915 - dense_34_loss: 0.1957 - dense_34_acc: 0.0000e+00 - dense_34_acc_1: 0.0000e+00"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-394-b2fb2117729b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m history = model.fit(x=[x1_train, x2_train], y=[train_label1_reshape, train_label2_reshape], batch_size=BATCH_SIZE, epochs=NUM_EPOCHS, callbacks=[earlyStopping,mcp_save], validation_data=([x1_dev,x2_dev], [dev_label1_reshape, dev_label2_reshape]),\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m )\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1037\u001b[0m                                         \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1038\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1039\u001b[0;31m                                         validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1040\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1041\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[0;34m(model, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m    197\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 199\u001b[0;31m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    200\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2713\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2714\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2715\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2716\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2717\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2673\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2674\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2675\u001b[0;31m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2676\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1437\u001b[0m           ret = tf_session.TF_SessionRunCallable(\n\u001b[1;32m   1438\u001b[0m               \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1439\u001b[0;31m               run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1440\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1441\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "history = model.fit(x=concat_train, y=concat_train_l, batch_size=BATCH_SIZE, epochs=NUM_EPOCHS, callbacks=[earlyStopping,mcp_save], validation_data=(concat_dev, dev_label1_reshape, concat_dev_l),\n",
    "    \n",
    "    shuffle=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for x in range (len(train_label1)):\n",
    "    zz=train_label2[x]+\n",
    "    new.append(zz)\n",
    "new=np.asarray(new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 416,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "all the input array dimensions except for the concatenation axis must match exactly",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-416-dcb45b662c52>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mconcat_train\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_sen1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mconcat_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_sen1\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_sen2\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mconcat_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/numpy/core/shape_base.py\u001b[0m in \u001b[0;36mvstack\u001b[0;34m(tup)\u001b[0m\n\u001b[1;32m    232\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    233\u001b[0m     \"\"\"\n\u001b[0;32m--> 234\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_nx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0matleast_2d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_m\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0m_m\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtup\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    235\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    236\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mhstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtup\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: all the input array dimensions except for the concatenation axis must match exactly"
     ]
    }
   ],
   "source": [
    "concat_train=[]\n",
    "for x in range (len(train_sen1)):\n",
    "    concat_train.append(np.vstack((train_sen1[x], train_sen2[x])))\n",
    "concat_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'tolist'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-325-0d8a32033a45>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mx1_label_d\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mx1_label_d\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mx2_label_d\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mx2_label_d\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mnew\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx1_label_d\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mzz\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx1_label_d\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mx2_label_d\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'tolist'"
     ]
    }
   ],
   "source": [
    "x1_label_d=x1_label_d.tolist()\n",
    "x2_label_d=x2_label_d.tolist()\n",
    "new=[]\n",
    "for x in range (len(x1_label_d)):\n",
    "    zz=(x1_label_d[x]+x2_label_d[x])\n",
    "    new2.append(zz)\n",
    "new2=np.asarray(new2)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Error when checking model target: the list of Numpy arrays that you are passing to your model is not the size the model expected. Expected to see 1 array(s), but instead got the following list of 2 arrays: [array([[[1., 0., 0., 0., 0.],\n        [1., 0., 0., 0., 0.],\n        [1., 0., 0., 0., 0.],\n        ...,\n        [1., 0., 0., 0., 0.],\n        [1., 0., 0., 0., 0.],\n        [1., 0., 0., 0., 0.]],\n\n    ...",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-335-2b8819482f13>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m history = model.fit(x=[x1_train, x2_train], y=[train_label1, train_label2], batch_size=BATCH_SIZE, epochs=NUM_EPOCHS, callbacks=[earlyStopping,mcp_save], validation_data=([x1_dev,x2_dev], [dev_label1, dev_label2]),\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m )\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m    950\u001b[0m             \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    951\u001b[0m             \u001b[0mclass_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 952\u001b[0;31m             batch_size=batch_size)\n\u001b[0m\u001b[1;32m    953\u001b[0m         \u001b[0;31m# Prepare validation data.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    954\u001b[0m         \u001b[0mdo_validation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[0;34m(self, x, y, sample_weight, class_weight, check_array_lengths, batch_size)\u001b[0m\n\u001b[1;32m    787\u001b[0m                 \u001b[0mfeed_output_shapes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    788\u001b[0m                 \u001b[0mcheck_batch_axis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# Don't enforce the batch size.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 789\u001b[0;31m                 exception_prefix='target')\n\u001b[0m\u001b[1;32m    790\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    791\u001b[0m             \u001b[0;31m# Generate sample-wise weight values given the `sample_weight` and\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/engine/training_utils.py\u001b[0m in \u001b[0;36mstandardize_input_data\u001b[0;34m(data, names, shapes, check_batch_axis, exception_prefix)\u001b[0m\n\u001b[1;32m    100\u001b[0m                 \u001b[0;34m'Expected to see '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnames\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' array(s), '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m                 \u001b[0;34m'but instead got the following list of '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 102\u001b[0;31m                 str(len(data)) + ' arrays: ' + str(data)[:200] + '...')\n\u001b[0m\u001b[1;32m    103\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnames\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m             raise ValueError(\n",
      "\u001b[0;31mValueError\u001b[0m: Error when checking model target: the list of Numpy arrays that you are passing to your model is not the size the model expected. Expected to see 1 array(s), but instead got the following list of 2 arrays: [array([[[1., 0., 0., 0., 0.],\n        [1., 0., 0., 0., 0.],\n        [1., 0., 0., 0., 0.],\n        ...,\n        [1., 0., 0., 0., 0.],\n        [1., 0., 0., 0., 0.],\n        [1., 0., 0., 0., 0.]],\n\n    ..."
     ]
    }
   ],
   "source": [
    "\n",
    "history = model.fit(x=[x1_train, x2_train], y=[train_label1, train_label2], batch_size=BATCH_SIZE, epochs=NUM_EPOCHS, callbacks=[earlyStopping,mcp_save], validation_data=([x1_dev,x2_dev], [dev_label1, dev_label2]),\n",
    "    \n",
    "    shuffle=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 5708 samples, validate on 537 samples\n",
      "Epoch 1/30\n",
      "5708/5708 [==============================] - 31s 5ms/step - loss: 0.0606 - dense_26_loss: 0.0255 - dense_26_acc: 0.0801 - dense_26_acc_1: 0.0879 - val_loss: 0.0363 - val_dense_26_loss: 0.0112 - val_dense_26_acc: 0.0577 - val_dense_26_acc_1: 0.0801\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.03632, saving model to ../Phrase_Models_H5/big_merge_attention.h5\n",
      "Epoch 2/30\n",
      "5708/5708 [==============================] - 25s 4ms/step - loss: 0.0590 - dense_26_loss: 0.0248 - dense_26_acc: 0.1228 - dense_26_acc_1: 0.1312 - val_loss: 0.0367 - val_dense_26_loss: 0.0111 - val_dense_26_acc: 0.0633 - val_dense_26_acc_1: 0.1080\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 0.03632\n",
      "Epoch 3/30\n",
      "5708/5708 [==============================] - 25s 4ms/step - loss: 0.0554 - dense_26_loss: 0.0234 - dense_26_acc: 0.1869 - dense_26_acc_1: 0.1910 - val_loss: 0.0365 - val_dense_26_loss: 0.0112 - val_dense_26_acc: 0.0596 - val_dense_26_acc_1: 0.1527\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.03632\n",
      "Epoch 4/30\n",
      "5708/5708 [==============================] - 25s 4ms/step - loss: 0.0513 - dense_26_loss: 0.0217 - dense_26_acc: 0.2409 - dense_26_acc_1: 0.2565 - val_loss: 0.0376 - val_dense_26_loss: 0.0117 - val_dense_26_acc: 0.0689 - val_dense_26_acc_1: 0.1993\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.03632\n",
      "Epoch 5/30\n",
      "5708/5708 [==============================] - 24s 4ms/step - loss: 0.0474 - dense_26_loss: 0.0202 - dense_26_acc: 0.2880 - dense_26_acc_1: 0.3276 - val_loss: 0.0389 - val_dense_26_loss: 0.0120 - val_dense_26_acc: 0.0559 - val_dense_26_acc_1: 0.1713\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.03632\n",
      "Epoch 6/30\n",
      "5708/5708 [==============================] - 24s 4ms/step - loss: 0.0445 - dense_26_loss: 0.0189 - dense_26_acc: 0.3138 - dense_26_acc_1: 0.3670 - val_loss: 0.0388 - val_dense_26_loss: 0.0119 - val_dense_26_acc: 0.0987 - val_dense_26_acc_1: 0.2421\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.03632\n",
      "Epoch 7/30\n",
      "5708/5708 [==============================] - 24s 4ms/step - loss: 0.0422 - dense_26_loss: 0.0179 - dense_26_acc: 0.3395 - dense_26_acc_1: 0.4122 - val_loss: 0.0392 - val_dense_26_loss: 0.0121 - val_dense_26_acc: 0.0875 - val_dense_26_acc_1: 0.1769\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.03632\n",
      "Epoch 8/30\n",
      "5708/5708 [==============================] - 25s 4ms/step - loss: 0.0399 - dense_26_loss: 0.0169 - dense_26_acc: 0.3483 - dense_26_acc_1: 0.4434 - val_loss: 0.0398 - val_dense_26_loss: 0.0123 - val_dense_26_acc: 0.0857 - val_dense_26_acc_1: 0.1881\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.03632\n",
      "Epoch 9/30\n",
      "5708/5708 [==============================] - 25s 4ms/step - loss: 0.0382 - dense_26_loss: 0.0162 - dense_26_acc: 0.3646 - dense_26_acc_1: 0.4713 - val_loss: 0.0407 - val_dense_26_loss: 0.0127 - val_dense_26_acc: 0.0652 - val_dense_26_acc_1: 0.1490\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.03632\n",
      "Epoch 10/30\n",
      "5708/5708 [==============================] - 25s 4ms/step - loss: 0.0369 - dense_26_loss: 0.0156 - dense_26_acc: 0.3700 - dense_26_acc_1: 0.4928 - val_loss: 0.0399 - val_dense_26_loss: 0.0125 - val_dense_26_acc: 0.0912 - val_dense_26_acc_1: 0.1918\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.03632\n",
      "Epoch 11/30\n",
      "5708/5708 [==============================] - 25s 4ms/step - loss: 0.0357 - dense_26_loss: 0.0151 - dense_26_acc: 0.3816 - dense_26_acc_1: 0.5061 - val_loss: 0.0398 - val_dense_26_loss: 0.0125 - val_dense_26_acc: 0.1192 - val_dense_26_acc_1: 0.2104\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 0.03632\n",
      "Epoch 00011: early stopping\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(x=[x1_train, x2_train], y=[x1_label_t, x2_label_t], batch_size=BATCH_SIZE, epochs=NUM_EPOCHS, callbacks=[earlyStopping,mcp_save], validation_data=([x1_dev,x2_dev],[x1_label_d, x2_label_d]),\n",
    "    \n",
    "    shuffle=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_53 (InputLayer)           (None, 58)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_54 (InputLayer)           (None, 58)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_2 (Embedding)         (None, 58, 300)      3000300     input_53[0][0]                   \n",
      "                                                                 input_54[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_27 (Bidirectional (None, 256)          439296      embedding_2[18][0]               \n",
      "                                                                 embedding_2[19][0]               \n",
      "__________________________________________________________________________________________________\n",
      "dense_24 (Dense)                (None, 290)          74530       bidirectional_27[0][0]           \n",
      "                                                                 bidirectional_27[1][0]           \n",
      "==================================================================================================\n",
      "Total params: 3,514,126\n",
      "Trainable params: 513,826\n",
      "Non-trainable params: 3,000,300\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#BiLSTM without attention\n",
    "\n",
    "\n",
    "#this is arbitrary?\n",
    "\n",
    "NUM_LSTM_UNITS = 128 #output dimension\n",
    "\n",
    "\n",
    "NUM_EMBEDDING_DIM = EMBEDDING_DIM\n",
    "\n",
    "top_input = Input(\n",
    "    shape=(MAX_SEQUENCE_LENGTH, ),    #this is the first sentence\n",
    "    )\n",
    "\n",
    "bm_input = Input(\n",
    "    shape=(MAX_SEQUENCE_LENGTH, ),   #this is the second\n",
    "    )\n",
    "\n",
    "\n",
    "top_embedded = embedding_layer(\n",
    "    top_input)\n",
    "bm_embedded = embedding_layer(\n",
    "    bm_input)\n",
    "\n",
    "BiLSTM = Bidirectional(LSTM(NUM_LSTM_UNITS, return_sequences=True))\n",
    "\n",
    "\n",
    "\n",
    "top_output = BiLSTM(top_embedded)\n",
    "\n",
    "bm_output = BiLSTM(bm_embedded)\n",
    "\n",
    "\n",
    "  \n",
    "\n",
    "dense =  Dense(\n",
    "    units=290, \n",
    "    activation='sigmoid')\n",
    "\n",
    "predictions1 = dense(top_output)\n",
    "\n",
    "predictions2 = dense(bm_output)\n",
    "\n",
    "\n",
    "model = Model(\n",
    "    inputs=[top_input, bm_input], \n",
    "    outputs=[predictions1, predictions2])\n",
    "\n",
    "\n",
    "model.summary()\n",
    "\n",
    "model.compile(\n",
    "    optimizer='Adam',\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=['accuracy'])\n",
    "\n",
    "BATCH_SIZE = 72\n",
    "\n",
    "NUM_EPOCHS = 30\n",
    "\n",
    "earlyStopping = EarlyStopping(monitor='val_loss', patience=7, verbose=1, mode='min')\n",
    "mcp_save = ModelCheckpoint('../Phrase_Models_H5/big_merge.h5', save_best_only=True,  verbose=1, monitor='val_loss', mode='min')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5708, 58, 5)"
      ]
     },
     "execution_count": 250,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 5708 samples, validate on 537 samples\n",
      "Epoch 1/30\n",
      "5708/5708 [==============================] - 29s 5ms/step - loss: 515.2046 - dense_20_loss: 256.7866 - dense_20_acc: 0.0000e+00 - dense_20_acc_1: 0.0000e+00 - val_loss: 486.2978 - val_dense_20_loss: 242.2169 - val_dense_20_acc: 0.0000e+00 - val_dense_20_acc_1: 0.0000e+00\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 486.29782, saving model to ../Phrase_Models_H5/big_merge.h5\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-256-c3e1ec4dee70>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m history = model.fit(x=[x1_train, x2_train], y=[train_label1, train_label2], batch_size=BATCH_SIZE, epochs=NUM_EPOCHS, callbacks=[earlyStopping,mcp_save], validation_data=([x1_dev, x2_dev],[dev_label1, dev_label2]),\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m )\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1037\u001b[0m                                         \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1038\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1039\u001b[0;31m                                         validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1040\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1041\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[0;34m(model, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m    215\u001b[0m                         \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_outs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    216\u001b[0m                             \u001b[0mepoch_logs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'val_'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mo\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 217\u001b[0;31m         \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_epoch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch_logs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    218\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcallback_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop_training\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    219\u001b[0m             \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/callbacks.py\u001b[0m in \u001b[0;36mon_epoch_end\u001b[0;34m(self, epoch, logs)\u001b[0m\n\u001b[1;32m     77\u001b[0m         \u001b[0mlogs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlogs\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mcallback\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 79\u001b[0;31m             \u001b[0mcallback\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_epoch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     80\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mon_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/callbacks.py\u001b[0m in \u001b[0;36mon_epoch_end\u001b[0;34m(self, epoch, logs)\u001b[0m\n\u001b[1;32m    444\u001b[0m                             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moverwrite\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    445\u001b[0m                         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 446\u001b[0;31m                             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moverwrite\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    447\u001b[0m                     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    448\u001b[0m                         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mverbose\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/engine/network.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(self, filepath, overwrite, include_optimizer)\u001b[0m\n\u001b[1;32m   1088\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mNotImplementedError\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1089\u001b[0m         \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msave_model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1090\u001b[0;31m         \u001b[0msave_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilepath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moverwrite\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minclude_optimizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1091\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1092\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0msave_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilepath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moverwrite\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/engine/saving.py\u001b[0m in \u001b[0;36msave_model\u001b[0;34m(model, filepath, overwrite, include_optimizer)\u001b[0m\n\u001b[1;32m    380\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    381\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 382\u001b[0;31m         \u001b[0m_serialize_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minclude_optimizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    383\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    384\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mopened_new_file\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/engine/saving.py\u001b[0m in \u001b[0;36m_serialize_model\u001b[0;34m(model, f, include_optimizer)\u001b[0m\n\u001b[1;32m    140\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0msymbolic_weights\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m                 \u001b[0moptimizer_weights_group\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'optimizer_weights'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 142\u001b[0;31m                 \u001b[0mweight_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mK\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_get_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msymbolic_weights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    143\u001b[0m                 \u001b[0mweight_names\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    144\u001b[0m                 for i, (w, val) in enumerate(zip(symbolic_weights,\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36mbatch_get_value\u001b[0;34m(ops)\u001b[0m\n\u001b[1;32m   2418\u001b[0m     \"\"\"\n\u001b[1;32m   2419\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2420\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mget_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mops\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2421\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2422\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    927\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    928\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 929\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    930\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    931\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1150\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1151\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1152\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1153\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1154\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1326\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1327\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1328\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1329\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1330\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1332\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1333\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1334\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1335\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1336\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1317\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1318\u001b[0m       return self._call_tf_sessionrun(\n\u001b[0;32m-> 1319\u001b[0;31m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1320\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1321\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1405\u001b[0m     return tf_session.TF_SessionRun_wrapper(\n\u001b[1;32m   1406\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1407\u001b[0;31m         run_metadata)\n\u001b[0m\u001b[1;32m   1408\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1409\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_tf_sessionprun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "history = model.fit(x=[x1_train, x2_train], y=[train_label1, train_label2], batch_size=BATCH_SIZE, epochs=NUM_EPOCHS, callbacks=[earlyStopping,mcp_save], validation_data=([x1_dev, x2_dev],[dev_label1, dev_label2]),\n",
    "    \n",
    "    shuffle=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved model to disk\n"
     ]
    }
   ],
   "source": [
    "\n",
    "model.save(\"BiLSTM_PAWS_QQP.h5\")        ##first attempt\n",
    "print(\"Saved model to disk\")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Unknown layer: HAttLayer",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-65-f926a3b39fad>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"../Phrase_Models_H5/PAWS_Wiki_PHRASE.h5\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/engine/saving.py\u001b[0m in \u001b[0;36mload_model\u001b[0;34m(filepath, custom_objects, compile)\u001b[0m\n\u001b[1;32m    417\u001b[0m     \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mh5dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    418\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 419\u001b[0;31m         \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_deserialize_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcustom_objects\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    420\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    421\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mopened_new_file\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/engine/saving.py\u001b[0m in \u001b[0;36m_deserialize_model\u001b[0;34m(f, custom_objects, compile)\u001b[0m\n\u001b[1;32m    223\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'No model found in config.'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    224\u001b[0m     \u001b[0mmodel_config\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_config\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'utf-8'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 225\u001b[0;31m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_from_config\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_config\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcustom_objects\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcustom_objects\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    226\u001b[0m     \u001b[0mmodel_weights_group\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'model_weights'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    227\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/engine/saving.py\u001b[0m in \u001b[0;36mmodel_from_config\u001b[0;34m(config, custom_objects)\u001b[0m\n\u001b[1;32m    456\u001b[0m                         '`Sequential.from_config(config)`?')\n\u001b[1;32m    457\u001b[0m     \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdeserialize\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 458\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mdeserialize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcustom_objects\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcustom_objects\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    459\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    460\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/layers/__init__.py\u001b[0m in \u001b[0;36mdeserialize\u001b[0;34m(config, custom_objects)\u001b[0m\n\u001b[1;32m     53\u001b[0m                                     \u001b[0mmodule_objects\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mglobs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m                                     \u001b[0mcustom_objects\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcustom_objects\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 55\u001b[0;31m                                     printable_module_name='layer')\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/utils/generic_utils.py\u001b[0m in \u001b[0;36mdeserialize_keras_object\u001b[0;34m(identifier, module_objects, custom_objects, printable_module_name)\u001b[0m\n\u001b[1;32m    143\u001b[0m                     \u001b[0mconfig\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'config'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    144\u001b[0m                     custom_objects=dict(list(_GLOBAL_CUSTOM_OBJECTS.items()) +\n\u001b[0;32m--> 145\u001b[0;31m                                         list(custom_objects.items())))\n\u001b[0m\u001b[1;32m    146\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mCustomObjectScope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcustom_objects\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    147\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_config\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'config'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/engine/network.py\u001b[0m in \u001b[0;36mfrom_config\u001b[0;34m(cls, config, custom_objects)\u001b[0m\n\u001b[1;32m   1020\u001b[0m         \u001b[0;31m# First, we create all layers and enqueue nodes to be processed\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1021\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mlayer_data\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'layers'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1022\u001b[0;31m             \u001b[0mprocess_layer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlayer_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1023\u001b[0m         \u001b[0;31m# Then we process nodes in order of layer depth.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1024\u001b[0m         \u001b[0;31m# Nodes that cannot yet be processed (if the inbound node\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/engine/network.py\u001b[0m in \u001b[0;36mprocess_layer\u001b[0;34m(layer_data)\u001b[0m\n\u001b[1;32m   1006\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1007\u001b[0m             layer = deserialize_layer(layer_data,\n\u001b[0;32m-> 1008\u001b[0;31m                                       custom_objects=custom_objects)\n\u001b[0m\u001b[1;32m   1009\u001b[0m             \u001b[0mcreated_layers\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlayer_name\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1010\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/layers/__init__.py\u001b[0m in \u001b[0;36mdeserialize\u001b[0;34m(config, custom_objects)\u001b[0m\n\u001b[1;32m     53\u001b[0m                                     \u001b[0mmodule_objects\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mglobs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m                                     \u001b[0mcustom_objects\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcustom_objects\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 55\u001b[0;31m                                     printable_module_name='layer')\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/utils/generic_utils.py\u001b[0m in \u001b[0;36mdeserialize_keras_object\u001b[0;34m(identifier, module_objects, custom_objects, printable_module_name)\u001b[0m\n\u001b[1;32m    136\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mcls\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    137\u001b[0m                 raise ValueError('Unknown ' + printable_module_name +\n\u001b[0;32m--> 138\u001b[0;31m                                  ': ' + class_name)\n\u001b[0m\u001b[1;32m    139\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'from_config'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m             \u001b[0mcustom_objects\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcustom_objects\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Unknown layer: HAttLayer"
     ]
    }
   ],
   "source": [
    "model = load_model(\"../Phrase_Models_H5/PAWS_Wiki_PHRASE.h5\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "predicts = model.predict(\n",
    "    [x1_train, x2_train])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3.0341744e-04 9.4264746e-05 1.8477440e-05 2.0176172e-05 7.6800585e-05\n",
      " 4.6434999e-04 1.1703372e-04 1.2126565e-04 2.7704239e-04 3.5896897e-04\n",
      " 4.5788288e-04 1.6086102e-03 2.0542145e-03 3.3183098e-03 4.0072203e-03\n",
      " 5.5718124e-03 5.0963759e-03 2.3481518e-02 2.1280676e-01 1.6916370e-01\n",
      " 1.4345798e-01 4.0997237e-02 1.9110203e-02 1.1213988e-02 1.4996201e-02\n",
      " 4.2207539e-03 2.4484098e-03 2.4202466e-03 2.3058057e-04 2.8967857e-05\n",
      " 7.7039003e-05 3.9488077e-05 8.2224607e-05 3.2246113e-05 2.6464462e-05\n",
      " 5.6028366e-06 1.0550022e-05 1.3023615e-05 4.1872263e-05 3.6954880e-05\n",
      " 5.9604645e-08 3.5762787e-07 4.7683716e-07 2.6822090e-07 2.0861626e-07\n",
      " 8.9406967e-08 1.4901161e-07 2.6822090e-07 4.8875809e-06 2.6226044e-06\n",
      " 8.3446503e-07 8.0466270e-07 1.7881393e-07 2.0861626e-07 2.6822090e-07\n",
      " 6.2584877e-07 5.9604645e-07 1.4901161e-07]\n"
     ]
    }
   ],
   "source": [
    "print((predicts[0][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7814212256596139\n"
     ]
    }
   ],
   "source": [
    "real1=train.sentence1\n",
    "real2=train.sentence2\n",
    "realCat=train.label\n",
    "no = []\n",
    "err1 = []\n",
    "err2 = []\n",
    "wrongCat = []\n",
    "\n",
    "\n",
    "for x in range(len(predicts)):\n",
    "    if (np.argmax(predicts[x])!=(train.label[x])):\n",
    "        no=np.append(no, x)\n",
    "        err1=np.append(err1, real1[x])\n",
    "        err2=np.append(err2, real2[x])\n",
    "        wrongCat = np.append(wrongCat, realCat[x])\n",
    "print(1-(len(wrongCat)/len(predicts)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "    <thead>\n",
       "        <tr>\n",
       "            <th>id</th> <th>sent1</th> <th>sent2</th> <th>label</th>\n",
       "        </tr>\n",
       "    </thead>\n",
       "    <tbody>\n",
       "        <tr>\n",
       "            <td>1   </td> <td>b'How can you treat ocd ? Is there any helpful suggestio ...</td> <td>b'How can you treat OCD ? Is there any helpful suggestio ...</td> <td>1    </td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>7   </td> <td>b'What are some beautiful lines to comment on Beautiful  ...</td> <td>b'What are some Beautiful lines to comment on beautiful  ...</td> <td>1    </td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>8   </td> <td>b'Can small dogs breed with large dogs ?'                   </td> <td>b'Can small dogs breed with large dogs ?'                   </td> <td>1    </td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>13  </td> <td>b'Are Indian IT services companies like Tech Mahindra/ W ...</td> <td>b'Are Indian IT services companies like Wipro/TCS/Tech . ...</td> <td>1    </td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>14  </td> <td>b'Is nuclear energy renewable energy ?'                     </td> <td>b\"`` Is nuclear energy `` renewable energy `` ? ''\"         </td> <td>1    </td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>15  </td> <td>b'What should I solve and what or how should I revise fo ...</td> <td>b'What should I revise and what or how should I solve fo ...</td> <td>0    </td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>20  </td> <td>b'How is the relative ratio of brain waves ( alpha/beta/ ...</td> <td>b'How is the ratio of relative brain waves ( alpha/beta/ ...</td> <td>0    </td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>21  </td> <td>b'How can I make an android app using Python 3 ? Is ther ...</td> <td>b'How can I develop an android app using Python 3 ? Is t ...</td> <td>1    </td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>24  </td> <td>b'Headbreak : What can I do to prevent heart break night ...</td> <td>b'Headbreak : What can I do to prevent heart break night ...</td> <td>1    </td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>25  </td> <td>b'When is latent heat positive ?'                           </td> <td>b'When is latent heat positive ?'                           </td> <td>1    </td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>28  </td> <td>b\"What does the phrase 'Do you feel me ' mean ?\"            </td> <td>b\"What does the phrase `Do you feel me'mean ?\"              </td> <td>1    </td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>29  </td> <td>b'What are some unexpected things first-time visitors to ...</td> <td>b'What are some unexpected things , first-time visitors  ...</td> <td>1    </td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>35  </td> <td>b'How do I prepare for cognizant campus placement online ...</td> <td>b\"How do I prepare for Cognizant 's online campus placem ...</td> <td>1    </td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>40  </td> <td>b'Which city is better to choose in India to settle from ...</td> <td>b'Which city is better to settle in India to choose from ...</td> <td>1    </td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>53  </td> <td>b'How does quality of life in Vancouver compare to that  ...</td> <td>b'How does quality of life in Vancouver compare to that  ...</td> <td>1    </td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>59  </td> <td>b'How do you write an algorithm to find the sum of the f ...</td> <td>b'How do you write an algorithm to find the sum of the f ...</td> <td>1    </td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>61  </td> <td>b'I have only a very nice startup idea but I m not good  ...</td> <td>b'I have only a very good startup idea but I m not nice  ...</td> <td>1    </td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>67  </td> <td>b'What should a mechanical engineer do besides doing col ...</td> <td>b'What should a mechanical engineer do besides studying  ...</td> <td>1    </td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>73  </td> <td>b'How can entropy of two reversible process be same , be ...</td> <td>b'How can entropy of two same process be reversible , be ...</td> <td>0    </td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>83  </td> <td>b'During muscle gain program , Heavy weight with more re ...</td> <td>b'During muscle gain program , Heavy weight with less re ...</td> <td>0    </td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>85  </td> <td>b'When does the second batch of training for freshers in ...</td> <td>b'When does the first batch of training for freshers in  ...</td> <td>0    </td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>105 </td> <td>b'It is possible to apply at the Diversity Immigrant Vis ...</td> <td>b'It is possible to get at the Diversity Immigrant Visa  ...</td> <td>0    </td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>106 </td> <td>b\"Why do n't audiobooks create Text-to-speech technology ...</td> <td>b\"Why do n't audiobooks use Text-to-speech technology to ...</td> <td>0    </td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>108 </td> <td>b'I have polycythemia . Can I qualify for government job ...</td> <td>b'I have polycythemia can I apply for government job can ...</td> <td>0    </td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>109 </td> <td>b'If I see a video on YouTube , does the owner of the vi ...</td> <td>b'If I watch a video on YouTube , does the owner of the  ...</td> <td>1    </td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>111 </td> <td>b'Who should they cast if they remake Friends in Bollywo ...</td> <td>b'Who should they cast if they remake Friends in Bollywo ...</td> <td>1    </td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>113 </td> <td>b\"I want to learn piano but I do n't have the basic know ...</td> <td>b\"I want to learn piano but I do n't have the basic know ...</td> <td>1    </td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>120 </td> <td>b'Can you shoot a Dangerous Human easily death with only ...</td> <td>b'Can you shoot a Dangerous Human easily death with only ...</td> <td>1    </td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>123 </td> <td>b'Why Are angels shown as having wings ? are they ever s ...</td> <td>b'Why are angels shown as having wings ? Are they ever s ...</td> <td>1    </td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>134 </td> <td>b\"I have to apply fresh passport for my wife and childre ...</td> <td>b\"I have to apply fresh passport for my wife and childre ...</td> <td>1    </td>\n",
       "        </tr>\n",
       "    </tbody>\n",
       "</table>\n",
       "<p>... (2563 rows omitted)</p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "error_analysis = Table()\n",
    "\n",
    "\n",
    "error_analysis = error_analysis.with_column(\"id\", no, \"sent1\", err1, \"sent2\", err2, \"label\", wrongCat)\n",
    "\n",
    "\n",
    "#false POSITIVE: swap two words\n",
    "#false negative: swap two specific nouns\n",
    "error_analysis.show(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "    <thead>\n",
       "        <tr>\n",
       "            <th>id</th> <th>sent1</th> <th>sent2</th> <th>label</th>\n",
       "        </tr>\n",
       "    </thead>\n",
       "    <tbody>\n",
       "        <tr>\n",
       "            <td>1   </td> <td>b'How can you treat ocd ? Is there any helpful suggestio ...</td> <td>b'How can you treat OCD ? Is there any helpful suggestio ...</td> <td>1    </td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>7   </td> <td>b'What are some beautiful lines to comment on Beautiful  ...</td> <td>b'What are some Beautiful lines to comment on beautiful  ...</td> <td>1    </td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>8   </td> <td>b'Can small dogs breed with large dogs ?'                   </td> <td>b'Can small dogs breed with large dogs ?'                   </td> <td>1    </td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>13  </td> <td>b'Are Indian IT services companies like Tech Mahindra/ W ...</td> <td>b'Are Indian IT services companies like Wipro/TCS/Tech . ...</td> <td>1    </td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>14  </td> <td>b'Is nuclear energy renewable energy ?'                     </td> <td>b\"`` Is nuclear energy `` renewable energy `` ? ''\"         </td> <td>1    </td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>21  </td> <td>b'How can I make an android app using Python 3 ? Is ther ...</td> <td>b'How can I develop an android app using Python 3 ? Is t ...</td> <td>1    </td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>24  </td> <td>b'Headbreak : What can I do to prevent heart break night ...</td> <td>b'Headbreak : What can I do to prevent heart break night ...</td> <td>1    </td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>25  </td> <td>b'When is latent heat positive ?'                           </td> <td>b'When is latent heat positive ?'                           </td> <td>1    </td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>28  </td> <td>b\"What does the phrase 'Do you feel me ' mean ?\"            </td> <td>b\"What does the phrase `Do you feel me'mean ?\"              </td> <td>1    </td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>29  </td> <td>b'What are some unexpected things first-time visitors to ...</td> <td>b'What are some unexpected things , first-time visitors  ...</td> <td>1    </td>\n",
       "        </tr>\n",
       "    </tbody>\n",
       "</table>\n",
       "<p>... (1967 rows omitted)</p>"
      ],
      "text/plain": [
       "id   | sent1                                                        | sent2                                                        | label\n",
       "1    | b'How can you treat ocd ? Is there any helpful suggestio ... | b'How can you treat OCD ? Is there any helpful suggestio ... | 1\n",
       "7    | b'What are some beautiful lines to comment on Beautiful  ... | b'What are some Beautiful lines to comment on beautiful  ... | 1\n",
       "8    | b'Can small dogs breed with large dogs ?'                    | b'Can small dogs breed with large dogs ?'                    | 1\n",
       "13   | b'Are Indian IT services companies like Tech Mahindra/ W ... | b'Are Indian IT services companies like Wipro/TCS/Tech . ... | 1\n",
       "14   | b'Is nuclear energy renewable energy ?'                      | b\"`` Is nuclear energy `` renewable energy `` ? ''\"          | 1\n",
       "21   | b'How can I make an android app using Python 3 ? Is ther ... | b'How can I develop an android app using Python 3 ? Is t ... | 1\n",
       "24   | b'Headbreak : What can I do to prevent heart break night ... | b'Headbreak : What can I do to prevent heart break night ... | 1\n",
       "25   | b'When is latent heat positive ?'                            | b'When is latent heat positive ?'                            | 1\n",
       "28   | b\"What does the phrase 'Do you feel me ' mean ?\"             | b\"What does the phrase `Do you feel me'mean ?\"               | 1\n",
       "29   | b'What are some unexpected things first-time visitors to ... | b'What are some unexpected things , first-time visitors  ... | 1\n",
       "... (1967 rows omitted)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
