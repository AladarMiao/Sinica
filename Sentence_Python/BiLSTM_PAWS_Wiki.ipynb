{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib\n",
    "import argparse\n",
    "import random\n",
    "import numpy as np\n",
    "import csv\n",
    "import pandas as pd\n",
    "from datascience import *\n",
    "import tensorflow as tf\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from keras.models import Model, load_model\n",
    "from keras.models import Sequential\n",
    "from keras.layers import *\n",
    "from keras.optimizers import Adam\n",
    "from keras import Input\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.utils import plot_model\n",
    "\n",
    "import keras\n",
    "#from keras.backend.tensorflow_backend import set_session\n",
    "#config = tf.ConfigProto()\n",
    "# config.gpu_options.allocator_type = 'BFC' #A \"Best-fit with coalescing\" algorithm, simplified from a version of dlmalloc.\n",
    "# config.gpu_options.per_process_gpu_memory_fraction = 0.7\n",
    "#config.gpu_options.allow_growth =True\n",
    "\n",
    "#set_session(tf.Session(config=config)) \n",
    "import re\n",
    "os.environ['PYTHONHASHSEED'] = '0'\n",
    "random.seed(1)\n",
    "np.random.seed(1)\n",
    "tf.set_random_seed(1)\n",
    "from keras.callbacks import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fri Jul 19 02:18:31 2019       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 418.56       Driver Version: 418.56       CUDA Version: 10.1     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  GeForce RTX 208...  On   | 00000000:D8:00.0 Off |                  N/A |\n",
      "| 27%   31C    P8    20W / 250W |  10827MiB / 10989MiB |      0%      Default |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                       GPU Memory |\n",
      "|  GPU       PID   Type   Process name                             Usage      |\n",
      "|=============================================================================|\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "InternalError",
     "evalue": "CUDA runtime implicit initialization on GPU:0 failed. Status: out of memory",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInternalError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-ae932be897c3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_gpu_available\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/test_util.py\u001b[0m in \u001b[0;36mis_gpu_available\u001b[0;34m(cuda_only, min_cuda_compute_capability)\u001b[0m\n\u001b[1;32m   1240\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1241\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1242\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mlocal_device\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdevice_lib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlist_local_devices\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1243\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mlocal_device\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice_type\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"GPU\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1244\u001b[0m         if (min_cuda_compute_capability is None or\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/device_lib.py\u001b[0m in \u001b[0;36mlist_local_devices\u001b[0;34m(session_config)\u001b[0m\n\u001b[1;32m     39\u001b[0m   return [\n\u001b[1;32m     40\u001b[0m       \u001b[0m_convert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m       \u001b[0;32mfor\u001b[0m \u001b[0ms\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpywrap_tensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlist_devices\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession_config\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msession_config\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m   ]\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/pywrap_tensorflow_internal.py\u001b[0m in \u001b[0;36mlist_devices\u001b[0;34m(session_config)\u001b[0m\n\u001b[1;32m   1827\u001b[0m                                           status)\n\u001b[1;32m   1828\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1829\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mListDevices\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstatus\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1830\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1831\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/errors_impl.py\u001b[0m in \u001b[0;36m__exit__\u001b[0;34m(self, type_arg, value_arg, traceback_arg)\u001b[0m\n\u001b[1;32m    526\u001b[0m             \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    527\u001b[0m             \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc_api\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_Message\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 528\u001b[0;31m             c_api.TF_GetCode(self.status.status))\n\u001b[0m\u001b[1;32m    529\u001b[0m     \u001b[0;31m# Delete the underlying status object from memory otherwise it stays alive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    530\u001b[0m     \u001b[0;31m# as there is a reference to status from this from the traceback due to\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mInternalError\u001b[0m: CUDA runtime implicit initialization on GPU:0 failed. Status: out of memory"
     ]
    }
   ],
   "source": [
    "print(tf.test.is_gpu_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sanitize(string):\n",
    "    words = string.split(' ')\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this is the test set\n",
    "testTb = Table()\n",
    "list1 = []\n",
    "list2 = []\n",
    "\n",
    "with open(\"dataset/PAWS_Wiki/train.tsv\", 'r', encoding='utf-8') as fd:\n",
    "    rd = csv.reader(fd, delimiter=\"\\t\", quotechar='\"')\n",
    "    for row in rd:\n",
    "        \n",
    "        list1.append(row) #pairs start at index 1, with id, setence1, sentence2, label, respectively.\n",
    "     \n",
    "    for y in range(4):\n",
    "        temp=[]\n",
    "        for x in range(1, len(list1)):\n",
    "            temp.append(list1[x][y])\n",
    "            \n",
    "        list2.append(temp)\n",
    "\n",
    "    for ele in range(len(list2)):\n",
    "        testTb=testTb.with_columns(list1[0][ele], list2[ele])\n",
    "\n",
    "\n",
    "testX = testTb.select(1, 2)\n",
    "testY = testTb.select(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "21829"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#this is the training set\n",
    "tb = Table()\n",
    "list1 = []\n",
    "list2 = []\n",
    "\n",
    "with open(\"dataset/PAWS_Wiki/train.tsv\", 'r', encoding='utf-8') as fd:\n",
    "    rd = csv.reader(fd, delimiter=\"\\t\", quotechar='\"')\n",
    "    for row in rd:\n",
    "        \n",
    "        list1.append(row) #pairs start at index 1, with id, setence1, sentence2, label, respectively.\n",
    "     \n",
    "    for y in range(4):\n",
    "        temp=[]\n",
    "        for x in range(1, len(list1)):\n",
    "            temp.append(list1[x][y])\n",
    "            \n",
    "        list2.append(temp)\n",
    "\n",
    "    for ele in range(len(list2)):\n",
    "        tb=tb.with_columns(list1[0][ele], list2[ele])\n",
    "        \n",
    "len(tb.where(\"label\", are.equal_to('1')).column(\"label\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   id                                          sentence1  \\\n",
      "0   1  In Paris , in October 1560 , he secretly met t...   \n",
      "1   2  The NBA season of 1975 -- 76 was the 30th seas...   \n",
      "2   3  There are also specific discussions , public p...   \n",
      "3   4  When comparable rates of flow can be maintaine...   \n",
      "4   5  It is the seat of Zerendi District in Akmola R...   \n",
      "\n",
      "                                           sentence2  label  \n",
      "0  In October 1560 , he secretly met with the Eng...      0  \n",
      "1  The 1975 -- 76 season of the National Basketba...      1  \n",
      "2  There are also public discussions , profile sp...      0  \n",
      "3  The results are high when comparable flow rate...      1  \n",
      "4  It is the seat of the district of Zerendi in A...      1  \n"
     ]
    }
   ],
   "source": [
    "#read in the csv files \n",
    "\n",
    "\n",
    "train = pd.read_csv(\"dataset/PAWS_Wiki/train.tsv\", sep='\\t')\n",
    "dev = pd.read_csv(\"dataset/PAWS_Wiki/dev.tsv\", sep='\\t')\n",
    "test = pd.read_csv(\"dataset/PAWS_Wiki/test.tsv\", sep='\\t')\n",
    "\n",
    "print(train[:5])\n",
    "\n",
    "train['title1_tokenized'] = \\\n",
    "    train.loc[:, 'sentence1'] \\\n",
    "         .apply(sanitize)\n",
    "train['title2_tokenized'] = \\\n",
    "    train.loc[:, 'sentence2'] \\\n",
    "         .apply(sanitize)\n",
    "\n",
    "\n",
    "dev['title1_tokenized'] = \\\n",
    "    dev.loc[:, 'sentence1'] \\\n",
    "         .apply(sanitize)\n",
    "dev['title2_tokenized'] = \\\n",
    "    dev.loc[:, 'sentence2'] \\\n",
    "         .apply(sanitize)\n",
    "\n",
    "\n",
    "test['title1_tokenized'] = \\\n",
    "    test.loc[:, 'sentence1'] \\\n",
    "         .apply(sanitize)\n",
    "test['title2_tokenized'] = \\\n",
    "    test.loc[:, 'sentence2'] \\\n",
    "         .apply(sanitize)\n",
    "\n",
    "#train_corpus_x1=tb.select(1)\n",
    "#train_corpus_x2=tb.select(2)\n",
    "#test_corpus_x1=testTb.select(1)\n",
    "#test_corpus_x2=testTb.select(2)\n",
    "#corpus = pd.concat([train_corpus_x1, train_corpus_x2, test_corpus_x1, test_corpus_x2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "98802\n",
      "130802\n"
     ]
    }
   ],
   "source": [
    "#create a dictionary\n",
    "\n",
    "MAX_NUM_WORDS = 10000\n",
    "tokenizer = keras \\\n",
    "    .preprocessing \\\n",
    "    .text \\\n",
    "    .Tokenizer(num_words=MAX_NUM_WORDS)\n",
    "\n",
    "train_corpus_x1 = train.title1_tokenized\n",
    "train_corpus_x2 = train.title2_tokenized\n",
    "train_corpus = pd.concat([\n",
    "    train_corpus_x1, train_corpus_x2])\n",
    "#corpus.shape\n",
    "tokenizer.fit_on_texts(train_corpus)\n",
    "tokenizer.texts_to_sequences(train_corpus)\n",
    "x1_train = tokenizer \\\n",
    "    .texts_to_sequences(train_corpus_x1)\n",
    "x2_train = tokenizer \\\n",
    "    .texts_to_sequences(train_corpus_x2)\n",
    "\n",
    "\n",
    "print(tokenizer.document_count)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "dev_corpus_x1 = dev.title1_tokenized\n",
    "dev_corpus_x2 = dev.title2_tokenized\n",
    "dev_corpus = pd.concat([\n",
    "    dev_corpus_x1, dev_corpus_x2])\n",
    "#corpus.shape\n",
    "tokenizer.fit_on_texts(dev_corpus)\n",
    "tokenizer.texts_to_sequences(dev_corpus)\n",
    "x1_dev = tokenizer \\\n",
    "    .texts_to_sequences(dev_corpus_x1)\n",
    "x2_dev = tokenizer \\\n",
    "    .texts_to_sequences(dev_corpus_x2)\n",
    "\n",
    "\n",
    "test_corpus_x1 = test.title1_tokenized\n",
    "test_corpus_x2 = test.title2_tokenized\n",
    "test_corpus = pd.concat([\n",
    "    test_corpus_x1, test_corpus_x2])\n",
    "#corpus.shape\n",
    "tokenizer.fit_on_texts(test_corpus)\n",
    "tokenizer.texts_to_sequences(test_corpus)\n",
    "x1_test = tokenizer \\\n",
    "    .texts_to_sequences(test_corpus_x1)\n",
    "x2_test = tokenizer \\\n",
    "    .texts_to_sequences(test_corpus_x2)\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "\n",
    "\n",
    "print(tokenizer.document_count)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,   43,\n",
       "           8,    7,   97,    6, 6066, 5611, 3080,    2,  446,   29, 6910,\n",
       "           4,    5, 3542,  683,   49, 3108, 1221,    4, 4527, 2922, 3890,\n",
       "          10,    1,    3],\n",
       "       [   0,    0,    0,    0,    0,    0,    0,    0,    0,   22,  131,\n",
       "        2253,   10, 1628,    4, 2550,   47,  275,   61,   22,  224,  108,\n",
       "        3874,    5,  141,    1, 1968,    6,    1,   67,  683,   34, 1742,\n",
       "          10,  280,    3],\n",
       "       [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    4,  120,  151,    2,    1, 1180, 1172,  198,\n",
       "           6, 5608,  905,    2,    2,    1, 3980,  304,   15,   32,  269,\n",
       "        2334, 2052,    3]], dtype=int32)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#padding and one-hot\n",
    "\n",
    "max_seq_len1 = max([\n",
    "    len(seq) for seq in x1_train])\n",
    "print(max_seq_len1)   #36\n",
    "\n",
    "MAX_SEQUENCE_LENGTH = 36  #better to have words covered than uncovered\n",
    "x1_train = keras \\\n",
    "    .preprocessing \\\n",
    "    .sequence \\\n",
    "    .pad_sequences(x1_train, \n",
    "                   maxlen=MAX_SEQUENCE_LENGTH)\n",
    "\n",
    "\n",
    "x2_train = keras \\\n",
    "    .preprocessing \\\n",
    "    .sequence \\\n",
    "    .pad_sequences(x2_train, \n",
    "                   maxlen=MAX_SEQUENCE_LENGTH)\n",
    "\n",
    "x1_dev = keras \\\n",
    "    .preprocessing \\\n",
    "    .sequence \\\n",
    "    .pad_sequences(x1_dev, \n",
    "                   maxlen=MAX_SEQUENCE_LENGTH)\n",
    "\n",
    "\n",
    "x2_dev = keras \\\n",
    "    .preprocessing \\\n",
    "    .sequence \\\n",
    "    .pad_sequences(x2_dev, \n",
    "                   maxlen=MAX_SEQUENCE_LENGTH)\n",
    "\n",
    "\n",
    "x1_test = keras \\\n",
    "    .preprocessing \\\n",
    "    .sequence \\\n",
    "    .pad_sequences(x1_test, \n",
    "                   maxlen=MAX_SEQUENCE_LENGTH)\n",
    "\n",
    "x2_test = keras \\\n",
    "    .preprocessing \\\n",
    "    .sequence \\\n",
    "    .pad_sequences(x2_test, \n",
    "                   maxlen=MAX_SEQUENCE_LENGTH)\n",
    "\n",
    "\n",
    "y_train = train.label\n",
    "\n",
    "y_train = keras \\\n",
    "    .utils \\\n",
    "    .to_categorical(y_train, num_classes=2)\n",
    "\n",
    "\n",
    "y_dev = dev.label\n",
    "\n",
    "y_dev = keras \\\n",
    "    .utils \\\n",
    "    .to_categorical(y_dev, num_classes=2)\n",
    "\n",
    "\n",
    "\n",
    "y_test = test.label\n",
    "\n",
    "y_test = keras \\\n",
    "    .utils \\\n",
    "    .to_categorical(y_test, num_classes=2)\n",
    "\n",
    "x1_test[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Set\n",
      "----------\n",
      "(49401, 36)\n",
      "(49401, 36)\n",
      "(49401, 2)\n",
      "(98802,)\n",
      "----------\n",
      "(8000, 36)\n",
      "(8000, 36)\n",
      "(8000, 2)\n",
      "(16000,)\n",
      "Test Set\n"
     ]
    }
   ],
   "source": [
    "print(\"Training Set\")\n",
    "print(\"-\" * 10)\n",
    "print(np.shape(x1_train))\n",
    "print(np.shape(x2_train))\n",
    "print(np.shape(y_train))\n",
    "print(np.shape(train_corpus))\n",
    "\n",
    "print(\"-\" * 10)\n",
    "print(np.shape(x1_test))\n",
    "print(np.shape(x2_test))\n",
    "print(np.shape(y_test))\n",
    "print(np.shape(test_corpus))\n",
    "\n",
    "print(\"Test Set\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_glove_line(line, dim):\n",
    "    word = None\n",
    "    embedding = None\n",
    "\n",
    "    try:\n",
    "        splitLine = line.split()\n",
    "        word = \" \".join(splitLine[:len(splitLine)-dim])\n",
    "        embedding = np.array([float(val) for val in splitLine[-dim:]])\n",
    "    except:\n",
    "        print(line)\n",
    "\n",
    "    return word, embedding\n",
    "\n",
    "def load_glove_model(glove_filepath, dim):\n",
    "    with open(glove_filepath, encoding=\"utf8\" ) as f:\n",
    "        content = f.readlines()\n",
    "        model = {}\n",
    "        for line in content:\n",
    "            word, embedding = process_glove_line(line, dim)\n",
    "            if embedding is not None:\n",
    "                model[word] = embedding\n",
    "        return model\n",
    "\n",
    "glove = load_glove_model(\"glove.840B.300d.txt\", 300)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_EMBEDDING_DIM = 300\n",
    "\n",
    "embedding_matrix = np.zeros((len(word_index)+1, NUM_EMBEDDING_DIM))\n",
    "\n",
    "\n",
    "for word, i in word_index.items():\n",
    "    embedding_vector = glove.get(word)\n",
    "    \n",
    "    if embedding_vector is not None:\n",
    "        # words not found in embedding index will be all-zeros.\n",
    "        embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'embedding_matrix' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-22-a575fa5f2140>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     24\u001b[0m embedding_layer = Embedding(len(word_index)+1,\n\u001b[1;32m     25\u001b[0m                             \u001b[0mNUM_EMBEDDING_DIM\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m                             \u001b[0mweights\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0membedding_matrix\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m                             \u001b[0minput_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mMAX_SEQUENCE_LENGTH\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m                             trainable=False)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'embedding_matrix' is not defined"
     ]
    }
   ],
   "source": [
    "#Siamese with GLoVE\n",
    "NUM_CLASSES=2 #boolean; 1 or 0\n",
    "\n",
    " #rough estimate\n",
    "\n",
    "MAX_SEQUENCE_LENGTH = 36 #how long one sentence is\n",
    "\n",
    " #this is arbitrary?\n",
    "\n",
    "NUM_LSTM_UNITS = 128 #output dimension\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "top_input = Input(\n",
    "    shape=(MAX_SEQUENCE_LENGTH, ),    #this is the first sentence\n",
    "    dtype='int32')\n",
    "\n",
    "bm_input = Input(\n",
    "    shape=(MAX_SEQUENCE_LENGTH, ),   #this is the second\n",
    "    dtype='int32')\n",
    "\n",
    "\n",
    "embedding_layer = Embedding(len(word_index)+1,\n",
    "                            NUM_EMBEDDING_DIM,\n",
    "                            weights=[embedding_matrix],\n",
    "                            input_length=MAX_SEQUENCE_LENGTH,\n",
    "                            trainable=False)\n",
    "\n",
    "top_embedded = embedding_layer(\n",
    "    top_input)\n",
    "bm_embedded = embedding_layer(\n",
    "    bm_input)\n",
    "\n",
    "\n",
    "shared_lstm = LSTM(NUM_LSTM_UNITS)\n",
    "top_output = shared_lstm(top_embedded)\n",
    "bm_output = shared_lstm(bm_embedded)\n",
    "\n",
    "\n",
    "merged = concatenate(\n",
    "    [top_output, bm_output], \n",
    "    axis=-1)\n",
    "\n",
    "\n",
    "dense =  Dense(\n",
    "    units=NUM_CLASSES, \n",
    "    activation='sigmoid')\n",
    "predictions = dense(merged)\n",
    "\n",
    "\n",
    "\n",
    "model = Model(\n",
    "    inputs=[top_input, bm_input], \n",
    "    outputs=predictions)\n",
    "\n",
    "\n",
    "model.summary()\n",
    "\n",
    "model.compile(\n",
    "    optimizer='Adam',\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=['accuracy'])\n",
    "\n",
    "BATCH_SIZE = 512\n",
    "\n",
    "NUM_EPOCHS = 5\n",
    "\n",
    "history = model.fit(x=[x1_train, x2_train], y=y_train, batch_size=BATCH_SIZE, epochs=NUM_EPOCHS, validation_data=([x1_test, x2_test], y_test),\n",
    "    \n",
    "    shuffle=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_15 (InputLayer)           (None, 36)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_16 (InputLayer)           (None, 36)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_7 (Embedding)         (None, 36, 300)      3000000     input_15[0][0]                   \n",
      "                                                                 input_16[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "lstm_7 (LSTM)                   (None, 128)          219648      embedding_7[0][0]                \n",
      "                                                                 embedding_7[1][0]                \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_7 (Concatenate)     (None, 256)          0           lstm_7[0][0]                     \n",
      "                                                                 lstm_7[1][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "dense_7 (Dense)                 (None, 2)            514         concatenate_7[0][0]              \n",
      "==================================================================================================\n",
      "Total params: 3,220,162\n",
      "Trainable params: 3,220,162\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "Train on 49401 samples, validate on 8000 samples\n",
      "Epoch 1/30\n",
      "49401/49401 [==============================] - 21s 431us/step - loss: 0.6779 - acc: 0.5771 - val_loss: 0.6872 - val_acc: 0.5642\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.68721, saving model to Sentence_Models_H5/siamese.h5\n",
      "Epoch 2/30\n",
      "19456/49401 [==========>...................] - ETA: 10s - loss: 0.6427 - acc: 0.6189"
     ]
    },
    {
     "ename": "ResourceExhaustedError",
     "evalue": "OOM when allocating tensor with shape[36,512,300] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n\t [[{{node training_5/Adam/gradients/lstm_7/TensorArrayUnstack/TensorArrayScatter/TensorArrayScatterV3_grad/TensorArrayGatherV3}}]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mResourceExhaustedError\u001b[0m                    Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-36-8c72beab4aa6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     69\u001b[0m history = model.fit(x=[x1_train, x2_train], y=y_train, batch_size=BATCH_SIZE, epochs=NUM_EPOCHS, verbose=1, callbacks=[earlyStopping, mcp_save], validation_data=([x1_test, x2_test], y_test),\n\u001b[1;32m     70\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 71\u001b[0;31m     \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     72\u001b[0m )\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1037\u001b[0m                                         \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1038\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1039\u001b[0;31m                                         validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1040\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1041\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[0;34m(model, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m    197\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 199\u001b[0;31m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    200\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2713\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2714\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2715\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2716\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2717\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2673\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2674\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2675\u001b[0;31m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2676\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1437\u001b[0m           ret = tf_session.TF_SessionRunCallable(\n\u001b[1;32m   1438\u001b[0m               \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1439\u001b[0;31m               run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1440\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1441\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/errors_impl.py\u001b[0m in \u001b[0;36m__exit__\u001b[0;34m(self, type_arg, value_arg, traceback_arg)\u001b[0m\n\u001b[1;32m    526\u001b[0m             \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    527\u001b[0m             \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc_api\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_Message\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 528\u001b[0;31m             c_api.TF_GetCode(self.status.status))\n\u001b[0m\u001b[1;32m    529\u001b[0m     \u001b[0;31m# Delete the underlying status object from memory otherwise it stays alive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    530\u001b[0m     \u001b[0;31m# as there is a reference to status from this from the traceback due to\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mResourceExhaustedError\u001b[0m: OOM when allocating tensor with shape[36,512,300] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n\t [[{{node training_5/Adam/gradients/lstm_7/TensorArrayUnstack/TensorArrayScatter/TensorArrayScatterV3_grad/TensorArrayGatherV3}}]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n"
     ]
    }
   ],
   "source": [
    "#Naive Siamese\n",
    "NUM_CLASSES=2 #boolean; 1 or 0\n",
    "\n",
    " #rough estimate\n",
    "\n",
    "MAX_SEQUENCE_LENGTH = 36 #how long one sentence is\n",
    "\n",
    " #this is arbitrary?\n",
    "\n",
    "NUM_LSTM_UNITS = 128 #output dimension\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "top_input = Input(\n",
    "    shape=(MAX_SEQUENCE_LENGTH, ),    #this is the first sentence\n",
    "    dtype='int32')\n",
    "\n",
    "bm_input = Input(\n",
    "    shape=(MAX_SEQUENCE_LENGTH, ),   #this is the second\n",
    "    dtype='int32')\n",
    "\n",
    "\n",
    "embedding_layer = Embedding(\n",
    "    MAX_NUM_WORDS, NUM_EMBEDDING_DIM, mask_zero=True)\n",
    "\n",
    "top_embedded = embedding_layer(\n",
    "    top_input)\n",
    "bm_embedded = embedding_layer(\n",
    "    bm_input)\n",
    "\n",
    "\n",
    "shared_lstm = LSTM(NUM_LSTM_UNITS)\n",
    "top_output = shared_lstm(top_embedded)\n",
    "bm_output = shared_lstm(bm_embedded)\n",
    "\n",
    "\n",
    "merged = concatenate(\n",
    "    [top_output, bm_output], \n",
    "    axis=-1)\n",
    "\n",
    "\n",
    "dense =  Dense(\n",
    "    units=NUM_CLASSES, \n",
    "    activation='sigmoid')\n",
    "predictions = dense(merged)\n",
    "\n",
    "\n",
    "\n",
    "model = Model(\n",
    "    inputs=[top_input, bm_input], \n",
    "    outputs=predictions)\n",
    "\n",
    "\n",
    "model.summary()\n",
    "\n",
    "model.compile(\n",
    "    optimizer='Adam',\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=['accuracy'])\n",
    "\n",
    "BATCH_SIZE = 512\n",
    "\n",
    "NUM_EPOCHS = 30\n",
    "\n",
    "earlyStopping = EarlyStopping(monitor='val_loss', patience=10, verbose=1, mode='min')\n",
    "mcp_save = ModelCheckpoint('Sentence_Models_H5/siamese.h5', verbose=1, save_best_only=True, monitor='val_loss', mode='min')\n",
    "\n",
    "history = model.fit(x=[x1_train, x2_train], y=y_train, batch_size=BATCH_SIZE, epochs=NUM_EPOCHS, verbose=1, callbacks=[earlyStopping, mcp_save], validation_data=([x1_test, x2_test], y_test),\n",
    "    \n",
    "    shuffle=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thu Jul 25 02:03:35 2019       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 418.56       Driver Version: 418.56       CUDA Version: 10.1     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  GeForce RTX 208...  On   | 00000000:D8:00.0 Off |                  N/A |\n",
      "| 28%   34C    P8    20W / 250W |  10827MiB / 10989MiB |      0%      Default |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                       GPU Memory |\n",
      "|  GPU       PID   Type   Process name                             Usage      |\n",
      "|=============================================================================|\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 36)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_2 (InputLayer)            (None, 36)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)         (None, 36, 300)      3000000     input_1[0][0]                    \n",
      "                                                                 input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_1 (Bidirectional) (None, 256)          439296      embedding_1[0][0]                \n",
      "                                                                 embedding_1[1][0]                \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 512)          0           bidirectional_1[0][0]            \n",
      "                                                                 bidirectional_1[1][0]            \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 2)            1026        concatenate_1[0][0]              \n",
      "==================================================================================================\n",
      "Total params: 3,440,322\n",
      "Trainable params: 3,440,322\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#BiLSTM?\n",
    "NUM_CLASSES=2 #boolean; 1 or 0\n",
    "\n",
    "#rough estimate\n",
    "\n",
    "MAX_SEQUENCE_LENGTH = 36 #how long one sentence is\n",
    "\n",
    "#this is arbitrary?\n",
    "\n",
    "NUM_LSTM_UNITS = 128 #output dimension\n",
    "\n",
    "\n",
    "NUM_EMBEDDING_DIM = 300\n",
    "\n",
    "top_input = Input(\n",
    "    shape=(MAX_SEQUENCE_LENGTH, ),    #this is the first sentence\n",
    "    dtype='int32')\n",
    "\n",
    "bm_input = Input(\n",
    "    shape=(MAX_SEQUENCE_LENGTH, ),   #this is the second\n",
    "    dtype='int32')\n",
    "\n",
    "\n",
    "embedding_layer = Embedding(\n",
    "    MAX_NUM_WORDS, NUM_EMBEDDING_DIM, mask_zero=True)\n",
    "\n",
    "\n",
    "top_embedded = embedding_layer(\n",
    "    top_input)\n",
    "bm_embedded = embedding_layer(\n",
    "    bm_input)\n",
    "\n",
    "\n",
    "shared_lstm = Bidirectional(LSTM(NUM_LSTM_UNITS))\n",
    "top_output = shared_lstm(top_embedded)\n",
    "bm_output = shared_lstm(bm_embedded)\n",
    "\n",
    "\n",
    "merged = concatenate(\n",
    "    [top_output, bm_output], \n",
    "    axis=-1)\n",
    "\n",
    "\n",
    "dense =  Dense(\n",
    "    units=NUM_CLASSES, \n",
    "    activation='sigmoid')\n",
    "predictions = dense(merged)\n",
    "\n",
    "\n",
    "\n",
    "model = Model(\n",
    "    inputs=[top_input, bm_input], \n",
    "    outputs=predictions)\n",
    "\n",
    "\n",
    "model.summary()\n",
    "\n",
    "model.compile(\n",
    "    optimizer='Adam',\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=['accuracy'])\n",
    "\n",
    "BATCH_SIZE = 512\n",
    "\n",
    "NUM_EPOCHS = 20\n",
    "\n",
    "earlyStopping = EarlyStopping(monitor='val_loss', patience=5, verbose=1, mode='min')\n",
    "mcp_save = ModelCheckpoint('BiLSTM_PAWS_Wiki_GLOVE.h5', verbose=1, save_best_only=True, monitor='val_loss', mode='min')\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'dot_2/ExpandDims:0' shape=(?, 1) dtype=float32>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dot(\n",
    "    [top_output, bm_output], \n",
    "    axes=1, normalize = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(49401, 36)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x1_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 36)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_2 (InputLayer)            (None, 36)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)         (None, 36, 256)      2560000     input_1[0][0]                    \n",
      "                                                                 input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_1 (Bidirectional) (None, 256)          394240      embedding_1[0][0]                \n",
      "                                                                 embedding_1[1][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dot_1 (Dot)                     (None, 1)            0           bidirectional_1[0][0]            \n",
      "                                                                 bidirectional_1[1][0]            \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 2)            4           dot_1[0][0]                      \n",
      "==================================================================================================\n",
      "Total params: 2,954,244\n",
      "Trainable params: 2,954,244\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#cosine similarity\n",
    "\n",
    "\n",
    "NUM_CLASSES=2 #boolean; 1 or 0\n",
    "\n",
    "#rough estimate\n",
    "\n",
    "MAX_SEQUENCE_LENGTH = 36 #how long one sentence is\n",
    "\n",
    "#this is arbitrary?\n",
    "\n",
    "NUM_LSTM_UNITS = 128 #output dimension\n",
    "\n",
    "\n",
    "NUM_EMBEDDING_DIM = 256\n",
    "\n",
    "top_input = Input(\n",
    "    shape=(MAX_SEQUENCE_LENGTH, ),    #this is the first sentence\n",
    "    dtype='int32')\n",
    "\n",
    "bm_input = Input(\n",
    "    shape=(MAX_SEQUENCE_LENGTH, ),   #this is the second\n",
    "    dtype='int32')\n",
    "\n",
    "\n",
    "embedding_layer = Embedding(\n",
    "    MAX_NUM_WORDS, NUM_EMBEDDING_DIM)\n",
    "\n",
    "top_embedded = embedding_layer(\n",
    "    top_input)\n",
    "bm_embedded = embedding_layer(\n",
    "    bm_input)\n",
    "\n",
    "\n",
    "shared_lstm = Bidirectional(LSTM(NUM_LSTM_UNITS))\n",
    "top_output = shared_lstm(top_embedded)\n",
    "bm_output = shared_lstm(bm_embedded)\n",
    "\n",
    "\n",
    "merged = dot(\n",
    "    [top_output, bm_output], \n",
    "    axes=1, normalize = True)\n",
    "\n",
    "\n",
    "dense =  Dense(\n",
    "    units=NUM_CLASSES, \n",
    "    activation='sigmoid')\n",
    "predictions = dense(merged)\n",
    "\n",
    "\n",
    "\n",
    "model = Model(\n",
    "    inputs=[top_input, bm_input], \n",
    "    outputs=predictions)\n",
    "\n",
    "\n",
    "model.summary()\n",
    "\n",
    "model.compile(\n",
    "    optimizer='Adam',\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=['accuracy'])\n",
    "\n",
    "BATCH_SIZE = 512\n",
    "\n",
    "NUM_EPOCHS = 20\n",
    "\n",
    "earlyStopping = EarlyStopping(monitor='val_loss', patience=5, verbose=1, mode='min')\n",
    "mcp_save = ModelCheckpoint('BiLSTM_PAWS_Wiki_cos_sim.h5', verbose=1, save_best_only=True, monitor='val_loss', mode='min')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fri Jul 19 08:50:17 2019       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 418.56       Driver Version: 418.56       CUDA Version: 10.1     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  GeForce RTX 208...  On   | 00000000:D8:00.0 Off |                  N/A |\n",
      "| 27%   31C    P8    20W / 250W |  10982MiB / 10989MiB |      0%      Default |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                       GPU Memory |\n",
      "|  GPU       PID   Type   Process name                             Usage      |\n",
      "|=============================================================================|\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Train on 49401 samples, validate on 8000 samples\n",
      "Epoch 1/20\n",
      "49401/49401 [==============================] - 24s 490us/step - loss: 0.6860 - acc: 0.5548 - val_loss: 0.6893 - val_acc: 0.5221\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.68929, saving model to BiLSTM_PAWS_Wiki_cos_sim.h5\n",
      "Epoch 2/20\n",
      "49401/49401 [==============================] - 22s 444us/step - loss: 0.6400 - acc: 0.7442 - val_loss: 0.6912 - val_acc: 0.5279\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 0.68929\n",
      "Epoch 3/20\n",
      "49401/49401 [==============================] - 22s 442us/step - loss: 0.6024 - acc: 0.8041 - val_loss: 0.6939 - val_acc: 0.5319\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.68929\n",
      "Epoch 4/20\n",
      "49401/49401 [==============================] - 22s 443us/step - loss: 0.5679 - acc: 0.8312 - val_loss: 0.6993 - val_acc: 0.5379\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.68929\n",
      "Epoch 5/20\n",
      "49401/49401 [==============================] - 22s 443us/step - loss: 0.5309 - acc: 0.8562 - val_loss: 0.7060 - val_acc: 0.5414\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.68929\n",
      "Epoch 6/20\n",
      "49401/49401 [==============================] - 22s 443us/step - loss: 0.4939 - acc: 0.8726 - val_loss: 0.7165 - val_acc: 0.5416\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.68929\n",
      "Epoch 00006: early stopping\n"
     ]
    }
   ],
   "source": [
    "        \n",
    "history = model.fit(x=[x1_train, x2_train], y=y_train, batch_size=BATCH_SIZE, epochs=NUM_EPOCHS, verbose=1, callbacks=[earlyStopping, mcp_save], validation_data=([x1_dev, x2_dev], y_dev),\n",
    "    \n",
    "    shuffle=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "Unable to open file (unable to open file: name = '../Sentence_Models_H5/siamese.h5', errno = 2, error message = 'No such file or directory', flags = 0, o_flags = 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-34-7216d82b31e0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"../Sentence_Models_H5/siamese.h5\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/engine/saving.py\u001b[0m in \u001b[0;36mload_model\u001b[0;34m(filepath, custom_objects, compile)\u001b[0m\n\u001b[1;32m    415\u001b[0m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    416\u001b[0m     \u001b[0mopened_new_file\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh5py\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGroup\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 417\u001b[0;31m     \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mh5dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    418\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    419\u001b[0m         \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_deserialize_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcustom_objects\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/utils/io_utils.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, path, mode)\u001b[0m\n\u001b[1;32m    184\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_is_file\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    185\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 186\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mh5py\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    187\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_is_file\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    188\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/h5py/_hl/files.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, name, mode, driver, libver, userblock_size, swmr, rdcc_nslots, rdcc_nbytes, rdcc_w0, track_order, **kwds)\u001b[0m\n\u001b[1;32m    392\u001b[0m                 fid = make_fid(name, mode, userblock_size,\n\u001b[1;32m    393\u001b[0m                                \u001b[0mfapl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfcpl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmake_fcpl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrack_order\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrack_order\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 394\u001b[0;31m                                swmr=swmr)\n\u001b[0m\u001b[1;32m    395\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    396\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mswmr_support\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/h5py/_hl/files.py\u001b[0m in \u001b[0;36mmake_fid\u001b[0;34m(name, mode, userblock_size, fapl, fcpl, swmr)\u001b[0m\n\u001b[1;32m    168\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mswmr\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mswmr_support\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    169\u001b[0m             \u001b[0mflags\u001b[0m \u001b[0;34m|=\u001b[0m \u001b[0mh5f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mACC_SWMR_READ\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 170\u001b[0;31m         \u001b[0mfid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mh5f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfapl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfapl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    171\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'r+'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    172\u001b[0m         \u001b[0mfid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mh5f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh5f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mACC_RDWR\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfapl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfapl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mh5py/_objects.pyx\u001b[0m in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mh5py/_objects.pyx\u001b[0m in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mh5py/h5f.pyx\u001b[0m in \u001b[0;36mh5py.h5f.open\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mOSError\u001b[0m: Unable to open file (unable to open file: name = '../Sentence_Models_H5/siamese.h5', errno = 2, error message = 'No such file or directory', flags = 0, o_flags = 0)"
     ]
    }
   ],
   "source": [
    "model = load_model(\"../Sentence_Models_H5/siamese.h5\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicts = model.predict(\n",
    "    [x1_train, x2_train])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_test = model.predict([x1_test, x2_test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.507625\n"
     ]
    }
   ],
   "source": [
    "sen1 = test.sentence1\n",
    "sen2=test.sentence2\n",
    "\n",
    "error = []\n",
    "errr1 = []\n",
    "errr2 = []\n",
    "real_cat = []\n",
    "\n",
    "\n",
    "for x in range(len(predict_test)):\n",
    "    if (np.argmax(predict_test[x])!=(test.label[x])):\n",
    "        error = np.append(error, x)\n",
    "        errr1=np.append(errr1, sen1[x])\n",
    "        errr2=np.append(errr2, sen2[x])\n",
    "        real_cat = np.append(real_cat, test.label[x])\n",
    "print(1-(len(real_cat)/len(x1_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'error' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-30-df87d7f78df0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mtest_error_analysis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtest_error_analysis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_column\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"id\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merror\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"sent1\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrr1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"sent2\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrr2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"classification\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreal_cat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mtest_error_analysis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'error' is not defined"
     ]
    }
   ],
   "source": [
    "test_error_analysis = Table()\n",
    "\n",
    "\n",
    "test_error_analysis=test_error_analysis.with_column(\"id\", error, \"sent1\", errr1, \"sent2\", errr2, \"classification\", real_cat)\n",
    "test_error_analysis.show(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "49401\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "print(len(predicts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5772150361328718\n"
     ]
    }
   ],
   "source": [
    "real1=train.sentence1\n",
    "real2=train.sentence2\n",
    "realCat=train.label\n",
    "no = []\n",
    "err1 = []\n",
    "err2 = []\n",
    "wrongCat = []\n",
    "\n",
    "\n",
    "for x in range(len(predicts)):\n",
    "    if ((np.argmax(predicts[x])!=(train.label[x]))):\n",
    "        no=np.append(no, x)\n",
    "        err1=np.append(err1, real1[x])\n",
    "        err2=np.append(err2, real2[x])\n",
    "        wrongCat = np.append(wrongCat, realCat[x])\n",
    "print(1-(len(wrongCat)/len(x1_train)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "    <thead>\n",
       "        <tr>\n",
       "            <th>id</th> <th>sent1</th> <th>sent2</th> <th>classification</th>\n",
       "        </tr>\n",
       "    </thead>\n",
       "    <tbody>\n",
       "        <tr>\n",
       "            <td>1   </td> <td>The NBA season of 1975 -- 76 was the 30th season of the  ...</td> <td>The 1975 -- 76 season of the National Basketball Associa ...</td> <td>1             </td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>2   </td> <td>There are also specific discussions , public profile deb ...</td> <td>There are also public discussions , profile specific dis ...</td> <td>0             </td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>3   </td> <td>When comparable rates of flow can be maintained , the re ...</td> <td>The results are high when comparable flow rates can be m ...</td> <td>1             </td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>4   </td> <td>It is the seat of Zerendi District in Akmola Region .       </td> <td>It is the seat of the district of Zerendi in Akmola region .</td> <td>1             </td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>5   </td> <td>William Henry Henry Harman was born on 17 February 1828  ...</td> <td>William Henry Harman was born in Waynesboro , Virginia o ...</td> <td>1             </td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>7   </td> <td>With a discrete amount of probabilities Formula 1 with t ...</td> <td>Given a discrete set of probabilities formula _ 1 with t ...</td> <td>1             </td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>12  </td> <td>The city sits at the confluence of the Snake River with  ...</td> <td>The city lies at the confluence of the Snake River and t ...</td> <td>1             </td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>14  </td> <td>The Austrian school assumes that the subjective choices  ...</td> <td>The Austrian school assumes that the subjective choices  ...</td> <td>0             </td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>15  </td> <td>Werder 's forces invested Belfort and reached the city o ...</td> <td>Werder 's troops invested Belfort and reached the city o ...</td> <td>1             </td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>17  </td> <td>The first five weapons were delivered in the first half  ...</td> <td>The first five weapons were delivered in the first half  ...</td> <td>1             </td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>19  </td> <td>The friendship between him and Duncan ended at a club me ...</td> <td>The friendship between him and Duncan ended in 1951 at a ...</td> <td>1             </td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>22  </td> <td>Shaffer Creek is a tributary of the Raystown Branch Juni ...</td> <td>Shaffer Creek is an tributary of Brush Creek ( Raystown  ...</td> <td>1             </td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>24  </td> <td>Briggs later met Briggs at the 1967 Monterey Pop Festiva ...</td> <td>Briggs met Briggs later at the Monterey Pop Festival of  ...</td> <td>1             </td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>26  </td> <td>The female lead role was played by Cortez in `` Ali Baba ...</td> <td>Cortez played the female lead in `` Ali Baba and the Sac ...</td> <td>1             </td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>27  </td> <td>She worked and lived in Stuttgart , Berlin ( Germany ) a ...</td> <td>She worked and lived in Germany ( Stuttgart , Berlin ) a ...</td> <td>1             </td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>29  </td> <td>The Little Jocko River flows across the Saint Lawrence R ...</td> <td>The Little Jocko River flows via the Saint Lawrence Rive ...</td> <td>1             </td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>30  </td> <td>In 1951 , he died and retired in 1956 .                     </td> <td>He died in 1951 and retired in 1956 .                       </td> <td>1             </td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>31  </td> <td>WORHP , also referred to as eNLP ( European NLP Solver ) ...</td> <td>WORHP , also referred to as eNLP ( European NLP solver ) ...</td> <td>1             </td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>33  </td> <td>The leaves are generally 1.5-4 mm long and 0.2-0.7 mm wide .</td> <td>The leaves are usually 1.5-4 mm long and 0.2-0.7 mm wide .  </td> <td>1             </td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>35  </td> <td>In addition to Michael Boddicker , and Patrick Moraz , t ...</td> <td>In addition to Diana Hubbard , the album contains musica ...</td> <td>0             </td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>37  </td> <td>Quintus Caecilius Metellus Macedonicus was the second so ...</td> <td>Quintus Caecilius Metellus Macedonicus was the second so ...</td> <td>1             </td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>39  </td> <td>Here we view pseudo-differential operators as a generali ...</td> <td>We consider pseudo-differential operators here as a gene ...</td> <td>1             </td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>42  </td> <td>Based on the city of Baltimore , only mentioned , never  ...</td> <td>Based on the city of Baltimore , only mentioned , has ne ...</td> <td>1             </td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>43  </td> <td>Debbie Downer is a name of a fictional `` Saturday Night ...</td> <td>Debbie Downer is a name of a fictional `` Saturday Night ...</td> <td>1             </td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>44  </td> <td>Taieri is a former parliamentary electorate in the Otago ...</td> <td>Taieri is a former parliamentary electorate in the New Z ...</td> <td>1             </td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>46  </td> <td>Brockton is approximately 25 miles northeast of Providen ...</td> <td>Brockton is located approximately 25 miles northeast of  ...</td> <td>1             </td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>47  </td> <td>Seb Janiak is a French photographer and video director o ...</td> <td>Seb Janiak is the French photographer and video director ...</td> <td>1             </td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>50  </td> <td>Ashley was born on 1 November 1986 and is a contemporary ...</td> <td>Born on November 1 , 1986 , Ashley is a contemporary dan ...</td> <td>1             </td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>52  </td> <td>Dorothy Kate Richmond , Frances Hodgkins , and Gwen Knig ...</td> <td>Dorothy Kate Richmond , Frances Hodgkins and Gwen Knight ...</td> <td>1             </td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>53  </td> <td>Hastings Ndlovu was buried with Hector Pieterson at Aval ...</td> <td>Hastings Ndlovu , together with Hector Pieterson , was b ...</td> <td>1             </td>\n",
       "        </tr>\n",
       "    </tbody>\n",
       "</table>\n",
       "<p>... (20856 rows omitted)</p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_error_analysis = Table()\n",
    "\n",
    "\n",
    "train_error_analysis=train_error_analysis.with_column(\"id\", no, \"sent1\", err1, \"sent2\", err2, \"classification\", wrongCat)\n",
    "train_error_analysis.show(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "    <thead>\n",
       "        <tr>\n",
       "            <th>id</th> <th>sent1</th> <th>sent2</th> <th>classification</th>\n",
       "        </tr>\n",
       "    </thead>\n",
       "    <tbody>\n",
       "        <tr>\n",
       "            <td>1   </td> <td>The NBA season of 1975 -- 76 was the 30th season of the  ...</td> <td>The 1975 -- 76 season of the National Basketball Associa ...</td> <td>1             </td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>3   </td> <td>When comparable rates of flow can be maintained , the re ...</td> <td>The results are high when comparable flow rates can be m ...</td> <td>1             </td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>4   </td> <td>It is the seat of Zerendi District in Akmola Region .       </td> <td>It is the seat of the district of Zerendi in Akmola region .</td> <td>1             </td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>5   </td> <td>William Henry Henry Harman was born on 17 February 1828  ...</td> <td>William Henry Harman was born in Waynesboro , Virginia o ...</td> <td>1             </td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>7   </td> <td>With a discrete amount of probabilities Formula 1 with t ...</td> <td>Given a discrete set of probabilities formula _ 1 with t ...</td> <td>1             </td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>12  </td> <td>The city sits at the confluence of the Snake River with  ...</td> <td>The city lies at the confluence of the Snake River and t ...</td> <td>1             </td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>15  </td> <td>Werder 's forces invested Belfort and reached the city o ...</td> <td>Werder 's troops invested Belfort and reached the city o ...</td> <td>1             </td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>17  </td> <td>The first five weapons were delivered in the first half  ...</td> <td>The first five weapons were delivered in the first half  ...</td> <td>1             </td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>19  </td> <td>The friendship between him and Duncan ended at a club me ...</td> <td>The friendship between him and Duncan ended in 1951 at a ...</td> <td>1             </td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>22  </td> <td>Shaffer Creek is a tributary of the Raystown Branch Juni ...</td> <td>Shaffer Creek is an tributary of Brush Creek ( Raystown  ...</td> <td>1             </td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>24  </td> <td>Briggs later met Briggs at the 1967 Monterey Pop Festiva ...</td> <td>Briggs met Briggs later at the Monterey Pop Festival of  ...</td> <td>1             </td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>26  </td> <td>The female lead role was played by Cortez in `` Ali Baba ...</td> <td>Cortez played the female lead in `` Ali Baba and the Sac ...</td> <td>1             </td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>27  </td> <td>She worked and lived in Stuttgart , Berlin ( Germany ) a ...</td> <td>She worked and lived in Germany ( Stuttgart , Berlin ) a ...</td> <td>1             </td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>29  </td> <td>The Little Jocko River flows across the Saint Lawrence R ...</td> <td>The Little Jocko River flows via the Saint Lawrence Rive ...</td> <td>1             </td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>30  </td> <td>In 1951 , he died and retired in 1956 .                     </td> <td>He died in 1951 and retired in 1956 .                       </td> <td>1             </td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>31  </td> <td>WORHP , also referred to as eNLP ( European NLP Solver ) ...</td> <td>WORHP , also referred to as eNLP ( European NLP solver ) ...</td> <td>1             </td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>33  </td> <td>The leaves are generally 1.5-4 mm long and 0.2-0.7 mm wide .</td> <td>The leaves are usually 1.5-4 mm long and 0.2-0.7 mm wide .  </td> <td>1             </td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>37  </td> <td>Quintus Caecilius Metellus Macedonicus was the second so ...</td> <td>Quintus Caecilius Metellus Macedonicus was the second so ...</td> <td>1             </td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>39  </td> <td>Here we view pseudo-differential operators as a generali ...</td> <td>We consider pseudo-differential operators here as a gene ...</td> <td>1             </td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>42  </td> <td>Based on the city of Baltimore , only mentioned , never  ...</td> <td>Based on the city of Baltimore , only mentioned , has ne ...</td> <td>1             </td>\n",
       "        </tr>\n",
       "    </tbody>\n",
       "</table>\n",
       "<p>... (19326 rows omitted)</p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "false_negative = train_error_analysis.where(\"classification\", are.equal_to(1))\n",
    "false_negative.show(20)\n",
    "\n",
    "#\n",
    "#formula\n",
    "#more of prepositional phrases/verbs instead of the changing of order that is causing the misclassification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "    <thead>\n",
       "        <tr>\n",
       "            <th>id</th> <th>sent1</th> <th>sent2</th> <th>classification</th>\n",
       "        </tr>\n",
       "    </thead>\n",
       "    <tbody>\n",
       "        <tr>\n",
       "            <td>2   </td> <td>There are also specific discussions , public profile deb ...</td> <td>There are also public discussions , profile specific dis ...</td> <td>0             </td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>14  </td> <td>The Austrian school assumes that the subjective choices  ...</td> <td>The Austrian school assumes that the subjective choices  ...</td> <td>0             </td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>35  </td> <td>In addition to Michael Boddicker , and Patrick Moraz , t ...</td> <td>In addition to Diana Hubbard , the album contains musica ...</td> <td>0             </td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>135 </td> <td>His parents are Ernie Gordon , who grew up with Yankees  ...</td> <td>His parents are Wendy Abrahamsen , who grew up a Yankees ...</td> <td>0             </td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>142 </td> <td>Fish species of the parks rivers and lakes are Sperchios ...</td> <td>Fish species of the parks rivers and lakes are Sperchios ...</td> <td>0             </td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>155 </td> <td>Neyab ( also Romanized as Neyab ) is a village in Esfara ...</td> <td>Neyab ( also romanized as Neyab ) is a village in the di ...</td> <td>0             </td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>196 </td> <td>Delaware County is a city in and the county town of Dela ...</td> <td>Delaware is a city in and the county seat of Delaware Co ...</td> <td>0             </td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>247 </td> <td>Cornelis van Cleve painted primarily mythological painti ...</td> <td>Cornelis van Cleve painted mainly religious paintings an ...</td> <td>0             </td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>248 </td> <td>Their skin also secretes chemicals that are poisonous an ...</td> <td>Also , their skin secretes chemicals that are distastefu ...</td> <td>0             </td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>251 </td> <td>In rats , it also inhibits the central , if not peripher ...</td> <td>It also inhibits the peripheral , if not central , secre ...</td> <td>0             </td>\n",
       "        </tr>\n",
       "    </tbody>\n",
       "</table>\n",
       "<p>... (1530 rows omitted)</p>"
      ],
      "text/plain": [
       "id   | sent1                                                        | sent2                                                        | classification\n",
       "2    | There are also specific discussions , public profile deb ... | There are also public discussions , profile specific dis ... | 0\n",
       "14   | The Austrian school assumes that the subjective choices  ... | The Austrian school assumes that the subjective choices  ... | 0\n",
       "35   | In addition to Michael Boddicker , and Patrick Moraz , t ... | In addition to Diana Hubbard , the album contains musica ... | 0\n",
       "135  | His parents are Ernie Gordon , who grew up with Yankees  ... | His parents are Wendy Abrahamsen , who grew up a Yankees ... | 0\n",
       "142  | Fish species of the parks rivers and lakes are Sperchios ... | Fish species of the parks rivers and lakes are Sperchios ... | 0\n",
       "155  | Neyab ( also Romanized as Neyab ) is a village in Esfara ... | Neyab ( also romanized as Neyab ) is a village in the di ... | 0\n",
       "196  | Delaware County is a city in and the county town of Dela ... | Delaware is a city in and the county seat of Delaware Co ... | 0\n",
       "247  | Cornelis van Cleve painted primarily mythological painti ... | Cornelis van Cleve painted mainly religious paintings an ... | 0\n",
       "248  | Their skin also secretes chemicals that are poisonous an ... | Also , their skin secretes chemicals that are distastefu ... | 0\n",
       "251  | In rats , it also inhibits the central , if not peripher ... | It also inhibits the peripheral , if not central , secre ... | 0\n",
       "... (1530 rows omitted)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "false_positive = train_error_analysis.where(\"classification\", are.equal_to(0))\n",
    "false_positive\n",
    "#is/has, address scrambling\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\"when...\", split into two sentences, when two sentences are equal,  \n",
    "#mostly false positives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#embedding layer output dimension is arbitrary?\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I', 'like,', 'hue']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sanitize(\"I like, hue\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tue Jun 25 03:32:57 2019       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 418.56       Driver Version: 418.56       CUDA Version: 10.1     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  GeForce GTX 1080    Off  | 00000000:02:00.0 Off |                  N/A |\n",
      "|  0%   47C    P8     8W / 200W |   7772MiB /  8118MiB |      0%      Default |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                       GPU Memory |\n",
      "|  GPU       PID   Type   Process name                             Usage      |\n",
      "|=============================================================================|\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#possible optimization strats:\n",
    "\n",
    "#tokenization without punctuation aka modify the dictionary\n",
    "#tweak MAX_SEQUENCE_LENGTH \n",
    "#masking\n",
    "#NUM_EMBEDDING_DIMENSION\n",
    "#\"optimizer\" in the compile function\n",
    "#loss function\n",
    "#a different activation function?\n",
    "#sth other than word level tokenization?\n",
    "#train on other databases?\n",
    "#pretrained word embedding\n",
    "#check errors on the test set?\n",
    "#model attention seq2seq\n",
    "#GLOVE dimension modification\n",
    "#more sanitization?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model adaptation: https://leemeng.tw/shortest-path-to-the-nlp-world-a-gentle-guide-of-natural-language-processing-and-deep-learning-for-everyone.html#%E6%B7%B1%E5%BA%A6%E5%AD%B8%E7%BF%92-3-%E6%AD%A5%E9%A9%9F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://paperswithcode.com/paper/multi-task-deep-neural-networks-for-natural\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#attempts: with self trained word embedder, glove embedder, zero masking, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We randomly select 5,000 paraphrases and 5,000 non-paraphrases as the dev set, and sample\n",
    "#another 5,000 paraphrases and 5,000 non-paraphrases as the\n",
    "#test set. We keep the remaining instances as the training set 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
