{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib\n",
    "import argparse\n",
    "import random\n",
    "import numpy as np\n",
    "import csv\n",
    "import pandas as pd\n",
    "from datascience import *\n",
    "import tensorflow as tf\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from keras.models import Model\n",
    "from keras.models import Sequential, load_model\n",
    "from keras.layers import *\n",
    "from keras.optimizers import Adam\n",
    "from keras import Input\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.utils import plot_model\n",
    "import keras\n",
    "from keras.backend.tensorflow_backend import set_session\n",
    "#config = tf.ConfigProto()\n",
    "# config.gpu_options.allocator_type = 'BFC' #A \"Best-fit with coalescing\" algorithm, simplified from a version of dlmalloc.\n",
    "# config.gpu_options.per_process_gpu_memory_fraction = 0.7\n",
    "#config.gpu_options.allow_growth =True\n",
    "\n",
    "#set_session(tf.Session(config=config)) \n",
    "import re\n",
    "os.environ['PYTHONHASHSEED'] = '0'\n",
    "random.seed(1)\n",
    "np.random.seed(1)\n",
    "tf.set_random_seed(1)\n",
    "from keras.callbacks import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "print(tf.test.is_gpu_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def sanitize(string):\n",
    "    words = string.split(' ')\n",
    "    return words\n",
    "\n",
    "\n",
    "def converter(x):\n",
    "    try:\n",
    "        return ' '.join([x.lower() for x in str(x).split() if x not in stop_words])\n",
    "    except AttributeError:\n",
    "        return None  # or some other value\n",
    "    \n",
    "def whole(x):\n",
    "    return int(round(x))\n",
    "\n",
    "whole(1.0)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>qid1</th>\n",
       "      <th>qid2</th>\n",
       "      <th>question1</th>\n",
       "      <th>question2</th>\n",
       "      <th>is_duplicate</th>\n",
       "      <th>title1_tokenized</th>\n",
       "      <th>title2_tokenized</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>What is the step by step guide to invest in sh...</td>\n",
       "      <td>What is the step by step guide to invest in sh...</td>\n",
       "      <td>0</td>\n",
       "      <td>[What, is, the, step, by, step, guide, to, inv...</td>\n",
       "      <td>[What, is, the, step, by, step, guide, to, inv...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>What is the story of Kohinoor (Koh-i-Noor) Dia...</td>\n",
       "      <td>What would happen if the Indian government sto...</td>\n",
       "      <td>0</td>\n",
       "      <td>[What, is, the, story, of, Kohinoor, (Koh-i-No...</td>\n",
       "      <td>[What, would, happen, if, the, Indian, governm...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>How can I increase the speed of my internet co...</td>\n",
       "      <td>How can Internet speed be increased by hacking...</td>\n",
       "      <td>0</td>\n",
       "      <td>[How, can, I, increase, the, speed, of, my, in...</td>\n",
       "      <td>[How, can, Internet, speed, be, increased, by,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>8</td>\n",
       "      <td>Why am I mentally very lonely? How can I solve...</td>\n",
       "      <td>Find the remainder when [math]23^{24}[/math] i...</td>\n",
       "      <td>0</td>\n",
       "      <td>[Why, am, I, mentally, very, lonely?, How, can...</td>\n",
       "      <td>[Find, the, remainder, when, [math]23^{24}[/ma...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>9</td>\n",
       "      <td>10</td>\n",
       "      <td>Which one dissolve in water quikly sugar, salt...</td>\n",
       "      <td>Which fish would survive in salt water?</td>\n",
       "      <td>0</td>\n",
       "      <td>[Which, one, dissolve, in, water, quikly, suga...</td>\n",
       "      <td>[Which, fish, would, survive, in, salt, water?]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id  qid1  qid2                                          question1  \\\n",
       "0   0     1     2  What is the step by step guide to invest in sh...   \n",
       "1   1     3     4  What is the story of Kohinoor (Koh-i-Noor) Dia...   \n",
       "2   2     5     6  How can I increase the speed of my internet co...   \n",
       "3   3     7     8  Why am I mentally very lonely? How can I solve...   \n",
       "4   4     9    10  Which one dissolve in water quikly sugar, salt...   \n",
       "\n",
       "                                           question2  is_duplicate  \\\n",
       "0  What is the step by step guide to invest in sh...             0   \n",
       "1  What would happen if the Indian government sto...             0   \n",
       "2  How can Internet speed be increased by hacking...             0   \n",
       "3  Find the remainder when [math]23^{24}[/math] i...             0   \n",
       "4            Which fish would survive in salt water?             0   \n",
       "\n",
       "                                    title1_tokenized  \\\n",
       "0  [What, is, the, step, by, step, guide, to, inv...   \n",
       "1  [What, is, the, story, of, Kohinoor, (Koh-i-No...   \n",
       "2  [How, can, I, increase, the, speed, of, my, in...   \n",
       "3  [Why, am, I, mentally, very, lonely?, How, can...   \n",
       "4  [Which, one, dissolve, in, water, quikly, suga...   \n",
       "\n",
       "                                    title2_tokenized  \n",
       "0  [What, is, the, step, by, step, guide, to, inv...  \n",
       "1  [What, would, happen, if, the, Indian, governm...  \n",
       "2  [How, can, Internet, speed, be, increased, by,...  \n",
       "3  [Find, the, remainder, when, [math]23^{24}[/ma...  \n",
       "4    [Which, fish, would, survive, in, salt, water?]  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#read in the csv files \n",
    "\n",
    "quora = pd.read_csv(\"quora_duplicate_questions (1).tsv\", sep='\\t')\n",
    "\n",
    "\n",
    "quora['question1']=quora['question1'].fillna(\"\")\n",
    "quora['question2']=quora['question2'].fillna(\"\")\n",
    "\n",
    "\n",
    "quora['title1_tokenized'] = \\\n",
    "    quora.loc[:, 'question1'] \\\n",
    "         .apply(sanitize)\n",
    "quora['title2_tokenized'] = \\\n",
    "    quora.loc[:, 'question2'] \\\n",
    "         .apply(sanitize)\n",
    "\n",
    "\n",
    "\n",
    "quora.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "    <thead>\n",
       "        <tr>\n",
       "            <th>id</th> <th>qid1</th> <th>qid2</th> <th>question1</th> <th>question2</th> <th>is_duplicate</th>\n",
       "        </tr>\n",
       "    </thead>\n",
       "    <tbody>\n",
       "        <tr>\n",
       "            <td>0   </td> <td>1   </td> <td>2   </td> <td>What is the step by step guide to invest in share market ...</td> <td>What is the step by step guide to invest in share market?   </td> <td>0           </td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>1   </td> <td>3   </td> <td>4   </td> <td>What is the story of Kohinoor (Koh-i-Noor) Diamond?         </td> <td>What would happen if the Indian government stole the Koh ...</td> <td>0           </td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>2   </td> <td>5   </td> <td>6   </td> <td>How can I increase the speed of my internet connection w ...</td> <td>How can Internet speed be increased by hacking through DNS? </td> <td>0           </td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>3   </td> <td>7   </td> <td>8   </td> <td>Why am I mentally very lonely? How can I solve it?          </td> <td>Find the remainder when [math]23^{24}[/math] is divided  ...</td> <td>0           </td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>4   </td> <td>9   </td> <td>10  </td> <td>Which one dissolve in water quikly sugar, salt, methane  ...</td> <td>Which fish would survive in salt water?                     </td> <td>0           </td>\n",
       "        </tr>\n",
       "    </tbody>\n",
       "</table>\n",
       "<p>... (404285 rows omitted)</p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "Tb = Table()\n",
    "list1 = []\n",
    "list2 = []\n",
    "\n",
    "with open(\"quora_duplicate_questions (1).tsv\", 'r', encoding='utf-8') as fd:\n",
    "    rd = csv.reader(fd, delimiter=\"\\t\", quotechar='\"')\n",
    "    for row in rd:\n",
    "        \n",
    "        list1.append(row) #pairs start at index 1, with id, setence1, sentence2, label, respectively.\n",
    "     \n",
    "    for y in range(6):\n",
    "        temp=[]\n",
    "        for x in range(1, len(list1)):\n",
    "            temp.append(list1[x][y])\n",
    "            \n",
    "        list2.append(temp)\n",
    "\n",
    "    for ele in range(len(list2)):\n",
    "        Tb=Tb.with_columns(list1[0][ele], list2[ele])\n",
    "\n",
    "\n",
    "Tb.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "    <thead>\n",
       "        <tr>\n",
       "            <th>LHS</th> <th>PHRASE</th> <th>PARAPHRASE</th> <th>(FEATURE=VALUE)</th> <th>ALIGNMENT</th> <th>ENTAILMENT</th>\n",
       "        </tr>\n",
       "    </thead>\n",
       "    <tbody>\n",
       "        <tr>\n",
       "            <td>[VP/NNP] </td> <td> proposed by the president of the  </td> <td> proposed by the chairman of the   </td> <td> PPDB2.0Score=6.13443 PPDB1.0Score=5.794750 -logp(LHS|e1 ...</td> <td> 0-0 1-1 2-2 3-3 4-4 5-5 </td> <td> Equivalence      </td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>[VP/NNP] </td> <td> proposed by the chairman of the   </td> <td> proposed by the president of the  </td> <td> PPDB2.0Score=6.11747 PPDB1.0Score=5.794750 -logp(LHS|e1 ...</td> <td> 0-0 1-1 2-2 3-3 4-4 5-5 </td> <td> Equivalence      </td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>[VP]     </td> <td> referred to in this report        </td> <td> referred to in the present report </td> <td> PPDB2.0Score=6.04785 PPDB1.0Score=4.678690 -logp(LHS|e1 ...</td> <td> 0-0 1-1 2-2 3-3 3-4 4-5 </td> <td> Equivalence      </td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>[VP/NNP] </td> <td> addressed to the president of the </td> <td> addressed to the chairman of the  </td> <td> PPDB2.0Score=6.03040 PPDB1.0Score=1.919110 -logp(LHS|e1 ...</td> <td> 0-0 1-1 2-2 3-3 4-4 5-5 </td> <td> ReverseEntailment</td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>[VP/NNP] </td> <td> addressed to the chairman of the  </td> <td> addressed to the president of the </td> <td> PPDB2.0Score=6.01345 PPDB1.0Score=1.919110 -logp(LHS|e1 ...</td> <td> 0-0 1-1 2-2 3-3 4-4 5-5 </td> <td> ForwardEntailment</td>\n",
       "        </tr>\n",
       "    </tbody>\n",
       "</table>\n",
       "<p>... (1530807 rows omitted)</p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ppdb_Tb = Table()\n",
    "list1 = []\n",
    "list2 = []\n",
    "\n",
    "with open(\"dataset/ppdb/sanitized.csv\") as fd:\n",
    "    rd = csv.reader(fd)\n",
    "    for row in rd:\n",
    "        \n",
    "        list1.append(row) \n",
    "     \n",
    "    for y in range(6):\n",
    "        temp=[]\n",
    "        for x in range(1, len(list1)):\n",
    "            temp.append(list1[x][y])\n",
    "            \n",
    "        list2.append(temp)\n",
    "\n",
    "    for ele in range(len(list2)):\n",
    "        ppdb_Tb=ppdb_Tb.with_columns(list1[0][ele], list2[ele])\n",
    "\n",
    "\n",
    "ppdb_Tb.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "stop_words=(stopwords.words('english'))\n",
    "stop_words=np.append(stop_words, \"i'm\")\n",
    "print(stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words_punct=np.append(stop_words, \"?\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from scipy import spatial\n",
    "\n",
    "def cos_sim(vec1, vec2):\n",
    "    return (1 - spatial.distance.cosine(vec1, vec2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import difflib\n",
    "\n",
    "case_a = 'i like apple'\n",
    "case_b = 'i like pie'\n",
    "\n",
    "output_list = [li for li in difflib.ndiff(case_a, case_b) if li[0] != ' ']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import deque"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def diff(sen1, sen2):   \n",
    "    a=[]\n",
    "    b=[]\n",
    "    c=[]\n",
    "    wordlist1 = sen1.split()\n",
    "    wordlist2 = sen2.split()   \n",
    "    for i in wordlist1:\n",
    "        if i.lower() in stop_words:  #omit stop words\n",
    "            continue    \n",
    "        if i not in wordlist2:\n",
    "            a=np.append(a, i)\n",
    "        else:\n",
    "            c=np.append(c, i)\n",
    "    for j in wordlist2:\n",
    "        if j.lower() in stop_words:   #omit stop words\n",
    "            continue\n",
    "        if j not in wordlist1:\n",
    "            b=np.append(b, j)  \n",
    "    return np.array(a),np.array(b),np.array(c)       \n",
    "\n",
    "\n",
    "def seqDiff(sen1,sen2,a,b):      #include length 1\n",
    "    #wordlist1 = sen1.split()\n",
    "    #wordlist2 = sen2.split()\n",
    "    sen1=[x.lower() for x in sen1]\n",
    "    sen2=[x.lower() for x in sen2]\n",
    "    \n",
    "    \n",
    "    if len(sen1)==1:\n",
    "                \n",
    "        if sen1[0] in sen2 or sen1[0] in stop_words:\n",
    "            if len(a)>0:\n",
    "                b.append(a)\n",
    "                \n",
    "            return b\n",
    "        \n",
    "        else:\n",
    "            a=np.append(a,sen1[0])\n",
    "            b.append(a)\n",
    "                \n",
    "            return b\n",
    "    \n",
    "    elif sen1[0] in sen2:\n",
    "        \n",
    "        if len(a)>0:\n",
    "            b.append(a)\n",
    "        a=[]        \n",
    "        return seqDiff(sen1[1:], sen2,a,b)\n",
    "        \n",
    "    elif (sen1[0] not in sen2):\n",
    "        \n",
    "        if sen1[0] not in stop_words:\n",
    "            a=np.append(a,sen1[0])\n",
    "        \n",
    "        elif len(a)>0: \n",
    "            b.append(a)\n",
    "            a=[]\n",
    "            \n",
    "        return seqDiff(sen1[1:], sen2,a,b)\n",
    "    \n",
    "    \n",
    "    \n",
    "def seqDiff2(sen1,sen2,a,b):     # inlude length 1\n",
    "    sen1=[x.lower() for x in sen1]\n",
    "    sen2=[x.lower() for x in sen2]\n",
    "    \n",
    "    if len(sen1)==1:\n",
    "                \n",
    "        if sen1[0] in sen2:\n",
    "            if len(a)>0:\n",
    "                b.append(a)\n",
    "                \n",
    "            return b\n",
    "        \n",
    "        else:\n",
    "            a=np.append(a,sen1[0])\n",
    "            b.append(a)\n",
    "                \n",
    "            return b\n",
    "    \n",
    "    elif sen1[0] in sen2:\n",
    "        if len(a)>0:\n",
    "            b.append(a)\n",
    "        a=[]        \n",
    "        return seqDiff2(sen1[1:], sen2,a,b)\n",
    "        \n",
    "    elif (sen1[0] not in sen2):\n",
    "        \n",
    "        a=np.append(a,sen1[0])\n",
    "        \n",
    "                   \n",
    "        return seqDiff2(sen1[1:], sen2,a,b)\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g=\"How can I be a good geologist?\" \n",
    "hue=\"What should I do to be a great geologist\"\n",
    "g=g.split()\n",
    "hue=hue.split()\n",
    "g=[x.lower() for x in g]\n",
    "hue=[x.lower() for x in hue]\n",
    "deck=deque()\n",
    "temp=seqDiff2(g,hue,[],deck)\n",
    "temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "    <thead>\n",
       "        <tr>\n",
       "            <th>id</th> <th>qid1</th> <th>qid2</th> <th>question1</th> <th>question2</th> <th>is_duplicate</th>\n",
       "        </tr>\n",
       "    </thead>\n",
       "    <tbody>\n",
       "        <tr>\n",
       "            <td>5   </td> <td>11  </td> <td>12  </td> <td>Astrology: I am a Capricorn Sun Cap moon and cap rising. ...</td> <td>I'm a triple Capricorn (Sun, Moon and ascendant in Capri ...</td> <td>1           </td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>7   </td> <td>15  </td> <td>16  </td> <td>How can I be a good geologist?                              </td> <td>What should I do to be a great geologist?                   </td> <td>1           </td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>11  </td> <td>23  </td> <td>24  </td> <td>How do I read and find my YouTube comments?                 </td> <td>How can I see all my Youtube comments?                      </td> <td>1           </td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>12  </td> <td>25  </td> <td>26  </td> <td>What can make Physics easy to learn?                        </td> <td>How can you make physics easy to learn?                     </td> <td>1           </td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>13  </td> <td>27  </td> <td>28  </td> <td>What was your first sexual experience like?                 </td> <td>What was your first sexual experience?                      </td> <td>1           </td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>15  </td> <td>31  </td> <td>32  </td> <td>What would a Trump presidency mean for current internati ...</td> <td>How will a Trump presidency affect the students presentl ...</td> <td>1           </td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>16  </td> <td>33  </td> <td>34  </td> <td>What does manipulation mean?                                </td> <td>What does manipulation means?                               </td> <td>1           </td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>18  </td> <td>37  </td> <td>38  </td> <td>Why are so many Quora users posting questions that are r ...</td> <td>Why do people ask Quora questions which can be answered  ...</td> <td>1           </td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>20  </td> <td>41  </td> <td>42  </td> <td>Why do rockets look white?                                  </td> <td>Why are rockets and boosters painted white?                 </td> <td>1           </td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>29  </td> <td>59  </td> <td>60  </td> <td>How should I prepare for CA final law?                      </td> <td>How one should know that he/she completely prepare for C ...</td> <td>1           </td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>31  </td> <td>63  </td> <td>64  </td> <td>What are some special cares for someone with a nose that ...</td> <td>How can I keep my nose from getting stuffy at night?        </td> <td>1           </td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>32  </td> <td>65  </td> <td>66  </td> <td>What Game of Thrones villain would be the most likely to ...</td> <td>What Game of Thrones villain would you most like to be a ...</td> <td>1           </td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>38  </td> <td>77  </td> <td>78  </td> <td>How do we prepare for UPSC?                                 </td> <td>How do I prepare for civil service?                         </td> <td>1           </td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>48  </td> <td>97  </td> <td>98  </td> <td>What are some examples of products that can be make from ...</td> <td>What are some of the products made from crude oil?          </td> <td>1           </td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>49  </td> <td>99  </td> <td>100 </td> <td>How do I make friends.                                      </td> <td>How to make friends ?                                       </td> <td>1           </td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>50  </td> <td>101 </td> <td>102 </td> <td>Is Career Launcher good for RBI Grade B preparation?        </td> <td>How is career launcher online program for RBI Grade B?      </td> <td>1           </td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>51  </td> <td>103 </td> <td>104 </td> <td>Will a Blu Ray play on a regular DVD player? If so, how?    </td> <td>How can you play a Blu Ray DVD on a regular DVD player?     </td> <td>1           </td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>53  </td> <td>107 </td> <td>108 </td> <td>What is the best/most memorable thing you've ever eaten  ...</td> <td>What is the most delicious dish you've ever eaten and why?  </td> <td>1           </td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>58  </td> <td>117 </td> <td>118 </td> <td>I was suddenly logged off Gmail. I can't remember my Gma ...</td> <td>I can't remember my Gmail password or my recovery email. ...</td> <td>1           </td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>62  </td> <td>125 </td> <td>126 </td> <td>How is the new Harry Potter book 'Harry Potter and the C ...</td> <td>How bad is the new book by J.K Rowling?                     </td> <td>1           </td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>65  </td> <td>131 </td> <td>132 </td> <td>What is Java programming? How To Learn Java Programming  ...</td> <td>How do I learn a computer language like java?               </td> <td>1           </td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>66  </td> <td>133 </td> <td>134 </td> <td>What is the best book ever made?                            </td> <td>What is the most important book you have ever read?         </td> <td>1           </td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>67  </td> <td>135 </td> <td>136 </td> <td>Can we ever store energy produced in lightning?             </td> <td>Is it possible to store the energy of lightning?            </td> <td>1           </td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>71  </td> <td>143 </td> <td>144 </td> <td>What is a narcissistic personality disorder?                </td> <td>What is narcissistic personality disorder?                  </td> <td>1           </td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>72  </td> <td>145 </td> <td>146 </td> <td>How I can speak English fluently?                           </td> <td>How can I learn to speak English fluently?                  </td> <td>1           </td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>73  </td> <td>147 </td> <td>148 </td> <td>How helpful is QuickBooks' auto data recovery support ph ...</td> <td>What is the quickbooks customer support phone number USA?   </td> <td>1           </td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>74  </td> <td>149 </td> <td>150 </td> <td>Who is the richest gambler of all time and how can I rea ...</td> <td>Who is the richest gambler of all time and how can I rea ...</td> <td>1           </td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>79  </td> <td>159 </td> <td>160 </td> <td>What is purpose of life?                                    </td> <td>What's the purpose of life? What is life actually about?    </td> <td>1           </td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>84  </td> <td>169 </td> <td>170 </td> <td>What are some of the high salary income jobs in the fiel ...</td> <td>What are some high paying jobs for a fresher with an M.T ...</td> <td>1           </td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>85  </td> <td>171 </td> <td>172 </td> <td>How can I increase my height after 21 also?                 </td> <td>Can height increase after 25?                               </td> <td>1           </td>\n",
       "        </tr>\n",
       "    </tbody>\n",
       "</table>\n",
       "<p>... (149233 rows omitted)</p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "true_Tb = Tb.where(\"is_duplicate\", are.equal_to(\"1\"))\n",
    "true_Tb.show(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "row = true_Tb.num_rows\n",
    "sent1 =  true_Tb.column(\"question1\")\n",
    "sent2 = true_Tb.column(\"question2\")\n",
    "a=[0]*row\n",
    "b=[0]*row\n",
    "\n",
    "bleh=deque()\n",
    "\n",
    "for x in range((row)):\n",
    "    sen1=no_ques(sent1[x])\n",
    "    sen2=no_ques(sent2[x])\n",
    "    sen1=sen1.split()\n",
    "    sen2=sen2.split()\n",
    "    \n",
    "    d = seqDiff2(sen1, sen2, bleh, [])\n",
    "    e = seqDiff2(sen2,sen1,bleh,[])\n",
    "    a[x]=d\n",
    "    b[x]=e    \n",
    "   \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def no_ques(sent1):\n",
    "    \n",
    "    g = [x.replace(\"?\", \"\") for x in sent1]\n",
    "    \n",
    "    return ''.join(g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_Tb=Table().with_column( \"sent1_only\", a, \"sent2_only\",b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_Tb.to_csv(\"sanitized_QQP_dataset.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_Tb.show(40) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "row = Tb.num_rows\n",
    "id = Tb.column(\"id\")\n",
    "\n",
    "s1 =  Tb.column(\"question1\")\n",
    "s2 = Tb.column(\"question2\")  #qqp\n",
    "phrase = ppdb_Tb.column(\"PHRASE\")\n",
    "paraphrase=ppdb_Tb.column(\"PARAPHRASE\")   #ppdb\n",
    "label = Tb.column(\"is_duplicate\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " the types of \n",
      " the types of \n",
      " types of \n",
      " the types \n",
      " most powerful \n",
      " most people \n",
      " matter whether \n",
      " how many \n",
      " many different \n",
      " different things \n",
      " the effects of \n",
      " effects of \n",
      " the effects \n",
      " enough to \n",
      " mental illness \n",
      " most important \n",
      " most important \n",
      " are the most important \n",
      " friends \n",
      " 70 years \n",
      " too late \n",
      " too late \n",
      " climate change \n",
      " higher education \n",
      " feel like \n",
      " daily life \n",
      " solar energy \n",
      " solar \n",
      " friendly relations \n",
      " is the meaning of \n",
      " invest in \n",
      " pay off \n",
      " find love \n",
      " services in \n",
      " services in \n",
      " best available \n",
      " best available \n",
      " the best available \n",
      " is continuing \n",
      " continuing in \n",
      " run away \n",
      " run away \n",
      " most beautiful \n",
      " most amazing \n",
      " amazing \n",
      " is most important \n",
      " most important \n",
      " is most important \n",
      " is most important \n",
      " most important \n",
      " most important \n",
      " most important \n",
      " education system \n",
      " education system \n",
      " too late \n",
      " too late \n",
      " management information \n",
      " information systems \n",
      " weather conditions \n",
      " left behind \n",
      " feel like \n",
      " still exist \n"
     ]
    }
   ],
   "source": [
    "\n",
    "index_list=[]\n",
    "PPDB_id = []\n",
    "sent1=[]\n",
    "sent2=[]\n",
    "para_list1=[]\n",
    "para_list2=[]\n",
    "labeling = []\n",
    "\n",
    "ans = []\n",
    "for x in range(len(s1)):   #index of QQP\n",
    "    \n",
    "    for z in range(len(phrase)):  #index of PPDB\n",
    "        if (phrase[z] in s1[x] and paraphrase[z] in s2[x]):\n",
    "            print(phrase[z])\n",
    "            index_list=np.append(index_list, x)\n",
    "            PPDB_id=np.append(PPDB_id, z)\n",
    "            sent1=np.append(sent1, s1[x])\n",
    "            sent2=np.append(sent2, s2[x])\n",
    "            para_list1=np.append(para_list1, phrase[z])\n",
    "            para_list2=np.append(para_list2, paraphrase[z])\n",
    "            labeling = np.append(labeling, label[x])\n",
    "         \n",
    "\n",
    "        \n",
    "        \n",
    "   \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "phrase_dataset = Table().with_column(\"QQP_id\", index_list, \"PPDB_id\", PPDB_id, \"sent1\", sent1, \"sent2\", sent2, \"paraphrase1\", para_list1, \"paraphrase2\", para_list2, \"label\", labeling)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "    <thead>\n",
       "        <tr>\n",
       "            <th>QQP_id</th> <th>PPDB_id</th> <th>sent1</th> <th>sent2</th> <th>paraphrase1</th> <th>paraphrase2</th> <th>label</th>\n",
       "        </tr>\n",
       "    </thead>\n",
       "    <tbody>\n",
       "        <tr>\n",
       "            <td>70    </td> <td>641086     </td> <td>What are the types of immunity?               </td> <td>What are the different types of immunity in our body? </td> <td> the types of  </td> <td> the different types of </td> <td>0    </td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>70    </td> <td>641090     </td> <td>What are the types of immunity?               </td> <td>What are the different types of immunity in our body? </td> <td> the types of  </td> <td> the different types of </td> <td>0    </td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>70    </td> <td>894710     </td> <td>What are the types of immunity?               </td> <td>What are the different types of immunity in our body? </td> <td> types of      </td> <td> the different types of </td> <td>0    </td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>70    </td> <td>1.49785e+06</td> <td>What are the types of immunity?               </td> <td>What are the different types of immunity in our body? </td> <td> the types     </td> <td> the different types    </td> <td>0    </td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>111   </td> <td>173763     </td> <td>Is USA the most powerful country of the world?</td> <td>Why is the USA the most powerful country of the world?</td> <td> most powerful </td> <td> powerful               </td> <td>0    </td>\n",
       "        </tr>\n",
       "    </tbody>\n",
       "</table>\n",
       "<p>... (20899 rows omitted)</p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "phrase_dataset.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "phrase_dataset2=phrase_dataset.to_df()\n",
    "phrase_dataset2.to_csv(\"dataset/Phrase_Level/QQP/PPDB_QQP.tsv\", sep='\\t', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20904"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "phrase_dataset.num_rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_Tb.to_csv(\"QQP_Trues\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a dictionary\n",
    "\n",
    "MAX_NUM_WORDS = 20000\n",
    "tokenizer = keras \\\n",
    "    .preprocessing \\\n",
    "    .text \\\n",
    "    .Tokenizer(num_words=MAX_NUM_WORDS)\n",
    "\n",
    "corpus_x1 = quora.title1_tokenized\n",
    "corpus_x2 = quora.title2_tokenized\n",
    "corpus = pd.concat([\n",
    "  corpus_x1, corpus_x2])\n",
    "#corpus.shape\n",
    "tokenizer.fit_on_texts(corpus)\n",
    "x1 = tokenizer \\\n",
    "    .texts_to_sequences(corpus_x1)\n",
    "x2 = tokenizer \\\n",
    "    .texts_to_sequences(corpus_x2)\n",
    "\n",
    "word_index=tokenizer.word_index\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "corpus.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_seq_len = max([            #why does seq length change when I change dictionary size?\n",
    "    len(seq) for seq in x1])\n",
    "max_seq_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_seq_len1 = max([\n",
    "    len(seq) for seq in x1])\n",
    "print(max_seq_len1)   \n",
    "\n",
    "MAX_SEQUENCE_LENGTH = 115  #better to have words covered than uncovered\n",
    "x1 = keras \\\n",
    "    .preprocessing \\\n",
    "    .sequence \\\n",
    "    .pad_sequences(x1, \n",
    "                   maxlen=MAX_SEQUENCE_LENGTH)\n",
    "\n",
    "\n",
    "x2 = keras \\\n",
    "    .preprocessing \\\n",
    "    .sequence \\\n",
    "    .pad_sequences(x2, \n",
    "                   maxlen=MAX_SEQUENCE_LENGTH)\n",
    "\n",
    "\n",
    "x1[:5]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "dup = quora.is_duplicate\n",
    "\n",
    "y = keras \\\n",
    "    .utils \\\n",
    "    .to_categorical(dup)\n",
    "\n",
    "\n",
    "\n",
    "print(len(quora))\n",
    "\n",
    "y[:8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "\n",
    "def foo():\n",
    "    q=[]\n",
    "    p=0\n",
    "    for x in quora.is_duplicate:\n",
    "        if math.isnan(x):\n",
    "            q = np.append(q, p)\n",
    "        p+=1\n",
    "    return q\n",
    "foo()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We randomly select 5,000 paraphrases and 5,000 non-paraphrases as the dev set, and sample\n",
    "#another 5,000 paraphrases and 5,000 non-paraphrases as the\n",
    "#test set. We keep the remaining instances as the training set 2\n",
    "\n",
    "label = quora.is_duplicate\n",
    "eq_list = []\n",
    "uneq_list=[]\n",
    "\n",
    "count = 0\n",
    "\n",
    "for x in range(len(quora.is_duplicate)):\n",
    "    if label[x]==1:\n",
    "        eq_list = np.append(eq_list, x) \n",
    "    \n",
    "    else:\n",
    "        uneq_list = np.append(uneq_list, x)\n",
    "\n",
    "        \n",
    "eq_list[:3]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eq_list=eq_list.astype(int)\n",
    "uneq_list=uneq_list.astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "random.shuffle(eq_list)\n",
    "random.shuffle(uneq_list)\n",
    "\n",
    "eq_list[:10]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_test_eq = eq_list[:5000]\n",
    "\n",
    "index_test_uneq = uneq_list[:5000]\n",
    "\n",
    "index_dev_eq = eq_list[5000:10000]\n",
    "\n",
    "index_dev_uneq = uneq_list[5000:10000]\n",
    "\n",
    "index_train_eq = eq_list[10000:]\n",
    "\n",
    "index_train_uneq = uneq_list[10000:]\n",
    "\n",
    "index_test = np.append(index_test_eq, index_test_uneq)\n",
    "\n",
    "index_dev = np.append(index_dev_eq, index_dev_uneq)\n",
    "\n",
    "index_train = np.append(index_train_eq, index_train_uneq)\n",
    "\n",
    "random.shuffle(index_test)\n",
    "random.shuffle(index_dev)\n",
    "random.shuffle(index_train)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "x1_test = [x1[i] for i in index_test]\n",
    "\n",
    "x2_test = [x2[i] for i in index_test]\n",
    "\n",
    "y_test = [y[i] for i in index_test]\n",
    "\n",
    "\n",
    "x1_dev = [x1[i] for i in index_dev]\n",
    "\n",
    "x2_dev = [x2[i] for i in index_dev]\n",
    "\n",
    "y_dev=[y[i] for i in index_dev]\n",
    "\n",
    "\n",
    "x1_train = [x1[i] for i in index_train]\n",
    "\n",
    "x2_train = [x2[i] for i in index_train]\n",
    "\n",
    "y_train=[y[i] for i in index_train]\n",
    "\n",
    "\n",
    "print(\"Training Set\")\n",
    "print(\"-\" * 10)\n",
    "print(np.shape(x1_train))\n",
    "print(np.shape(x2_train))\n",
    "print(np.shape(y_train))\n",
    "\n",
    "\n",
    "print(\"-\" * 10)\n",
    "print(np.shape(x1_test))\n",
    "print(np.shape(x2_test))\n",
    "print(np.shape(y_test))\n",
    "\n",
    "\n",
    "print(\"Test Set\")\n",
    "print(y_test[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#add cos similarity value to last elem of word embedding\n",
    "x1_test_cos=[]\n",
    "x2_test_cos=[]\n",
    "x1_train_cos=[]\n",
    "x2_train_cos=[]\n",
    "x1_dev_cos=[]\n",
    "x2_dev_cos=[]\n",
    "\n",
    "from scipy import spatial\n",
    "\n",
    "def cos_sim(vec1, vec2):\n",
    "    return (1 - spatial.distance.cosine(vec1, vec2))\n",
    "\n",
    "for x in range(len(y_train)):\n",
    "    \n",
    "    \n",
    "    cos_train=cos_sim(x1_train[x], x2_train[x])\n",
    "    \n",
    "    \n",
    "    x1_train_cos.append(np.append(x1_train[x], cos_train))\n",
    "    x2_train_cos.append(np.append(x2_train[x], cos_train))\n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "for x in range(len(y_test)):\n",
    "    cos_dev=cos_sim(x1_dev[x], x2_dev[x])\n",
    "    cos_test=cos_sim(x1_test[x], x2_test[x])\n",
    "    x1_test_cos.append(np.append(x1_test[x], cos_test))\n",
    "    x2_test_cos.append(np.append(x2_test[x], cos_test))\n",
    "    x1_dev_cos.append(np.append(x1_dev[x], cos_dev))\n",
    "    x2_dev_cos.append(np.append(x2_dev[x], cos_dev))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array(x1_test_cos)[:10]\n",
    "len(x1_train_cos[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#BiLSTM with GLoVE \n",
    "\n",
    "\n",
    "NUM_CLASSES=2 #boolean; 1 or 0\n",
    "\n",
    "MAX_NUM_WORDS = 20000 #rough estimate\n",
    "\n",
    "MAX_SEQUENCE_LENGTH = 116 #how long one sentence is\n",
    "\n",
    "NUM_EMBEDDING_DIM = 300 #this is arbitrary?\n",
    "\n",
    "NUM_LSTM_UNITS = 128 #output dimension\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "top_input = Input(\n",
    "    shape=(MAX_SEQUENCE_LENGTH, ),    #this is the first sentence\n",
    "    dtype='int32')\n",
    "\n",
    "bm_input = Input(\n",
    "    shape=(MAX_SEQUENCE_LENGTH, ),   #this is the second\n",
    "    dtype='int32')\n",
    "\n",
    "\n",
    "\n",
    "embedding_layer = embedding_layer = Embedding(len(word_index)+1,\n",
    "                            NUM_EMBEDDING_DIM,\n",
    "                            weights=[embedding_matrix],\n",
    "                            input_length=MAX_SEQUENCE_LENGTH,\n",
    "                            trainable=False)\n",
    "top_embedded = embedding_layer(\n",
    "    top_input)\n",
    "bm_embedded = embedding_layer(\n",
    "    bm_input)\n",
    "\n",
    "\n",
    "shared_lstm = Bidirectional(LSTM(NUM_LSTM_UNITS))\n",
    "top_output = shared_lstm(top_embedded)\n",
    "bm_output = shared_lstm(bm_embedded)\n",
    "\n",
    "\n",
    "merged = concatenate(\n",
    "    [top_output, bm_output], \n",
    "    axis=-1)\n",
    "\n",
    "\n",
    "dense =  Dense(\n",
    "    units=NUM_CLASSES, \n",
    "    activation='sigmoid')\n",
    "predictions = dense(merged)\n",
    "\n",
    "\n",
    "\n",
    "model = Model(\n",
    "    inputs=[top_input, bm_input], \n",
    "    outputs=predictions)\n",
    "\n",
    "\n",
    "model.summary()\n",
    "\n",
    "model.compile(\n",
    "    optimizer='Adam',\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=['accuracy'])\n",
    "\n",
    "BATCH_SIZE = 1024\n",
    "\n",
    "NUM_EPOCHS = 15\n",
    "\n",
    "earlyStopping = EarlyStopping(monitor='val_loss', patience=3, verbose=1, mode='min')\n",
    "mcp_save = ModelCheckpoint('quora_weights_with_GLoVE.h5', verbose=1, save_best_only=True, monitor='val_loss', mode='min')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#BiLSTM with last cos_sim element concatenated?\n",
    "NUM_CLASSES=2 #boolean; 1 or 0\n",
    "\n",
    "#rough estimate\n",
    "\n",
    "MAX_SEQUENCE_LENGTH = 51 #how long one sentence is\n",
    "\n",
    "#this is arbitrary?\n",
    "\n",
    "NUM_LSTM_UNITS = 128 #output dimension\n",
    "\n",
    "\n",
    "NUM_EMBEDDING_DIM = 256\n",
    "\n",
    "top_input = Input(\n",
    "    shape=(MAX_SEQUENCE_LENGTH, ),    #this is the first sentence\n",
    "    dtype='int32')\n",
    "\n",
    "bm_input = Input(\n",
    "    shape=(MAX_SEQUENCE_LENGTH, ),   #this is the second\n",
    "    dtype='int32')\n",
    "\n",
    "\n",
    "\n",
    "embedding_layer = Embedding(\n",
    "    MAX_NUM_WORDS, NUM_EMBEDDING_DIM)\n",
    "\n",
    "top_embedded = embedding_layer(\n",
    "    top_input)\n",
    "bm_embedded = embedding_layer(\n",
    "    bm_input)\n",
    "\n",
    "\n",
    "\n",
    "shared_lstm = Bidirectional(LSTM(NUM_LSTM_UNITS))\n",
    "\n",
    "top_output = shared_lstm(top_embedded)\n",
    "\n",
    "bm_output = shared_lstm(bm_embedded)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "merged = concatenate(\n",
    "    [top_output, bm_output], \n",
    "    axis=-1)\n",
    "\n",
    "\n",
    "dense =  Dense(\n",
    "    units=NUM_CLASSES, \n",
    "    activation='sigmoid')\n",
    "predictions = dense(merged)\n",
    "\n",
    "\n",
    "\n",
    "model = Model(\n",
    "    inputs=[top_input, bm_input], \n",
    "    outputs=predictions)\n",
    "\n",
    "\n",
    "model.summary()\n",
    "\n",
    "model.compile(\n",
    "    optimizer='Adam',\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=['accuracy'])\n",
    "\n",
    "BATCH_SIZE = 512\n",
    "\n",
    "NUM_EPOCHS = 30\n",
    "\n",
    "earlyStopping = EarlyStopping(monitor='val_acc', patience=10, verbose=1, mode='max')\n",
    "mcp_save = ModelCheckpoint('BiLSTM_PAWS_QQP_concat_sim_cos.h5', save_best_only=True,  verbose=1, monitor='val_acc', mode='max')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "history = model.fit(x=([np.array(x1_train_cos), np.array(x2_train_cos)]), y=np.array(y_train), batch_size=BATCH_SIZE, epochs=NUM_EPOCHS, verbose=1, callbacks=[earlyStopping, mcp_save], validation_data=([np.array(x1_dev_cos), np.array(x2_dev_cos)], np.array(y_dev)),\n",
    "    \n",
    "    shuffle=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "history = model.fit(x=([np.array(x1_train), np.array(x2_train)]), y=np.array(y_train), batch_size=BATCH_SIZE, epochs=NUM_EPOCHS, verbose=1, callbacks=[earlyStopping, mcp_save], validation_data=([np.array(x1_dev), np.array(x2_dev)], np.array(y_dev)),\n",
    "    \n",
    "    shuffle=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = load_model(\"quora_weights_with_GLoVE.h5\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicts = model.predict(\n",
    "    [x1_test, x2_test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "one = Tb.column('question1')\n",
    "two = Tb.column('question2')\n",
    "no = []\n",
    "err1 = []\n",
    "err2 = []\n",
    "wrongCat = []\n",
    "\n",
    "\n",
    "for x in range(len(index_test)):\n",
    "    if (np.argmax(predicts[x])!=(np.argmax(y_test[x]))):\n",
    "        no=np.append(no, index_test[x])\n",
    "        err1=np.append(err1, one[index_test[x]])\n",
    "        err2=np.append(err2, two[index_test[x]])\n",
    "        wrongCat = np.append(wrongCat, np.argmax(y_test[x]))\n",
    "        \n",
    "print(1-(len(wrongCat)/len(x1_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "error_analysis = Table()\n",
    "\n",
    "\n",
    "error_analysis=error_analysis.with_column(\"id\", no, \"sent1\", err1, \"sent2\", err2, \"classification\", wrongCat)\n",
    "error_analysis.show(30)\n",
    "           \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hue = error_analysis.where(\"classification\", are.equal_to(0)) \n",
    "hue\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "print('Found %s word vectors.' % len(glove))\n",
    "\n",
    "NUM_EMBEDDING_DIM = 300\n",
    "\n",
    "embedding_matrix = np.zeros((len(word_index)+1, NUM_EMBEDDING_DIM))\n",
    "\n",
    "\n",
    "for word, i in word_index.items():\n",
    "    embedding_vector = glove.get(word)\n",
    "    \n",
    "    if embedding_vector is not None:\n",
    "        # words not found in embedding index will be all-zeros.\n",
    "        embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_glove_line(line, dim):\n",
    "    word = None\n",
    "    embedding = None\n",
    "\n",
    "    try:\n",
    "        splitLine = line.split()\n",
    "        word = \" \".join(splitLine[:len(splitLine)-dim])\n",
    "        embedding = np.array([float(val) for val in splitLine[-dim:]])\n",
    "    except:\n",
    "        print(line)\n",
    "\n",
    "    return word, embedding\n",
    "\n",
    "def load_glove_model(glove_filepath, dim):\n",
    "    with open(glove_filepath, encoding=\"utf8\" ) as f:\n",
    "        content = f.readlines()\n",
    "        model = {}\n",
    "        for line in content:\n",
    "            word, embedding = process_glove_line(line, dim)\n",
    "            if embedding is not None:\n",
    "                model[word] = embedding\n",
    "        return model\n",
    "\n",
    "glove = load_glove_model(\"glove.840B.300d.txt\", 300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = load_model('BiLSTM_QQP.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
