{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib\n",
    "import argparse\n",
    "import random\n",
    "import numpy as np\n",
    "import csv\n",
    "import pandas as pd\n",
    "from datascience import *\n",
    "import tensorflow as tf\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from keras.models import Model\n",
    "from keras.models import Sequential, load_model\n",
    "from keras.layers import *\n",
    "from keras.optimizers import Adam\n",
    "from keras import Input\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.utils import plot_model\n",
    "import keras\n",
    "from keras.backend.tensorflow_backend import set_session\n",
    "#config = tf.ConfigProto()\n",
    "# config.gpu_options.allocator_type = 'BFC' #A \"Best-fit with coalescing\" algorithm, simplified from a version of dlmalloc.\n",
    "# config.gpu_options.per_process_gpu_memory_fraction = 0.7\n",
    "#config.gpu_options.allow_growth =True\n",
    "\n",
    "#set_session(tf.Session(config=config)) \n",
    "import re\n",
    "os.environ['PYTHONHASHSEED'] = '0'\n",
    "random.seed(1)\n",
    "np.random.seed(1)\n",
    "tf.set_random_seed(1)\n",
    "from keras.callbacks import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "print(tf.test.is_gpu_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 348,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ff  ffa'"
      ]
     },
     "execution_count": 348,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def sanitize(string):\n",
    "    words = string.split(' ')\n",
    "    return words\n",
    "\n",
    "\n",
    "def converter(x):\n",
    "    try:\n",
    "        return ' '.join([x.lower() for x in str(x).split() if x not in stop_words])\n",
    "    except AttributeError:\n",
    "        return None  # or some other value\n",
    "    \n",
    "def whole(x):\n",
    "    return int(round(x))\n",
    "\n",
    "whole(1.0)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>qid1</th>\n",
       "      <th>qid2</th>\n",
       "      <th>question1</th>\n",
       "      <th>question2</th>\n",
       "      <th>is_duplicate</th>\n",
       "      <th>title1_tokenized</th>\n",
       "      <th>title2_tokenized</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>What is the step by step guide to invest in sh...</td>\n",
       "      <td>What is the step by step guide to invest in sh...</td>\n",
       "      <td>0</td>\n",
       "      <td>[What, is, the, step, by, step, guide, to, inv...</td>\n",
       "      <td>[What, is, the, step, by, step, guide, to, inv...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>What is the story of Kohinoor (Koh-i-Noor) Dia...</td>\n",
       "      <td>What would happen if the Indian government sto...</td>\n",
       "      <td>0</td>\n",
       "      <td>[What, is, the, story, of, Kohinoor, (Koh-i-No...</td>\n",
       "      <td>[What, would, happen, if, the, Indian, governm...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>How can I increase the speed of my internet co...</td>\n",
       "      <td>How can Internet speed be increased by hacking...</td>\n",
       "      <td>0</td>\n",
       "      <td>[How, can, I, increase, the, speed, of, my, in...</td>\n",
       "      <td>[How, can, Internet, speed, be, increased, by,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>8</td>\n",
       "      <td>Why am I mentally very lonely? How can I solve...</td>\n",
       "      <td>Find the remainder when [math]23^{24}[/math] i...</td>\n",
       "      <td>0</td>\n",
       "      <td>[Why, am, I, mentally, very, lonely?, How, can...</td>\n",
       "      <td>[Find, the, remainder, when, [math]23^{24}[/ma...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>9</td>\n",
       "      <td>10</td>\n",
       "      <td>Which one dissolve in water quikly sugar, salt...</td>\n",
       "      <td>Which fish would survive in salt water?</td>\n",
       "      <td>0</td>\n",
       "      <td>[Which, one, dissolve, in, water, quikly, suga...</td>\n",
       "      <td>[Which, fish, would, survive, in, salt, water?]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id  qid1  qid2                                          question1  \\\n",
       "0   0     1     2  What is the step by step guide to invest in sh...   \n",
       "1   1     3     4  What is the story of Kohinoor (Koh-i-Noor) Dia...   \n",
       "2   2     5     6  How can I increase the speed of my internet co...   \n",
       "3   3     7     8  Why am I mentally very lonely? How can I solve...   \n",
       "4   4     9    10  Which one dissolve in water quikly sugar, salt...   \n",
       "\n",
       "                                           question2  is_duplicate  \\\n",
       "0  What is the step by step guide to invest in sh...             0   \n",
       "1  What would happen if the Indian government sto...             0   \n",
       "2  How can Internet speed be increased by hacking...             0   \n",
       "3  Find the remainder when [math]23^{24}[/math] i...             0   \n",
       "4            Which fish would survive in salt water?             0   \n",
       "\n",
       "                                    title1_tokenized  \\\n",
       "0  [What, is, the, step, by, step, guide, to, inv...   \n",
       "1  [What, is, the, story, of, Kohinoor, (Koh-i-No...   \n",
       "2  [How, can, I, increase, the, speed, of, my, in...   \n",
       "3  [Why, am, I, mentally, very, lonely?, How, can...   \n",
       "4  [Which, one, dissolve, in, water, quikly, suga...   \n",
       "\n",
       "                                    title2_tokenized  \n",
       "0  [What, is, the, step, by, step, guide, to, inv...  \n",
       "1  [What, would, happen, if, the, Indian, governm...  \n",
       "2  [How, can, Internet, speed, be, increased, by,...  \n",
       "3  [Find, the, remainder, when, [math]23^{24}[/ma...  \n",
       "4    [Which, fish, would, survive, in, salt, water?]  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#read in the csv files \n",
    "\n",
    "quora = pd.read_csv(\"quora_duplicate_questions (1).tsv\", sep='\\t')\n",
    "\n",
    "\n",
    "quora['question1']=quora['question1'].fillna(\"\")\n",
    "quora['question2']=quora['question2'].fillna(\"\")\n",
    "\n",
    "\n",
    "quora['title1_tokenized'] = \\\n",
    "    quora.loc[:, 'question1'] \\\n",
    "         .apply(sanitize)\n",
    "quora['title2_tokenized'] = \\\n",
    "    quora.loc[:, 'question2'] \\\n",
    "         .apply(sanitize)\n",
    "\n",
    "\n",
    "\n",
    "quora.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "    <thead>\n",
       "        <tr>\n",
       "            <th>id</th> <th>qid1</th> <th>qid2</th> <th>question1</th> <th>question2</th> <th>is_duplicate</th>\n",
       "        </tr>\n",
       "    </thead>\n",
       "    <tbody>\n",
       "        <tr>\n",
       "            <td>0   </td> <td>1   </td> <td>2   </td> <td>What is the step by step guide to invest in share market ...</td> <td>What is the step by step guide to invest in share market?   </td> <td>0           </td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>1   </td> <td>3   </td> <td>4   </td> <td>What is the story of Kohinoor (Koh-i-Noor) Diamond?         </td> <td>What would happen if the Indian government stole the Koh ...</td> <td>0           </td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>2   </td> <td>5   </td> <td>6   </td> <td>How can I increase the speed of my internet connection w ...</td> <td>How can Internet speed be increased by hacking through DNS? </td> <td>0           </td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>3   </td> <td>7   </td> <td>8   </td> <td>Why am I mentally very lonely? How can I solve it?          </td> <td>Find the remainder when [math]23^{24}[/math] is divided  ...</td> <td>0           </td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>4   </td> <td>9   </td> <td>10  </td> <td>Which one dissolve in water quikly sugar, salt, methane  ...</td> <td>Which fish would survive in salt water?                     </td> <td>0           </td>\n",
       "        </tr>\n",
       "    </tbody>\n",
       "</table>\n",
       "<p>... (404285 rows omitted)</p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "Tb = Table()\n",
    "list1 = []\n",
    "list2 = []\n",
    "\n",
    "with open(\"quora_duplicate_questions (1).tsv\", 'r', encoding='utf-8') as fd:\n",
    "    rd = csv.reader(fd, delimiter=\"\\t\", quotechar='\"')\n",
    "    for row in rd:\n",
    "        \n",
    "        list1.append(row) #pairs start at index 1, with id, setence1, sentence2, label, respectively.\n",
    "     \n",
    "    for y in range(6):\n",
    "        temp=[]\n",
    "        for x in range(1, len(list1)):\n",
    "            temp.append(list1[x][y])\n",
    "            \n",
    "        list2.append(temp)\n",
    "\n",
    "    for ele in range(len(list2)):\n",
    "        Tb=Tb.with_columns(list1[0][ele], list2[ele])\n",
    "\n",
    "\n",
    "Tb.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i' 'me' 'my' 'myself' 'we' 'our' 'ours' 'ourselves' 'you' \"you're\"\n",
      " \"you've\" \"you'll\" \"you'd\" 'your' 'yours' 'yourself' 'yourselves' 'he'\n",
      " 'him' 'his' 'himself' 'she' \"she's\" 'her' 'hers' 'herself' 'it' \"it's\"\n",
      " 'its' 'itself' 'they' 'them' 'their' 'theirs' 'themselves' 'what' 'which'\n",
      " 'who' 'whom' 'this' 'that' \"that'll\" 'these' 'those' 'am' 'is' 'are'\n",
      " 'was' 'were' 'be' 'been' 'being' 'have' 'has' 'had' 'having' 'do' 'does'\n",
      " 'did' 'doing' 'a' 'an' 'the' 'and' 'but' 'if' 'or' 'because' 'as' 'until'\n",
      " 'while' 'of' 'at' 'by' 'for' 'with' 'about' 'against' 'between' 'into'\n",
      " 'through' 'during' 'before' 'after' 'above' 'below' 'to' 'from' 'up'\n",
      " 'down' 'in' 'out' 'on' 'off' 'over' 'under' 'again' 'further' 'then'\n",
      " 'once' 'here' 'there' 'when' 'where' 'why' 'how' 'all' 'any' 'both'\n",
      " 'each' 'few' 'more' 'most' 'other' 'some' 'such' 'no' 'nor' 'not' 'only'\n",
      " 'own' 'same' 'so' 'than' 'too' 'very' 's' 't' 'can' 'will' 'just' 'don'\n",
      " \"don't\" 'should' \"should've\" 'now' 'd' 'll' 'm' 'o' 're' 've' 'y' 'ain'\n",
      " 'aren' \"aren't\" 'couldn' \"couldn't\" 'didn' \"didn't\" 'doesn' \"doesn't\"\n",
      " 'hadn' \"hadn't\" 'hasn' \"hasn't\" 'haven' \"haven't\" 'isn' \"isn't\" 'ma'\n",
      " 'mightn' \"mightn't\" 'mustn' \"mustn't\" 'needn' \"needn't\" 'shan' \"shan't\"\n",
      " 'shouldn' \"shouldn't\" 'wasn' \"wasn't\" 'weren' \"weren't\" 'won' \"won't\"\n",
      " 'wouldn' \"wouldn't\" \"i'm\"]\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "stop_words=(stopwords.words('english'))\n",
    "stop_words=np.append(stop_words, \"i'm\")\n",
    "print(stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words_punct=np.append(stop_words, \"?\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from scipy import spatial\n",
    "\n",
    "def cos_sim(vec1, vec2):\n",
    "    return (1 - spatial.distance.cosine(vec1, vec2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "import difflib\n",
    "\n",
    "case_a = 'i like apple'\n",
    "case_b = 'i like pie'\n",
    "\n",
    "output_list = [li for li in difflib.ndiff(case_a, case_b) if li[0] != ' ']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import deque"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 366,
   "metadata": {},
   "outputs": [],
   "source": [
    "def diff(sen1, sen2):   \n",
    "    a=[]\n",
    "    b=[]\n",
    "    c=[]\n",
    "    wordlist1 = sen1.split()\n",
    "    wordlist2 = sen2.split()   \n",
    "    for i in wordlist1:\n",
    "        if i.lower() in stop_words:  #omit stop words\n",
    "            continue    \n",
    "        if i not in wordlist2:\n",
    "            a=np.append(a, i)\n",
    "        else:\n",
    "            c=np.append(c, i)\n",
    "    for j in wordlist2:\n",
    "        if j.lower() in stop_words:   #omit stop words\n",
    "            continue\n",
    "        if j not in wordlist1:\n",
    "            b=np.append(b, j)  \n",
    "    return np.array(a),np.array(b),np.array(c)       \n",
    "\n",
    "\n",
    "def seqDiff(sen1,sen2,a,b):      #include length 1\n",
    "    #wordlist1 = sen1.split()\n",
    "    #wordlist2 = sen2.split()\n",
    "    sen1=[x.lower() for x in sen1]\n",
    "    sen2=[x.lower() for x in sen2]\n",
    "    \n",
    "    \n",
    "    if len(sen1)==1:\n",
    "                \n",
    "        if sen1[0] in sen2 or sen1[0] in stop_words:\n",
    "            if len(a)>0:\n",
    "                b.append(a)\n",
    "                \n",
    "            return b\n",
    "        \n",
    "        else:\n",
    "            a=np.append(a,sen1[0])\n",
    "            b.append(a)\n",
    "                \n",
    "            return b\n",
    "    \n",
    "    elif sen1[0] in sen2:\n",
    "        \n",
    "        if len(a)>0:\n",
    "            b.append(a)\n",
    "        a=[]        \n",
    "        return seqDiff(sen1[1:], sen2,a,b)\n",
    "        \n",
    "    elif (sen1[0] not in sen2):\n",
    "        \n",
    "        if sen1[0] not in stop_words:\n",
    "            a=np.append(a,sen1[0])\n",
    "        \n",
    "        elif len(a)>0: \n",
    "            b.append(a)\n",
    "            a=[]\n",
    "            \n",
    "        return seqDiff(sen1[1:], sen2,a,b)\n",
    "    \n",
    "    \n",
    "    \n",
    "def seqDiff2(sen1,sen2,a,b):     # inlude length 1\n",
    "    #wordlist1 = sen1.split()\n",
    "    #wordlist2 = sen2.split()\n",
    "    \n",
    "    if len(sen1)==1:\n",
    "        \n",
    "        if  sen1[0] in sen2.lower() or sen1[0].lower() in sen2.lower() or sen1[0] in sen2 or sen1[0].lower() in sen2 or sen1[0].lower() in stop_words:\n",
    "            if len(a)!=0:\n",
    "                b.append(a)\n",
    "            return b\n",
    "        if sen1[0] not in sen2:\n",
    "            a=np.append(a,sen1[0])\n",
    "            b.append(a)\n",
    "                \n",
    "            return b\n",
    "    \n",
    "    elif sen1[0] in sen2 or sen1[0].lower() in sen2:\n",
    "        if len(a)!=0:\n",
    "            b.append(a)\n",
    "            a=[]\n",
    "                \n",
    "        return seqDiff(sen1[1:], sen2,a,b)\n",
    "        \n",
    "    elif (sen1[0] not in sen2) and (sen1[0].lower() not in sen2):\n",
    "        \n",
    "        if sen1[0].lower() not in stop_words:\n",
    "            a=np.append(a,sen1[0])\n",
    "        \n",
    "        return seqDiff(sen1[1:], sen2,a,b)\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 344,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'replace'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-344-3094469dd732>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mhue\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mdeck\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdeque\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mtemp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mseqDiff\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mg\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mhue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdeck\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0mtemp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-343-6aef7a22431d>\u001b[0m in \u001b[0;36mseqDiff\u001b[0;34m(sen1, sen2, a, b)\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0msen1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msen1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m     \u001b[0msen2\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msen2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m     \u001b[0msen1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mno_ques\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msen1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m     \u001b[0msen2\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mno_quest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msen2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-342-c78666faad29>\u001b[0m in \u001b[0;36mno_ques\u001b[0;34m(string)\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mno_ques\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstring\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0mstring\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"?\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mconverter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'replace'"
     ]
    }
   ],
   "source": [
    "g=\"How can I be a good geologist?\" \n",
    "hue=\"What should I do to be a great geologist\"\n",
    "g=g.split()\n",
    "hue=hue.split()\n",
    "deck=deque()\n",
    "temp=seqDiff(g,hue,[],deck)\n",
    "temp\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1,  2,  3,  4, 12,  3,  4])"
      ]
     },
     "execution_count": 218,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hh=[1,2,3,4]\n",
    "hhh=[12,3,4]\n",
    "hhhh=np.append(hh,hhh)\n",
    "hhhh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['- a', '+ i', '- p', '- l']"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "    <thead>\n",
       "        <tr>\n",
       "            <th>id</th> <th>qid1</th> <th>qid2</th> <th>question1</th> <th>question2</th> <th>is_duplicate</th>\n",
       "        </tr>\n",
       "    </thead>\n",
       "    <tbody>\n",
       "        <tr>\n",
       "            <td>5   </td> <td>11  </td> <td>12  </td> <td>Astrology: I am a Capricorn Sun Cap moon and cap rising. ...</td> <td>I'm a triple Capricorn (Sun, Moon and ascendant in Capri ...</td> <td>1           </td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>7   </td> <td>15  </td> <td>16  </td> <td>How can I be a good geologist?                              </td> <td>What should I do to be a great geologist?                   </td> <td>1           </td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>11  </td> <td>23  </td> <td>24  </td> <td>How do I read and find my YouTube comments?                 </td> <td>How can I see all my Youtube comments?                      </td> <td>1           </td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>12  </td> <td>25  </td> <td>26  </td> <td>What can make Physics easy to learn?                        </td> <td>How can you make physics easy to learn?                     </td> <td>1           </td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>13  </td> <td>27  </td> <td>28  </td> <td>What was your first sexual experience like?                 </td> <td>What was your first sexual experience?                      </td> <td>1           </td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>15  </td> <td>31  </td> <td>32  </td> <td>What would a Trump presidency mean for current internati ...</td> <td>How will a Trump presidency affect the students presentl ...</td> <td>1           </td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>16  </td> <td>33  </td> <td>34  </td> <td>What does manipulation mean?                                </td> <td>What does manipulation means?                               </td> <td>1           </td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>18  </td> <td>37  </td> <td>38  </td> <td>Why are so many Quora users posting questions that are r ...</td> <td>Why do people ask Quora questions which can be answered  ...</td> <td>1           </td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>20  </td> <td>41  </td> <td>42  </td> <td>Why do rockets look white?                                  </td> <td>Why are rockets and boosters painted white?                 </td> <td>1           </td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>29  </td> <td>59  </td> <td>60  </td> <td>How should I prepare for CA final law?                      </td> <td>How one should know that he/she completely prepare for C ...</td> <td>1           </td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>31  </td> <td>63  </td> <td>64  </td> <td>What are some special cares for someone with a nose that ...</td> <td>How can I keep my nose from getting stuffy at night?        </td> <td>1           </td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>32  </td> <td>65  </td> <td>66  </td> <td>What Game of Thrones villain would be the most likely to ...</td> <td>What Game of Thrones villain would you most like to be a ...</td> <td>1           </td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>38  </td> <td>77  </td> <td>78  </td> <td>How do we prepare for UPSC?                                 </td> <td>How do I prepare for civil service?                         </td> <td>1           </td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>48  </td> <td>97  </td> <td>98  </td> <td>What are some examples of products that can be make from ...</td> <td>What are some of the products made from crude oil?          </td> <td>1           </td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>49  </td> <td>99  </td> <td>100 </td> <td>How do I make friends.                                      </td> <td>How to make friends ?                                       </td> <td>1           </td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>50  </td> <td>101 </td> <td>102 </td> <td>Is Career Launcher good for RBI Grade B preparation?        </td> <td>How is career launcher online program for RBI Grade B?      </td> <td>1           </td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>51  </td> <td>103 </td> <td>104 </td> <td>Will a Blu Ray play on a regular DVD player? If so, how?    </td> <td>How can you play a Blu Ray DVD on a regular DVD player?     </td> <td>1           </td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>53  </td> <td>107 </td> <td>108 </td> <td>What is the best/most memorable thing you've ever eaten  ...</td> <td>What is the most delicious dish you've ever eaten and why?  </td> <td>1           </td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>58  </td> <td>117 </td> <td>118 </td> <td>I was suddenly logged off Gmail. I can't remember my Gma ...</td> <td>I can't remember my Gmail password or my recovery email. ...</td> <td>1           </td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>62  </td> <td>125 </td> <td>126 </td> <td>How is the new Harry Potter book 'Harry Potter and the C ...</td> <td>How bad is the new book by J.K Rowling?                     </td> <td>1           </td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>65  </td> <td>131 </td> <td>132 </td> <td>What is Java programming? How To Learn Java Programming  ...</td> <td>How do I learn a computer language like java?               </td> <td>1           </td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>66  </td> <td>133 </td> <td>134 </td> <td>What is the best book ever made?                            </td> <td>What is the most important book you have ever read?         </td> <td>1           </td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>67  </td> <td>135 </td> <td>136 </td> <td>Can we ever store energy produced in lightning?             </td> <td>Is it possible to store the energy of lightning?            </td> <td>1           </td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>71  </td> <td>143 </td> <td>144 </td> <td>What is a narcissistic personality disorder?                </td> <td>What is narcissistic personality disorder?                  </td> <td>1           </td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>72  </td> <td>145 </td> <td>146 </td> <td>How I can speak English fluently?                           </td> <td>How can I learn to speak English fluently?                  </td> <td>1           </td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>73  </td> <td>147 </td> <td>148 </td> <td>How helpful is QuickBooks' auto data recovery support ph ...</td> <td>What is the quickbooks customer support phone number USA?   </td> <td>1           </td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>74  </td> <td>149 </td> <td>150 </td> <td>Who is the richest gambler of all time and how can I rea ...</td> <td>Who is the richest gambler of all time and how can I rea ...</td> <td>1           </td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>79  </td> <td>159 </td> <td>160 </td> <td>What is purpose of life?                                    </td> <td>What's the purpose of life? What is life actually about?    </td> <td>1           </td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>84  </td> <td>169 </td> <td>170 </td> <td>What are some of the high salary income jobs in the fiel ...</td> <td>What are some high paying jobs for a fresher with an M.T ...</td> <td>1           </td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>85  </td> <td>171 </td> <td>172 </td> <td>How can I increase my height after 21 also?                 </td> <td>Can height increase after 25?                               </td> <td>1           </td>\n",
       "        </tr>\n",
       "    </tbody>\n",
       "</table>\n",
       "<p>... (149233 rows omitted)</p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "true_Tb = Tb.where(\"is_duplicate\", are.equal_to(\"1\"))\n",
    "true_Tb.show(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 368,
   "metadata": {},
   "outputs": [],
   "source": [
    "row = true_Tb.num_rows\n",
    "sent1 =  true_Tb.column(\"question1\")\n",
    "sent2 = true_Tb.column(\"question2\")\n",
    "a=[0]*row\n",
    "b=[0]*row\n",
    "c=[0]*row\n",
    "bleh=deque()\n",
    "\n",
    "for x in range((row)):\n",
    "    sen1=no_ques(sent1[x])\n",
    "    sen2=no_ques(sent2[x])\n",
    "    sen1=sen1.split()\n",
    "    sen2=sen2.split()\n",
    "    \n",
    "    d = seqDiff(sen1, sen2, bleh, [])\n",
    "    e = seqDiff(sen2,sen1,bleh,[])\n",
    "    a[x]=d\n",
    "    b[x]=e    \n",
    "   \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 367,
   "metadata": {},
   "outputs": [],
   "source": [
    "def no_ques(sent1):\n",
    "    \n",
    "    g = [x.replace(\"?\", \"\") for x in sent1]\n",
    "    \n",
    "    return ''.join(g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 369,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_Tb=Table().with_column( \"sent1_only\", a, \"sent2_only\",b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_Tb.to_csv(\"sanitized_QQP_dataset.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 370,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "    <thead>\n",
       "        <tr>\n",
       "            <th>sent1_only</th> <th>sent2_only</th>\n",
       "        </tr>\n",
       "    </thead>\n",
       "    <tbody>\n",
       "        <tr>\n",
       "            <td>[array(['astrology:'], dtype='<U32'), array(['sun', 'cap ...</td> <td>[array(['triple'], dtype='<U32'), array(['(sun,'], dtype ...</td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>[array(['good'], dtype='<U32')]                             </td> <td>[array(['great'], dtype='<U32')]                            </td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>[array(['read'], dtype='<U32'), array(['find'], dtype='< ...</td> <td>[array(['see'], dtype='<U32')]                              </td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>[]                                                          </td> <td>[]                                                          </td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>[array(['like'], dtype='<U32')]                             </td> <td>[]                                                          </td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>[array(['would'], dtype='<U32'), array(['mean'], dtype=' ...</td> <td>[array(['affect'], dtype='<U32'), array(['presently'], d ...</td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>[array(['mean'], dtype='<U32')]                             </td> <td>[array(['means'], dtype='<U32')]                            </td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>[array(['many'], dtype='<U32'), array(['users', 'posting ...</td> <td>[array(['people', 'ask'], dtype='<U32'), array(['easily' ...</td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>[array(['look'], dtype='<U32')]                             </td> <td>[array(['boosters', 'painted'], dtype='<U32')]              </td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>[array(['law'], dtype='<U32')]                              </td> <td>[array(['one'], dtype='<U32'), array(['know'], dtype='<U ...</td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>[array(['special', 'cares'], dtype='<U32'), array(['some ...</td> <td>[array(['keep'], dtype='<U32'), array(['getting'], dtype ...</td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>[array(['likely'], dtype='<U32'), array(['give'], dtype= ...</td> <td>[array(['like'], dtype='<U32')]                             </td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>[array(['upsc'], dtype='<U32')]                             </td> <td>[array(['civil', 'service'], dtype='<U32')]                 </td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>[array(['examples'], dtype='<U32'), array(['make'], dtyp ...</td> <td>[array(['made'], dtype='<U32')]                             </td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>[array(['friends.'], dtype='<U32')]                         </td> <td>[array(['friends'], dtype='<U32')]                          </td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>[array(['good'], dtype='<U32'), array(['preparation'], d ...</td> <td>[array(['online', 'program'], dtype='<U32')]                </td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>[array(['so,'], dtype='<U32')]                              </td> <td>[]                                                          </td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>[array(['best/most', 'memorable', 'thing'], dtype='<U32')]  </td> <td>[array(['delicious', 'dish'], dtype='<U32')]                </td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>[array(['suddenly', 'logged'], dtype='<U32'), array(['gm ...</td> <td>[array(['email.'], dtype='<U32'), array(['recover'], dty ...</td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>[array(['harry', 'potter'], dtype='<U32'), array([\"'harr ...</td> <td>[array(['bad'], dtype='<U32'), array(['j.k', 'rowling'], ...</td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>[array(['programming'], dtype='<U32'), array(['programmi ...</td> <td>[array(['computer'], dtype='<U32'), array(['like'], dtyp ...</td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>[array(['best'], dtype='<U32'), array(['made'], dtype='< ...</td> <td>[array(['important'], dtype='<U32'), array(['read'], dty ...</td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>[array(['ever'], dtype='<U32'), array(['produced'], dtyp ...</td> <td>[array(['possible'], dtype='<U32')]                         </td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>[]                                                          </td> <td>[]                                                          </td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>[]                                                          </td> <td>[array(['learn'], dtype='<U32')]                            </td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>[array(['helpful'], dtype='<U32'), array([\"quickbooks'\", ...</td> <td>[array(['quickbooks', 'customer'], dtype='<U32'), array( ...</td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>[]                                                          </td> <td>[]                                                          </td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>[]                                                          </td> <td>[array([\"what's\"], dtype='<U32'), array(['actually'], dt ...</td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>[array(['salary', 'income'], dtype='<U32'), array(['fiel ...</td> <td>[array(['paying'], dtype='<U32'), array(['fresher'], dty ...</td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>[array(['21', 'also'], dtype='<U32')]                       </td> <td>[array(['25'], dtype='<U32')]                               </td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>[array(['kamchatca', 'earthquakes'], dtype='<U32'), arra ...</td> <td>[array(['valparaiso', 'earthquake'], dtype='<U32'), arra ...</td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>[array(['60k', 'inr'], dtype='<U32')]                       </td> <td>[array(['rs', '60000'], dtype='<U32')]                      </td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>[array(['movies'], dtype='<U32'), array(['english'], dty ...</td> <td>[array(['movie'], dtype='<U32'), array(['ever', 'seen'], ...</td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>[array(['nightmare'], dtype='<U32')]                        </td> <td>[array(['nightmares'], dtype='<U32'), array(['seem', 're ...</td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>[]                                                          </td> <td>[]                                                          </td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>[array(['really'], dtype='<U32'), array(['uri', 'attack' ...</td> <td>[array(['nuclear'], dtype='<U32')]                          </td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>[array(['recover'], dtype='<U32')]                          </td> <td>[]                                                          </td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>[array([\"what's\"], dtype='<U32')]                           </td> <td>[]                                                          </td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>[array(['china'], dtype='<U32')]                            </td> <td>[array(['chinese'], dtype='<U32')]                          </td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>[array(['needing', 'imrovement'], dtype='<U32')]            </td> <td>[array(['ask'], dtype='<U32'), array(['without', 'gettin ...</td>\n",
       "        </tr>\n",
       "    </tbody>\n",
       "</table>\n",
       "<p>... (149223 rows omitted)</p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "new_Tb.show(40)  #omitting stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_Tb.to_csv(\"QQP_Trues\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    [What, is, the, step, by, step, guide, to, inv...\n",
       "1    [What, is, the, story, of, Kohinoor, (Koh-i-No...\n",
       "2    [How, can, I, increase, the, speed, of, my, in...\n",
       "3    [Why, am, I, mentally, very, lonely?, How, can...\n",
       "4    [Which, one, dissolve, in, water, quikly, suga...\n",
       "dtype: object"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#create a dictionary\n",
    "\n",
    "MAX_NUM_WORDS = 20000\n",
    "tokenizer = keras \\\n",
    "    .preprocessing \\\n",
    "    .text \\\n",
    "    .Tokenizer(num_words=MAX_NUM_WORDS)\n",
    "\n",
    "corpus_x1 = quora.title1_tokenized\n",
    "corpus_x2 = quora.title2_tokenized\n",
    "corpus = pd.concat([\n",
    "  corpus_x1, corpus_x2])\n",
    "#corpus.shape\n",
    "tokenizer.fit_on_texts(corpus)\n",
    "x1 = tokenizer \\\n",
    "    .texts_to_sequences(corpus_x1)\n",
    "x2 = tokenizer \\\n",
    "    .texts_to_sequences(corpus_x2)\n",
    "\n",
    "word_index=tokenizer.word_index\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "corpus.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "115"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_seq_len = max([            #why does seq length change when I change dictionary size?\n",
    "    len(seq) for seq in x1])\n",
    "max_seq_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "115\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[    0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     2,     3,     1,  1394,    54,  1394,  2940,\n",
       "            7,   519,     8,   748,   566,     8,    58],\n",
       "       [    0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            2,     3,     1,   749,     9, 18336, 13264],\n",
       "       [    0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     4,    13,     5,   178,     1,   467,     9,\n",
       "           17,   522,  2556,   165,   128,     6,  7726],\n",
       "       [    0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,    16,    66,     5,  3010,\n",
       "          253,  5384,     4,    13,     5,   546,   112],\n",
       "       [    0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,    22,    49,  8304,\n",
       "            8,   275, 19661,    12,  2050, 13805, 13531]], dtype=int32)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_seq_len1 = max([\n",
    "    len(seq) for seq in x1])\n",
    "print(max_seq_len1)   \n",
    "\n",
    "MAX_SEQUENCE_LENGTH = 115  #better to have words covered than uncovered\n",
    "x1 = keras \\\n",
    "    .preprocessing \\\n",
    "    .sequence \\\n",
    "    .pad_sequences(x1, \n",
    "                   maxlen=MAX_SEQUENCE_LENGTH)\n",
    "\n",
    "\n",
    "x2 = keras \\\n",
    "    .preprocessing \\\n",
    "    .sequence \\\n",
    "    .pad_sequences(x2, \n",
    "                   maxlen=MAX_SEQUENCE_LENGTH)\n",
    "\n",
    "\n",
    "x1[:5]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "404290\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [0., 1.],\n",
       "       [1., 0.],\n",
       "       [0., 1.]], dtype=float32)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "dup = quora.is_duplicate\n",
    "\n",
    "y = keras \\\n",
    "    .utils \\\n",
    "    .to_categorical(dup)\n",
    "\n",
    "\n",
    "\n",
    "print(len(quora))\n",
    "\n",
    "y[:8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "\n",
    "def foo():\n",
    "    q=[]\n",
    "    p=0\n",
    "    for x in quora.is_duplicate:\n",
    "        if math.isnan(x):\n",
    "            q = np.append(q, p)\n",
    "        p+=1\n",
    "    return q\n",
    "foo()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 5.,  7., 11.])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We randomly select 5,000 paraphrases and 5,000 non-paraphrases as the dev set, and sample\n",
    "#another 5,000 paraphrases and 5,000 non-paraphrases as the\n",
    "#test set. We keep the remaining instances as the training set 2\n",
    "\n",
    "label = quora.is_duplicate\n",
    "eq_list = []\n",
    "uneq_list=[]\n",
    "\n",
    "count = 0\n",
    "\n",
    "for x in range(len(quora.is_duplicate)):\n",
    "    if label[x]==1:\n",
    "        eq_list = np.append(eq_list, x) \n",
    "    \n",
    "    else:\n",
    "        uneq_list = np.append(uneq_list, x)\n",
    "\n",
    "        \n",
    "eq_list[:3]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "eq_list=eq_list.astype(int)\n",
    "uneq_list=uneq_list.astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([316085, 199179,  30607,  49563, 306949, 348491,  65560, 308090,\n",
       "       191864,  79564])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "random.shuffle(eq_list)\n",
    "random.shuffle(uneq_list)\n",
    "\n",
    "eq_list[:10]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_test_eq = eq_list[:5000]\n",
    "\n",
    "index_test_uneq = uneq_list[:5000]\n",
    "\n",
    "index_dev_eq = eq_list[5000:10000]\n",
    "\n",
    "index_dev_uneq = uneq_list[5000:10000]\n",
    "\n",
    "index_train_eq = eq_list[10000:]\n",
    "\n",
    "index_train_uneq = uneq_list[10000:]\n",
    "\n",
    "index_test = np.append(index_test_eq, index_test_uneq)\n",
    "\n",
    "index_dev = np.append(index_dev_eq, index_dev_uneq)\n",
    "\n",
    "index_train = np.append(index_train_eq, index_train_uneq)\n",
    "\n",
    "random.shuffle(index_test)\n",
    "random.shuffle(index_dev)\n",
    "random.shuffle(index_train)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Set\n",
      "----------\n",
      "(384290, 115)\n",
      "(384290, 115)\n",
      "(384290, 2)\n",
      "----------\n",
      "(10000, 115)\n",
      "(10000, 115)\n",
      "(10000, 2)\n",
      "Test Set\n",
      "[array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32)]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "x1_test = [x1[i] for i in index_test]\n",
    "\n",
    "x2_test = [x2[i] for i in index_test]\n",
    "\n",
    "y_test = [y[i] for i in index_test]\n",
    "\n",
    "\n",
    "x1_dev = [x1[i] for i in index_dev]\n",
    "\n",
    "x2_dev = [x2[i] for i in index_dev]\n",
    "\n",
    "y_dev=[y[i] for i in index_dev]\n",
    "\n",
    "\n",
    "x1_train = [x1[i] for i in index_train]\n",
    "\n",
    "x2_train = [x2[i] for i in index_train]\n",
    "\n",
    "y_train=[y[i] for i in index_train]\n",
    "\n",
    "\n",
    "print(\"Training Set\")\n",
    "print(\"-\" * 10)\n",
    "print(np.shape(x1_train))\n",
    "print(np.shape(x2_train))\n",
    "print(np.shape(y_train))\n",
    "\n",
    "\n",
    "print(\"-\" * 10)\n",
    "print(np.shape(x1_test))\n",
    "print(np.shape(x2_test))\n",
    "print(np.shape(y_test))\n",
    "\n",
    "\n",
    "print(\"Test Set\")\n",
    "print(y_test[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/scipy/spatial/distance.py:702: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  dist = 1.0 - uv / np.sqrt(uu * vv)\n"
     ]
    }
   ],
   "source": [
    "#add cos similarity value to last elem of word embedding\n",
    "x1_test_cos=[]\n",
    "x2_test_cos=[]\n",
    "x1_train_cos=[]\n",
    "x2_train_cos=[]\n",
    "x1_dev_cos=[]\n",
    "x2_dev_cos=[]\n",
    "\n",
    "from scipy import spatial\n",
    "\n",
    "def cos_sim(vec1, vec2):\n",
    "    return (1 - spatial.distance.cosine(vec1, vec2))\n",
    "\n",
    "for x in range(len(y_train)):\n",
    "    \n",
    "    \n",
    "    cos_train=cos_sim(x1_train[x], x2_train[x])\n",
    "    \n",
    "    \n",
    "    x1_train_cos.append(np.append(x1_train[x], cos_train))\n",
    "    x2_train_cos.append(np.append(x2_train[x], cos_train))\n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "for x in range(len(y_test)):\n",
    "    cos_dev=cos_sim(x1_dev[x], x2_dev[x])\n",
    "    cos_test=cos_sim(x1_test[x], x2_test[x])\n",
    "    x1_test_cos.append(np.append(x1_test[x], cos_test))\n",
    "    x2_test_cos.append(np.append(x2_test[x], cos_test))\n",
    "    x1_dev_cos.append(np.append(x1_dev[x], cos_dev))\n",
    "    x2_dev_cos.append(np.append(x2_dev[x], cos_dev))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "116"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(x1_test_cos)[:10]\n",
    "len(x1_train_cos[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 116)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_2 (InputLayer)            (None, 116)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)         (None, 116, 300)     60332400    input_1[0][0]                    \n",
      "                                                                 input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_1 (Bidirectional) (None, 256)          439296      embedding_1[0][0]                \n",
      "                                                                 embedding_1[1][0]                \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 512)          0           bidirectional_1[0][0]            \n",
      "                                                                 bidirectional_1[1][0]            \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 2)            1026        concatenate_1[0][0]              \n",
      "==================================================================================================\n",
      "Total params: 60,772,722\n",
      "Trainable params: 440,322\n",
      "Non-trainable params: 60,332,400\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#BiLSTM with GLoVE \n",
    "\n",
    "\n",
    "NUM_CLASSES=2 #boolean; 1 or 0\n",
    "\n",
    "MAX_NUM_WORDS = 20000 #rough estimate\n",
    "\n",
    "MAX_SEQUENCE_LENGTH = 116 #how long one sentence is\n",
    "\n",
    "NUM_EMBEDDING_DIM = 300 #this is arbitrary?\n",
    "\n",
    "NUM_LSTM_UNITS = 128 #output dimension\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "top_input = Input(\n",
    "    shape=(MAX_SEQUENCE_LENGTH, ),    #this is the first sentence\n",
    "    dtype='int32')\n",
    "\n",
    "bm_input = Input(\n",
    "    shape=(MAX_SEQUENCE_LENGTH, ),   #this is the second\n",
    "    dtype='int32')\n",
    "\n",
    "\n",
    "\n",
    "embedding_layer = embedding_layer = Embedding(len(word_index)+1,\n",
    "                            NUM_EMBEDDING_DIM,\n",
    "                            weights=[embedding_matrix],\n",
    "                            input_length=MAX_SEQUENCE_LENGTH,\n",
    "                            trainable=False)\n",
    "top_embedded = embedding_layer(\n",
    "    top_input)\n",
    "bm_embedded = embedding_layer(\n",
    "    bm_input)\n",
    "\n",
    "\n",
    "shared_lstm = Bidirectional(LSTM(NUM_LSTM_UNITS))\n",
    "top_output = shared_lstm(top_embedded)\n",
    "bm_output = shared_lstm(bm_embedded)\n",
    "\n",
    "\n",
    "merged = concatenate(\n",
    "    [top_output, bm_output], \n",
    "    axis=-1)\n",
    "\n",
    "\n",
    "dense =  Dense(\n",
    "    units=NUM_CLASSES, \n",
    "    activation='sigmoid')\n",
    "predictions = dense(merged)\n",
    "\n",
    "\n",
    "\n",
    "model = Model(\n",
    "    inputs=[top_input, bm_input], \n",
    "    outputs=predictions)\n",
    "\n",
    "\n",
    "model.summary()\n",
    "\n",
    "model.compile(\n",
    "    optimizer='Adam',\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=['accuracy'])\n",
    "\n",
    "BATCH_SIZE = 1024\n",
    "\n",
    "NUM_EPOCHS = 15\n",
    "\n",
    "earlyStopping = EarlyStopping(monitor='val_loss', patience=3, verbose=1, mode='min')\n",
    "mcp_save = ModelCheckpoint('quora_weights_with_GLoVE.h5', verbose=1, save_best_only=True, monitor='val_loss', mode='min')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#BiLSTM with last cos_sim element concatenated?\n",
    "NUM_CLASSES=2 #boolean; 1 or 0\n",
    "\n",
    "#rough estimate\n",
    "\n",
    "MAX_SEQUENCE_LENGTH = 51 #how long one sentence is\n",
    "\n",
    "#this is arbitrary?\n",
    "\n",
    "NUM_LSTM_UNITS = 128 #output dimension\n",
    "\n",
    "\n",
    "NUM_EMBEDDING_DIM = 256\n",
    "\n",
    "top_input = Input(\n",
    "    shape=(MAX_SEQUENCE_LENGTH, ),    #this is the first sentence\n",
    "    dtype='int32')\n",
    "\n",
    "bm_input = Input(\n",
    "    shape=(MAX_SEQUENCE_LENGTH, ),   #this is the second\n",
    "    dtype='int32')\n",
    "\n",
    "\n",
    "\n",
    "embedding_layer = Embedding(\n",
    "    MAX_NUM_WORDS, NUM_EMBEDDING_DIM)\n",
    "\n",
    "top_embedded = embedding_layer(\n",
    "    top_input)\n",
    "bm_embedded = embedding_layer(\n",
    "    bm_input)\n",
    "\n",
    "\n",
    "\n",
    "shared_lstm = Bidirectional(LSTM(NUM_LSTM_UNITS))\n",
    "\n",
    "top_output = shared_lstm(top_embedded)\n",
    "\n",
    "bm_output = shared_lstm(bm_embedded)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "merged = concatenate(\n",
    "    [top_output, bm_output], \n",
    "    axis=-1)\n",
    "\n",
    "\n",
    "dense =  Dense(\n",
    "    units=NUM_CLASSES, \n",
    "    activation='sigmoid')\n",
    "predictions = dense(merged)\n",
    "\n",
    "\n",
    "\n",
    "model = Model(\n",
    "    inputs=[top_input, bm_input], \n",
    "    outputs=predictions)\n",
    "\n",
    "\n",
    "model.summary()\n",
    "\n",
    "model.compile(\n",
    "    optimizer='Adam',\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=['accuracy'])\n",
    "\n",
    "BATCH_SIZE = 512\n",
    "\n",
    "NUM_EPOCHS = 30\n",
    "\n",
    "earlyStopping = EarlyStopping(monitor='val_acc', patience=10, verbose=1, mode='max')\n",
    "mcp_save = ModelCheckpoint('BiLSTM_PAWS_QQP_concat_sim_cos.h5', save_best_only=True,  verbose=1, monitor='val_acc', mode='max')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Train on 384290 samples, validate on 10000 samples\n",
      "Epoch 1/15\n",
      "384290/384290 [==============================] - 302s 786us/step - loss: 0.5586 - acc: 0.7119 - val_loss: 0.5984 - val_acc: 0.6803\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.59837, saving model to quora_weights_with_GLoVE.h5\n",
      "Epoch 2/15\n",
      "384290/384290 [==============================] - 288s 749us/step - loss: 0.5126 - acc: 0.7474 - val_loss: 0.5627 - val_acc: 0.7122\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.59837 to 0.56267, saving model to quora_weights_with_GLoVE.h5\n",
      "Epoch 3/15\n",
      "384290/384290 [==============================] - 268s 698us/step - loss: 0.4926 - acc: 0.7602 - val_loss: 0.5487 - val_acc: 0.7209\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.56267 to 0.54866, saving model to quora_weights_with_GLoVE.h5\n",
      "Epoch 4/15\n",
      "384290/384290 [==============================] - 268s 697us/step - loss: 0.4759 - acc: 0.7709 - val_loss: 0.5332 - val_acc: 0.7316\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.54866 to 0.53319, saving model to quora_weights_with_GLoVE.h5\n",
      "Epoch 5/15\n",
      "384290/384290 [==============================] - 271s 706us/step - loss: 0.4613 - acc: 0.7797 - val_loss: 0.5396 - val_acc: 0.7332\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.53319\n",
      "Epoch 6/15\n",
      "384290/384290 [==============================] - 273s 711us/step - loss: 0.4471 - acc: 0.7881 - val_loss: 0.5239 - val_acc: 0.7382\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.53319 to 0.52392, saving model to quora_weights_with_GLoVE.h5\n",
      "Epoch 7/15\n",
      "384290/384290 [==============================] - 269s 700us/step - loss: 0.4381 - acc: 0.7937 - val_loss: 0.5263 - val_acc: 0.7358\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.52392\n",
      "Epoch 8/15\n",
      "384290/384290 [==============================] - 267s 696us/step - loss: 0.4216 - acc: 0.8022 - val_loss: 0.5187 - val_acc: 0.7443\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.52392 to 0.51868, saving model to quora_weights_with_GLoVE.h5\n",
      "Epoch 9/15\n",
      "384290/384290 [==============================] - 268s 698us/step - loss: 0.4087 - acc: 0.8090 - val_loss: 0.5292 - val_acc: 0.7436\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.51868\n",
      "Epoch 10/15\n",
      "384290/384290 [==============================] - 267s 694us/step - loss: 0.3952 - acc: 0.8165 - val_loss: 0.5305 - val_acc: 0.7473\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.51868\n",
      "Epoch 11/15\n",
      "384290/384290 [==============================] - 265s 690us/step - loss: 0.3839 - acc: 0.8221 - val_loss: 0.5290 - val_acc: 0.7517\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 0.51868\n",
      "Epoch 00011: early stopping\n"
     ]
    }
   ],
   "source": [
    "\n",
    "history = model.fit(x=([np.array(x1_train_cos), np.array(x2_train_cos)]), y=np.array(y_train), batch_size=BATCH_SIZE, epochs=NUM_EPOCHS, verbose=1, callbacks=[earlyStopping, mcp_save], validation_data=([np.array(x1_dev_cos), np.array(x2_dev_cos)], np.array(y_dev)),\n",
    "    \n",
    "    shuffle=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 384290 samples, validate on 10000 samples\n",
      "Epoch 1/15\n",
      "384290/384290 [==============================] - 301s 783us/step - loss: 0.5399 - acc: 0.7276 - val_loss: 0.5748 - val_acc: 0.6979\n",
      "Epoch 2/15\n",
      "384290/384290 [==============================] - 300s 779us/step - loss: 0.5068 - acc: 0.7511 - val_loss: 0.5521 - val_acc: 0.7143\n",
      "Epoch 3/15\n",
      "384290/384290 [==============================] - 299s 777us/step - loss: 0.4878 - acc: 0.7637 - val_loss: 0.5325 - val_acc: 0.7257\n",
      "Epoch 4/15\n",
      "384290/384290 [==============================] - 303s 789us/step - loss: 0.4697 - acc: 0.7747 - val_loss: 0.5322 - val_acc: 0.7318\n",
      "Epoch 5/15\n",
      "384290/384290 [==============================] - 295s 769us/step - loss: 0.4547 - acc: 0.7835 - val_loss: 0.5230 - val_acc: 0.7360\n",
      "Epoch 6/15\n",
      "384290/384290 [==============================] - 302s 785us/step - loss: 0.4395 - acc: 0.7930 - val_loss: 0.5301 - val_acc: 0.7397\n",
      "Epoch 7/15\n",
      "384290/384290 [==============================] - 303s 789us/step - loss: 0.4250 - acc: 0.8020 - val_loss: 0.5302 - val_acc: 0.7402\n",
      "Epoch 8/15\n",
      "384290/384290 [==============================] - 300s 781us/step - loss: 0.4116 - acc: 0.8091 - val_loss: 0.5466 - val_acc: 0.7330\n",
      "Epoch 00008: early stopping\n"
     ]
    }
   ],
   "source": [
    "\n",
    "history = model.fit(x=([np.array(x1_train), np.array(x2_train)]), y=np.array(y_train), batch_size=BATCH_SIZE, epochs=NUM_EPOCHS, verbose=1, callbacks=[earlyStopping, mcp_save], validation_data=([np.array(x1_dev), np.array(x2_dev)], np.array(y_dev)),\n",
    "    \n",
    "    shuffle=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = load_model(\"quora_weights_with_GLoVE.h5\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicts = model.predict(\n",
    "    [x1_test, x2_test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1., 0.], dtype=float32)"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7455\n"
     ]
    }
   ],
   "source": [
    "one = Tb.column('question1')\n",
    "two = Tb.column('question2')\n",
    "no = []\n",
    "err1 = []\n",
    "err2 = []\n",
    "wrongCat = []\n",
    "\n",
    "\n",
    "for x in range(len(index_test)):\n",
    "    if (np.argmax(predicts[x])!=(np.argmax(y_test[x]))):\n",
    "        no=np.append(no, index_test[x])\n",
    "        err1=np.append(err1, one[index_test[x]])\n",
    "        err2=np.append(err2, two[index_test[x]])\n",
    "        wrongCat = np.append(wrongCat, np.argmax(y_test[x]))\n",
    "        \n",
    "print(1-(len(wrongCat)/len(x1_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "    <thead>\n",
       "        <tr>\n",
       "            <th>id</th> <th>sent1</th> <th>sent2</th> <th>classification</th>\n",
       "        </tr>\n",
       "    </thead>\n",
       "    <tbody>\n",
       "        <tr>\n",
       "            <td>117573</td> <td>What was your favourite subject at school and why?          </td> <td>What is/was your favourite subject at school and why?       </td> <td>1             </td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>357893</td> <td>Suppose Host A sends two TCP segments back to back to Ho ...</td> <td>Suppose Host A sends two TCP segments back to back to Ho ...</td> <td>1             </td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>221481</td> <td>Will I always find chess easier than snooker?               </td> <td>Why do I find chess easier than snooker?                    </td> <td>0             </td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>187484</td> <td>What is the career of photography?                          </td> <td>What's necessary for a successful career in photography?    </td> <td>1             </td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>85852 </td> <td>Could we create an inmortal and indestructible brain usi ...</td> <td>Could we create an indestructible and inmortal brain wit ...</td> <td>1             </td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>307703</td> <td>What are good youtube channels that explain informative  ...</td> <td>What are the useful YouTube channels to gain knowledge?     </td> <td>0             </td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>182829</td> <td>Will I be in trouble if I download movies from torrents  ...</td> <td>In present days download movies from torrent is a crimin ...</td> <td>1             </td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>127301</td> <td>What is the best way to retain what you learn?              </td> <td>What are the best ways to improve my memory?                </td> <td>0             </td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>278486</td> <td>Why doesn't the police or the government help the child  ...</td> <td>Why doesn't the police or the government take actions ag ...</td> <td>1             </td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>203535</td> <td>In what ways is Australia better or different than America? </td> <td>How is living in Australia different from living in the  ...</td> <td>1             </td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>354377</td> <td>Is the second coming near?                                  </td> <td>Will the second coming of Jesus happen in the near future?  </td> <td>1             </td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>400515</td> <td>Is it really possible to lose fat and gain muscle at the ...</td> <td>How can I lower my body fat, while gaining lean muscles  ...</td> <td>0             </td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>115238</td> <td>What is feminism?                                           </td> <td>Can you explain feminism?                                   </td> <td>1             </td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>41602 </td> <td>Which battle has the most casualties in history?            </td> <td>What was the bloodiest battle in history?                   </td> <td>1             </td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>14799 </td> <td>Approximately how much does a Chevy G10 van weigh? Model ...</td> <td>Approximately how much does a Chevy G10 van weigh? Model ...</td> <td>1             </td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>342216</td> <td>What is the worst thing you can do to yourself?             </td> <td>What is the worst thing you have ever done to someone be ...</td> <td>0             </td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>176450</td> <td>Who exactly was Fidel Castro?                               </td> <td>Who is Fidel Castro?                                        </td> <td>1             </td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>97304 </td> <td>What is SIP (Systematic Investment Plan)?                   </td> <td>What is SIP or systematic investment plan?                  </td> <td>1             </td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>278416</td> <td>What is the function of VPN in an iPhone?                   </td> <td>What does VPN do on my iPhone?                              </td> <td>1             </td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>174193</td> <td>What lessons, if any, have you learned from the movie \"I ...</td> <td>What do I learn from the movie 'into the wild'?             </td> <td>1             </td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>48134 </td> <td>Why should I study physics?                                 </td> <td>Why do people study physics?                                </td> <td>1             </td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>177400</td> <td>How hard are Google's software engineer job interviews?     </td> <td>What should I expect in a Software Engineer interview at ...</td> <td>1             </td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>252269</td> <td>Who contributed to Indian independence more, Gandhi or S ...</td> <td>Who contributed to Indian independence more, Gandhi or S ...</td> <td>1             </td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>145437</td> <td>What's the best thing to do in life?                        </td> <td>What's the best thing in life?                              </td> <td>1             </td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>284760</td> <td>Why is \"step-daughter\" not included in the definition of ...</td> <td>Why is 'Step daughter' not been included in the definati ...</td> <td>1             </td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>255731</td> <td>How can Deadpool beat Goku?                                 </td> <td>Who would win in a fight: Deadpool or Goku?                 </td> <td>0             </td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>153359</td> <td>How do I know the name of the person the SIM card is reg ...</td> <td>How do I know on which name a particular SIM card is reg ...</td> <td>1             </td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>243324</td> <td>Is it too late to start learn programming again?            </td> <td>Is it too late to start learning programming at 30?         </td> <td>0             </td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>297316</td> <td>What Deve Gowda did for the country when he was prime mi ...</td> <td>What has Deve Gowda done as the PM of India?                </td> <td>1             </td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>40835 </td> <td>How does a neural turing machine work?                      </td> <td>How can Turing machine works?                               </td> <td>1             </td>\n",
       "        </tr>\n",
       "    </tbody>\n",
       "</table>\n",
       "<p>... (2515 rows omitted)</p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "error_analysis = Table()\n",
    "\n",
    "\n",
    "error_analysis=error_analysis.with_column(\"id\", no, \"sent1\", err1, \"sent2\", err2, \"classification\", wrongCat)\n",
    "error_analysis.show(30)\n",
    "           \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "    <thead>\n",
       "        <tr>\n",
       "            <th>id</th> <th>sent1</th> <th>sent2</th> <th>classification</th>\n",
       "        </tr>\n",
       "    </thead>\n",
       "    <tbody>\n",
       "        <tr>\n",
       "            <td>23  </td> <td>Does marijuana make you gain or lose weight?            </td> <td>Does marijuana make you lose weight?                        </td> <td>0             </td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>46  </td> <td>How can I learn online trading in india?                </td> <td>How can I do online trading in india?                       </td> <td>0             </td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>51  </td> <td>What if the 1802 Peace of Amiens would have been upheld?</td> <td>What is the best thing you have ever done to achieve inn ...</td> <td>0             </td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>53  </td> <td>What are some overrated firearms, and why?              </td> <td>Which historical figure is overrated?                       </td> <td>0             </td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>95  </td> <td>Pauls pemdulum theory?                                  </td> <td>With the modern technology, what are some impending dang ...</td> <td>0             </td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>114 </td> <td>How do I cope with major depression?                    </td> <td>How do you cope with depression?                            </td> <td>0             </td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>220 </td> <td>Is PESIT South campus mechanical really that bad?       </td> <td>Is PESIT south campus really bad?                           </td> <td>0             </td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>307 </td> <td>Who is likely to become the next President of India?    </td> <td>What should I do to become the next president of India?     </td> <td>0             </td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>324 </td> <td>How can India and China be friends?                     </td> <td>Should India join hands with China?                         </td> <td>0             </td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>397 </td> <td>How do I improve leadreship skills?                     </td> <td>How can we improve learning skills?                         </td> <td>0             </td>\n",
       "        </tr>\n",
       "    </tbody>\n",
       "</table>\n",
       "<p>... (12934 rows omitted)</p>"
      ],
      "text/plain": [
       "id   | sent1                                                    | sent2                                                        | classification\n",
       "23   | Does marijuana make you gain or lose weight?             | Does marijuana make you lose weight?                         | 0\n",
       "46   | How can I learn online trading in india?                 | How can I do online trading in india?                        | 0\n",
       "51   | What if the 1802 Peace of Amiens would have been upheld? | What is the best thing you have ever done to achieve inn ... | 0\n",
       "53   | What are some overrated firearms, and why?               | Which historical figure is overrated?                        | 0\n",
       "95   | Pauls pemdulum theory?                                   | With the modern technology, what are some impending dang ... | 0\n",
       "114  | How do I cope with major depression?                     | How do you cope with depression?                             | 0\n",
       "220  | Is PESIT South campus mechanical really that bad?        | Is PESIT south campus really bad?                            | 0\n",
       "307  | Who is likely to become the next President of India?     | What should I do to become the next president of India?      | 0\n",
       "324  | How can India and China be friends?                      | Should India join hands with China?                          | 0\n",
       "397  | How do I improve leadreship skills?                      | How can we improve learning skills?                          | 0\n",
       "... (12934 rows omitted)"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hue = error_analysis.where(\"classification\", are.equal_to(0)) \n",
    "hue\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2195895 word vectors.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "print('Found %s word vectors.' % len(glove))\n",
    "\n",
    "NUM_EMBEDDING_DIM = 300\n",
    "\n",
    "embedding_matrix = np.zeros((len(word_index)+1, NUM_EMBEDDING_DIM))\n",
    "\n",
    "\n",
    "for word, i in word_index.items():\n",
    "    embedding_vector = glove.get(word)\n",
    "    \n",
    "    if embedding_vector is not None:\n",
    "        # words not found in embedding index will be all-zeros.\n",
    "        embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_glove_line(line, dim):\n",
    "    word = None\n",
    "    embedding = None\n",
    "\n",
    "    try:\n",
    "        splitLine = line.split()\n",
    "        word = \" \".join(splitLine[:len(splitLine)-dim])\n",
    "        embedding = np.array([float(val) for val in splitLine[-dim:]])\n",
    "    except:\n",
    "        print(line)\n",
    "\n",
    "    return word, embedding\n",
    "\n",
    "def load_glove_model(glove_filepath, dim):\n",
    "    with open(glove_filepath, encoding=\"utf8\" ) as f:\n",
    "        content = f.readlines()\n",
    "        model = {}\n",
    "        for line in content:\n",
    "            word, embedding = process_glove_line(line, dim)\n",
    "            if embedding is not None:\n",
    "                model[word] = embedding\n",
    "        return model\n",
    "\n",
    "glove = load_glove_model(\"glove.840B.300d.txt\", 300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = load_model('BiLSTM_QQP.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
